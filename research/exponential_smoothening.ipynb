{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv('/Users/macbookpro/Documents/predict_publications/publications_prediction/data/test_data.csv')\n",
    "train_data = pd.read_csv('/Users/macbookpro/Documents/predict_publications/publications_prediction/data/train_data.csv')\n",
    "validation_data = pd.read_csv('/Users/macbookpro/Documents/predict_publications/publications_prediction/data/validation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>point</th>\n",
       "      <th>hour</th>\n",
       "      <th>likescount</th>\n",
       "      <th>commentscount</th>\n",
       "      <th>symbols_cnt</th>\n",
       "      <th>words_cnt</th>\n",
       "      <th>hashtags_cnt</th>\n",
       "      <th>mentions_cnt</th>\n",
       "      <th>links_cnt</th>\n",
       "      <th>emoji_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0101000020E61000000000000000000000000000000000...</td>\n",
       "      <td>0</td>\n",
       "      <td>31.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>51.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.136232</td>\n",
       "      <td>60.000054</td>\n",
       "      <td>0101000020E6100000B8E59619E0223E40ABB649C80100...</td>\n",
       "      <td>0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.138478</td>\n",
       "      <td>59.835705</td>\n",
       "      <td>0101000020E610000077D0A94773233E4097654065F8EA...</td>\n",
       "      <td>0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.142969</td>\n",
       "      <td>60.023627</td>\n",
       "      <td>0101000020E6100000F5A5CFA399243E400B9A5B330603...</td>\n",
       "      <td>0</td>\n",
       "      <td>77.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>34.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.142969</td>\n",
       "      <td>60.030359</td>\n",
       "      <td>0101000020E6100000F5A5CFA399243E40854A58CAE203...</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp        lon        lat  \\\n",
       "0  1546300800   0.000000   0.000000   \n",
       "1  1546300800  30.136232  60.000054   \n",
       "2  1546300800  30.138478  59.835705   \n",
       "3  1546300800  30.142969  60.023627   \n",
       "4  1546300800  30.142969  60.030359   \n",
       "\n",
       "                                               point  hour  likescount  \\\n",
       "0  0101000020E61000000000000000000000000000000000...     0   31.666667   \n",
       "1  0101000020E6100000B8E59619E0223E40ABB649C80100...     0   52.000000   \n",
       "2  0101000020E610000077D0A94773233E4097654065F8EA...     0   32.000000   \n",
       "3  0101000020E6100000F5A5CFA399243E400B9A5B330603...     0   77.666667   \n",
       "4  0101000020E6100000F5A5CFA399243E40854A58CAE203...     0   19.000000   \n",
       "\n",
       "   commentscount  symbols_cnt  words_cnt  hashtags_cnt  mentions_cnt  \\\n",
       "0       1.666667    51.333333   2.000000      2.000000           0.0   \n",
       "1       1.000000    28.000000   0.500000      2.000000           0.0   \n",
       "2       0.333333    46.000000   2.333333      3.000000           0.0   \n",
       "3       3.333333    34.666667   2.666667      0.666667           0.0   \n",
       "4       3.000000     0.000000   0.000000      0.000000           0.0   \n",
       "\n",
       "   links_cnt  emoji_cnt  \n",
       "0        0.0   0.000000  \n",
       "1        0.0   0.500000  \n",
       "2        0.0   1.333333  \n",
       "3        0.0   1.666667  \n",
       "4        0.0   0.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'timestamp' to a datetime format\n",
    "train_data['date'] = pd.to_datetime(train_data['timestamp'], unit='s')\n",
    "\n",
    "# Extracting the hour from the 'date' column\n",
    "train_data['hour'] = train_data['date'].dt.hour\n",
    "\n",
    "# Aggregate data based on 'hour', 'lon', and 'lat'\n",
    "agg_columns = {\n",
    "    'likescount': 'mean',\n",
    "    'commentscount': 'mean',\n",
    "    'symbols_cnt': 'mean',\n",
    "    'words_cnt': 'mean',\n",
    "    'hashtags_cnt': 'mean',\n",
    "    'mentions_cnt': 'mean',\n",
    "    'links_cnt': 'mean',\n",
    "    'emoji_cnt': 'mean',\n",
    "}\n",
    "\n",
    "grouped_data = train_data.groupby(['timestamp', 'lon', 'lat', 'point', 'hour']).agg(agg_columns).reset_index()\n",
    "grouped_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>point</th>\n",
       "      <th>hour</th>\n",
       "      <th>likescount</th>\n",
       "      <th>commentscount</th>\n",
       "      <th>symbols_cnt</th>\n",
       "      <th>words_cnt</th>\n",
       "      <th>hashtags_cnt</th>\n",
       "      <th>mentions_cnt</th>\n",
       "      <th>links_cnt</th>\n",
       "      <th>emoji_cnt</th>\n",
       "      <th>publication_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0101000020E61000000000000000000000000000000000...</td>\n",
       "      <td>0</td>\n",
       "      <td>31.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>51.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.136232</td>\n",
       "      <td>60.000054</td>\n",
       "      <td>0101000020E6100000B8E59619E0223E40ABB649C80100...</td>\n",
       "      <td>0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.138478</td>\n",
       "      <td>59.835705</td>\n",
       "      <td>0101000020E610000077D0A94773233E4097654065F8EA...</td>\n",
       "      <td>0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.142969</td>\n",
       "      <td>60.023627</td>\n",
       "      <td>0101000020E6100000F5A5CFA399243E400B9A5B330603...</td>\n",
       "      <td>0</td>\n",
       "      <td>77.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>34.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.142969</td>\n",
       "      <td>60.030359</td>\n",
       "      <td>0101000020E6100000F5A5CFA399243E40854A58CAE203...</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp        lon        lat  \\\n",
       "0  1546300800   0.000000   0.000000   \n",
       "1  1546300800  30.136232  60.000054   \n",
       "2  1546300800  30.138478  59.835705   \n",
       "3  1546300800  30.142969  60.023627   \n",
       "4  1546300800  30.142969  60.030359   \n",
       "\n",
       "                                               point  hour  likescount  \\\n",
       "0  0101000020E61000000000000000000000000000000000...     0   31.666667   \n",
       "1  0101000020E6100000B8E59619E0223E40ABB649C80100...     0   52.000000   \n",
       "2  0101000020E610000077D0A94773233E4097654065F8EA...     0   32.000000   \n",
       "3  0101000020E6100000F5A5CFA399243E400B9A5B330603...     0   77.666667   \n",
       "4  0101000020E6100000F5A5CFA399243E40854A58CAE203...     0   19.000000   \n",
       "\n",
       "   commentscount  symbols_cnt  words_cnt  hashtags_cnt  mentions_cnt  \\\n",
       "0       1.666667    51.333333   2.000000      2.000000           0.0   \n",
       "1       1.000000    28.000000   0.500000      2.000000           0.0   \n",
       "2       0.333333    46.000000   2.333333      3.000000           0.0   \n",
       "3       3.333333    34.666667   2.666667      0.666667           0.0   \n",
       "4       3.000000     0.000000   0.000000      0.000000           0.0   \n",
       "\n",
       "   links_cnt  emoji_cnt  publication_count  \n",
       "0        0.0   0.000000                  3  \n",
       "1        0.0   0.500000                  2  \n",
       "2        0.0   1.333333                  3  \n",
       "3        0.0   1.666667                  3  \n",
       "4        0.0   0.000000                  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data['publication_count'] = train_data.groupby(['timestamp', 'hour', 'lon', 'lat', 'point']).size().values\n",
    "grouped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'timestamp' as it's strongly correlated with other time features and may cause data leakage\n",
    "X_train = grouped_data.drop(['publication_count', 'timestamp', 'point'], axis=1)\n",
    "y_train = grouped_data['publication_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'hour' column to a datetime format\n",
    "test_data['date'] = pd.to_datetime(test_data['hour'], unit='s')\n",
    "\n",
    "# Drop the original 'hour' column which contains the timestamp\n",
    "test_data.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "# Extract the datetime features from the 'date' column\n",
    "test_data['hour'] = test_data['date'].dt.hour\n",
    "test_data['day'] = test_data['date'].dt.day\n",
    "test_data['dayofweek'] = test_data['date'].dt.dayofweek\n",
    "test_data['month'] = test_data['date'].dt.month\n",
    "\n",
    "# Drop the 'date' column as it's not needed for prediction\n",
    "test_data.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Set 'point' as the index for both datasets\n",
    "train_data.set_index('point', inplace=True)\n",
    "test_data.set_index('point', inplace=True)\n",
    "\n",
    "# List of features to create in the test dataset\n",
    "features_to_create = ['likescount', 'commentscount', 'symbols_cnt', 'words_cnt', \n",
    "                      'hashtags_cnt', 'mentions_cnt', 'links_cnt', 'emoji_cnt']\n",
    "\n",
    "# Aggregate the training dataset based on 'point' and compute the median for each feature\n",
    "aggregated_data = train_data[features_to_create].groupby('point').median()\n",
    "\n",
    "# Merge the test dataset with the aggregated training data on 'point'\n",
    "test_data = test_data.join(aggregated_data, on='point', how='left')\n",
    "\n",
    "# Reset index for both datasets after the operations\n",
    "train_data.reset_index(inplace=True)\n",
    "test_data.reset_index(inplace=True)\n",
    "\n",
    "X_test = test_data.drop(['sum', 'point', 'error'], axis=1)\n",
    "y_test = test_data['sum']\n",
    "X_test = X_test[X_train.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/statsmodels/tsa/holtwinters/model.py:83: RuntimeWarning: overflow encountered in matmul\n",
      "  return err.T @ err\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/macbookpro/Documents/predict_publications/publications_prediction/research/exponential_smoothening.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro/Documents/predict_publications/publications_prediction/research/exponential_smoothening.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# 2. Train the Exponential Smoothing model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro/Documents/predict_publications/publications_prediction/research/exponential_smoothening.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# We will use additive trend and seasonality as it's common for this kind of data. \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro/Documents/predict_publications/publications_prediction/research/exponential_smoothening.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# The seasonal period is set to 24, assuming the data is hourly.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro/Documents/predict_publications/publications_prediction/research/exponential_smoothening.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model \u001b[39m=\u001b[39m ExponentialSmoothing(y_train, trend\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madd\u001b[39m\u001b[39m'\u001b[39m, seasonal\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madd\u001b[39m\u001b[39m'\u001b[39m, seasonal_periods\u001b[39m=\u001b[39m\u001b[39m24\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/macbookpro/Documents/predict_publications/publications_prediction/research/exponential_smoothening.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m fit \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro/Documents/predict_publications/publications_prediction/research/exponential_smoothening.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# 3. Forecast for the future periods\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro/Documents/predict_publications/publications_prediction/research/exponential_smoothening.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m forecast_values \u001b[39m=\u001b[39m fit\u001b[39m.\u001b[39mforecast(steps\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(y_test))\n",
      "File \u001b[0;32m~/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/pandas/util/_decorators.py:210\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    209\u001b[0m     kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 210\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/pandas/util/_decorators.py:210\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    209\u001b[0m     kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 210\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/pandas/util/_decorators.py:210\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    209\u001b[0m     kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 210\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/statsmodels/tsa/holtwinters/model.py:1143\u001b[0m, in \u001b[0;36mExponentialSmoothing.fit\u001b[0;34m(self, smoothing_level, smoothing_trend, smoothing_seasonal, damping_trend, optimized, remove_bias, start_params, method, minimize_kwargs, use_brute, use_boxcox, use_basinhopping, initial_level, initial_trend)\u001b[0m\n\u001b[1;32m   1141\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSLSQP\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m method\n\u001b[1;32m   1142\u001b[0m \u001b[39mif\u001b[39;00m optimized:\n\u001b[0;32m-> 1143\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimize_parameters(\n\u001b[1;32m   1144\u001b[0m         res, use_brute, method, minimize_kwargs\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[1;32m   1146\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1147\u001b[0m     l0, b0, s0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_values(\n\u001b[1;32m   1148\u001b[0m         initial_level\u001b[39m=\u001b[39minitial_level, initial_trend\u001b[39m=\u001b[39minitial_trend\n\u001b[1;32m   1149\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/statsmodels/tsa/holtwinters/model.py:839\u001b[0m, in \u001b[0;36mExponentialSmoothing._optimize_parameters\u001b[0;34m(self, data, use_brute, method, kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m bounds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(orig_bounds[:\u001b[39m3\u001b[39m], dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[1;32m    836\u001b[0m hw_args \u001b[39m=\u001b[39m HoltWintersArgs(\n\u001b[1;32m    837\u001b[0m     sel\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m), params, bounds, y, m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnobs\n\u001b[1;32m    838\u001b[0m )\n\u001b[0;32m--> 839\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_starting_values(\n\u001b[1;32m    840\u001b[0m     params,\n\u001b[1;32m    841\u001b[0m     start_params,\n\u001b[1;32m    842\u001b[0m     use_brute,\n\u001b[1;32m    843\u001b[0m     sel,\n\u001b[1;32m    844\u001b[0m     hw_args,\n\u001b[1;32m    845\u001b[0m     bounds,\n\u001b[1;32m    846\u001b[0m     init_alpha,\n\u001b[1;32m    847\u001b[0m     func,\n\u001b[1;32m    848\u001b[0m )\n\u001b[1;32m    850\u001b[0m \u001b[39m# We always use [0, 1] for a, b and g and handle transform inside\u001b[39;00m\n\u001b[1;32m    851\u001b[0m mod_bounds \u001b[39m=\u001b[39m [(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)] \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m \u001b[39m+\u001b[39m orig_bounds[\u001b[39m3\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/statsmodels/tsa/holtwinters/model.py:747\u001b[0m, in \u001b[0;36mExponentialSmoothing._get_starting_values\u001b[0;34m(self, params, start_params, use_brute, sel, hw_args, bounds, alpha, func)\u001b[0m\n\u001b[1;32m    745\u001b[0m best_params \u001b[39m=\u001b[39m points[\u001b[39m0\u001b[39m]\n\u001b[1;32m    746\u001b[0m \u001b[39mfor\u001b[39;00m point \u001b[39min\u001b[39;00m points:\n\u001b[0;32m--> 747\u001b[0m     val \u001b[39m=\u001b[39m opt(point, hw_args)\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m val \u001b[39m<\u001b[39m best_val:\n\u001b[1;32m    749\u001b[0m         best_params \u001b[39m=\u001b[39m point\n",
      "File \u001b[0;32m~/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/statsmodels/tsa/holtwinters/model.py:81\u001b[0m, in \u001b[0;36mopt_wrapper.<locals>.f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 81\u001b[0m     err \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(err, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m     83\u001b[0m         \u001b[39mreturn\u001b[39;00m err\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m err\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train is a pandas series with a datetime index, if not, we will need to adjust.\n",
    "# If y_train is not defined in this context, please provide it.\n",
    "\n",
    "# 1. Decompose the training data to observe trend and seasonality\n",
    "decomposition = sm.tsa.seasonal_decompose(y_train, model='additive', period=24)  # assuming hourly data\n",
    "fig = decomposition.plot()\n",
    "\n",
    "# 2. Train the Exponential Smoothing model\n",
    "# We will use additive trend and seasonality as it's common for this kind of data. \n",
    "# The seasonal period is set to 24, assuming the data is hourly.\n",
    "model = ExponentialSmoothing(y_train, trend='add', seasonal='add', seasonal_periods=24)\n",
    "fit = model.fit()\n",
    "\n",
    "# 3. Forecast for the future periods\n",
    "forecast_values = fit.forecast(steps=len(y_test))\n",
    "\n",
    "# 4. Evaluate the forecasts\n",
    "mse_es = mean_squared_error(y_test, forecast_values)\n",
    "rmse_es = np.sqrt(mse_es)\n",
    "\n",
    "mse_es, rmse_es\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_publications_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
