{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/predict_publications/publications_prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration related to model training\n",
    "model_training:\n",
    "  # Directory where model training results and artifacts are stored\n",
    "  root_dir: artifacts/model_trainer\n",
    "  \n",
    "  # Path to the train data\n",
    "  train_data_path: artifacts/data_transformation/train_data.csv\n",
    "\n",
    "  # Path to the test data\n",
    "  test_data_path: artifacts/data_transformation/test_data.csv\n",
    "\n",
    "  # Path to save our model\n",
    "  model_name: model.joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the model training process.\n",
    "    \n",
    "    This configuration class captures the necessary paths, directories, \n",
    "    and hyperparameters required for training the model.\n",
    "    \n",
    "    Attributes:\n",
    "    - root_dir: Directory for storing trained model and related artifacts.\n",
    "    - train_data_path: Path to the training data.\n",
    "    - test_data_path: Path to the testing/validation data.\n",
    "    - model_name: Name (or path) to save the trained model.\n",
    "    - target_column: The column name of the target variable.\n",
    "    - n_estimators: Number of boosting stages.\n",
    "    - max_depth: Maximum depth of the individual regression estimators.\n",
    "    - learning_rate: Step size for updating weights.\n",
    "    - subsample: Fraction of samples used for fitting individual base learners.\n",
    "    - random_state: Seed for reproducibility.\n",
    "    - max_features: The number of features to consider for best split.\n",
    "    - min_samples_split: Minimum number of samples required to split an internal node.\n",
    "    - min_samples_leaf: Minimum number of samples required at a leaf node.\n",
    "    \"\"\"\n",
    "    \n",
    "    root_dir: Path  # Directory for storing model training results and related artifacts\n",
    "    train_data_path: Path  # Path to train data\n",
    "    test_data_path: Path  # Path to test data\n",
    "    model_name: str  # Name or path where the trained model should be saved\n",
    "    target_column: str  # The target column in the dataset\n",
    "    n_estimators: int  # Number of boosting stages\n",
    "    max_depth: int  # Maximum depth of the regression estimators\n",
    "    learning_rate: float  # Learning rate\n",
    "    random_state: int  # Seed for reproducibility\n",
    "    subsample: float  # Fraction of samples for fitting individual base learners\n",
    "    max_features: str  # Number of features to consider for best split\n",
    "    min_samples_split: int  # Min samples required to split an internal node\n",
    "    min_samples_leaf: int  # Min samples required at a leaf node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup Params.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "GradientBoostingRegressor:\n",
    "  # Number of boosting stages to run. More might improve accuracy, but will also increase training time.\n",
    "  n_estimators: 150\n",
    "\n",
    "  # Maximum depth of the individual regression estimators. Helps in making the model more complex. \n",
    "  # Avoid setting it too high, as it might overfit.\n",
    "  max_depth: 4\n",
    "\n",
    "  # Controls the step size in the wrong direction that each tree correction should make. \n",
    "  # Smaller values might improve accuracy but will require more boosting stages.\n",
    "  learning_rate: 0.05\n",
    "\n",
    "  # The fraction of samples used for fitting the individual base learners. \n",
    "  # Values less than 1.0 can reduce variance and overfitting.\n",
    "  subsample: 0.8\n",
    "\n",
    "  # A seed for reproducibility.\n",
    "  random_state: 42\n",
    "\n",
    "  # The number of features to consider when looking for the best split. \n",
    "  # Using a smaller value can create more diverse trees, but might reduce accuracy.\n",
    "  max_features: \"sqrt\"\n",
    "\n",
    "  # Minimum number of samples required to split an internal node. Can be used to control over-fitting.\n",
    "  min_samples_split: 2\n",
    "\n",
    "  # Minimum number of samples required to be at a leaf node. Can be used to control over-fitting.\n",
    "  min_samples_leaf: 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Transformed Schema.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "schema_type: \"transformed\"\n",
    "description: \"Schema of the transformed data after feature engineering and aggregation.\"\n",
    "\n",
    "columns:\n",
    "  timestamp: \n",
    "    type: datetime64[ns]\n",
    "    description: \"Timestamp of the data entry.\"\n",
    "  lon: \n",
    "    type: float64\n",
    "    description: \"Longitude value.\"\n",
    "  lat: \n",
    "    type: float64\n",
    "    description: \"Latitude value.\"\n",
    "  hour:\n",
    "    type: int64\n",
    "    description: \"Hour extracted from the timestamp.\"\n",
    "  day:\n",
    "    type: int64\n",
    "    description: \"Day extracted from the timestamp.\"\n",
    "  dayofweek:\n",
    "    type: int64\n",
    "    description: \"Day of the week extracted from the timestamp.\"\n",
    "  month:\n",
    "    type: int64\n",
    "    description: \"Month extracted from the timestamp.\"\n",
    "  likescount: \n",
    "    type: float64\n",
    "    description: \"Mean count of likes for the aggregated period.\"\n",
    "  commentscount: \n",
    "    type: float64\n",
    "    description: \"Mean count of comments for the aggregated period.\"\n",
    "  symbols_cnt: \n",
    "    type: float64\n",
    "    description: \"Mean count of symbols for the aggregated period.\"\n",
    "  words_cnt: \n",
    "    type: float64\n",
    "    description: \"Mean count of words for the aggregated period.\"\n",
    "  hashtags_cnt: \n",
    "    type: float64\n",
    "    description: \"Mean count of hashtags for the aggregated period.\"\n",
    "  mentions_cnt: \n",
    "    type: float64\n",
    "    description: \"Mean count of mentions for the aggregated period.\"\n",
    "  links_cnt: \n",
    "    type: float64\n",
    "    description: \"Mean count of links for the aggregated period.\"\n",
    "  emoji_cnt: \n",
    "    type: float64\n",
    "    description: \"Mean count of emojis for the aggregated period.\"\n",
    "  publication_count:\n",
    "    type: int64\n",
    "    description: \"Count of publications for the aggregated period.\"\n",
    "\n",
    "target_column: 'publication_count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicting_publications.constants import *\n",
    "from predicting_publications.utils.common import read_yaml, create_directories\n",
    "from predicting_publications import logger\n",
    "from predicting_publications.entity.config_entity import (DataIngestionConfig, \n",
    "                                                          DataValidationConfig,\n",
    "                                                          DataTransformationConfig)\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager manages configurations needed for the data pipeline.\n",
    "\n",
    "    The class reads configuration, parameter, and schema settings from specified files\n",
    "    and provides a set of methods to access these settings. It also takes care of\n",
    "    creating necessary directories defined in the configurations.\n",
    "\n",
    "    Attributes:\n",
    "    - config (dict): Configuration settings.\n",
    "    - params (dict): Parameters for the pipeline.\n",
    "    - schema (dict): Schema information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH, \n",
    "                 params_filepath = PARAMS_FILE_PATH, \n",
    "                 schema_filepath = SCHEMA_FILE_PATH,\n",
    "                 feature_schema_filepath = FEATURE_SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"initial_schema\")\n",
    "        self.feature_schema_filepath = self._read_config_file(feature_schema_filepath, \"feature_engineered_schema\")\n",
    "\n",
    "        # Create the directory for storing artifacts if it doesn't exist\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data ingestion configurations as a DataIngestionConfig object.\n",
    "\n",
    "        This method fetches settings related to data ingestion, like directories and file paths,\n",
    "        and returns them as a DataIngestionConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataIngestionConfig: Object containing data ingestion configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_ingestion' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_ingestion\n",
    "            # Create the root directory for data ingestion if it doesn't already exist\n",
    "            create_directories([config.root_dir])\n",
    "            \n",
    "            return DataIngestionConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                local_data_file=Path(config.local_data_file),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            logger.error(\"The 'data_ingestion' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data validation configurations as a DataValidationConfig object.\n",
    "\n",
    "        This method fetches settings related to data validation, like directories, file paths,\n",
    "        and schema, and returns them as a DataValidationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataValidationConfig: Object containing data validation configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_validation' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract data validation configurations\n",
    "            config = self.config.data_validation\n",
    "            \n",
    "            # Extract schema for data validation\n",
    "            schema = self.schema.columns\n",
    "            \n",
    "            # Ensure the parent directory for the status file exists\n",
    "            create_directories([os.path.dirname(config.status_file)])\n",
    "\n",
    "            \n",
    "            # Construct and return the DataValidationConfig object\n",
    "            return DataValidationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                status_file=Path(config.status_file),\n",
    "                initial_schema=schema\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_validation' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data transformation configurations as a DataTransformationConfig object.\n",
    "\n",
    "        This method fetches settings related to data transformation, like directories and file paths,\n",
    "        and returns them as a DataTransformationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataTransformationConfig: Object containing data transformation configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_transformation' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_transformation\n",
    "            \n",
    "            # Ensure the root directory for data transformation exists\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            # Construct and return the DataTransformationConfig object\n",
    "            return DataTransformationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                data_validation=Path(config.data_validation),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_transformation' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        \"\"\"\n",
    "        Extract and return model training configurations as a ModelTrainerConfig object.\n",
    "\n",
    "        This method fetches settings related to model training, like directories, file paths,\n",
    "        and hyperparameters, and returns them as a ModelTrainerConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - ModelTrainerConfig: Object containing model training configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the necessary attributes do not exist in the config or params files.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.model_training\n",
    "            params = self.params.GradientBoostingRegressor\n",
    "            \n",
    "            # The feature schema is a dictionary, extracting the target column\n",
    "            target_col = self.feature_schema_filepath.get(\"target_column\", \"\")\n",
    "\n",
    "\n",
    "            # Ensure the root directory for model training exists\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            # Construct and return the ModelTrainerConfig object\n",
    "            return ModelTrainerConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                train_data_path=Path(config.train_data_path),\n",
    "                test_data_path=Path(config.test_data_path),\n",
    "                model_name=config.model_name,\n",
    "                target_column=target_col,\n",
    "                n_estimators=params.n_estimators,\n",
    "                max_depth=params.max_depth,\n",
    "                learning_rate=params.learning_rate,\n",
    "                random_state=params.random_state,\n",
    "                subsample=params.subsample,\n",
    "                max_features=params.max_features,\n",
    "                min_samples_split=params.min_samples_split,\n",
    "                min_samples_leaf=params.min_samples_leaf\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"An expected attribute does not exist in the config or params files.\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicting_publications import logger\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import joblib\n",
    "import os \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    ModelTrainer class handles the training of the GradientBoostingRegressor model.\n",
    "\n",
    "    This component reads in the transformed training and test data, trains a Gradient \n",
    "    Boosting Regressor model using the specified hyperparameters, and saves the trained \n",
    "    model to the specified path.\n",
    "\n",
    "    Attributes:\n",
    "    - config (ModelTrainerConfig): Configuration settings for the model training process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        \"\"\"\n",
    "        Initialize ModelTrainer with the given configurations.\n",
    "\n",
    "        Args:\n",
    "        - config (ModelTrainerConfig): Configuration settings for model training.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def hyperparameter_tuning(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning using RandomizedSearchCV.\n",
    "        \n",
    "        Args:\n",
    "        - X_train: Training data features.\n",
    "        - y_train: Training data target.\n",
    "\n",
    "        Returns:\n",
    "        - Best hyperparameters found during the search.\n",
    "        \"\"\"\n",
    "        param_dist = {\n",
    "            'n_estimators': sp_randint(50, 200),\n",
    "            'max_depth': sp_randint(1, 10),\n",
    "            'learning_rate': sp_uniform(0.01, 0.2),\n",
    "            'subsample': sp_uniform(0.5, 0.5),\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'min_samples_split': sp_randint(2, 20),\n",
    "            'min_samples_leaf': sp_randint(1, 20)\n",
    "        }\n",
    "\n",
    "        n_iter_search = 10  # Reduced to 10 combinations\n",
    "        random_search = RandomizedSearchCV(GradientBoostingRegressor(),\n",
    "                                           param_distributions=param_dist,\n",
    "                                           n_iter=n_iter_search,\n",
    "                                           cv=3,  # Reduced to 3-fold cross-validation\n",
    "                                           scoring='neg_mean_squared_error',\n",
    "                                           verbose=2,\n",
    "                                           n_jobs=-1)\n",
    "\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        return random_search.best_params_\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train a Gradient Boosting Regressor model.\n",
    "\n",
    "        This method:\n",
    "        1. Loads the training and test data from the paths specified in the configuration.\n",
    "        2. Separates the predictors and target variables.\n",
    "        3. Initializes a Gradient Boosting Regressor model with the specified hyperparameters.\n",
    "        4. Fits the model on the training data.\n",
    "        5. Saves the trained model to the path specified in the configuration.\n",
    "        \"\"\"\n",
    "        # Load training dataset\n",
    "        train_data = pd.read_csv(self.config.train_data_path)\n",
    "\n",
    "        # Separate predictors and target variable\n",
    "        X_train = train_data.drop([self.config.target_column], axis=1)\n",
    "        y_train = train_data[[self.config.target_column]].values.ravel()\n",
    "\n",
    "        # Perform hyperparameter tuning\n",
    "        # best_params = self.hyperparameter_tuning(X_train, y_train)\n",
    "\n",
    "        # Log the best parameters\n",
    "        # logger.info(f\"Best hyperparameters found: {best_params}\")\n",
    "\n",
    "        # Best hyperparameters\n",
    "        best_params = {\n",
    "            'learning_rate': self.config.learning_rate,\n",
    "            'max_depth': self.config.max_depth,\n",
    "            'max_features': None if self.config.max_features == 'None' else self.config.max_features,\n",
    "            'min_samples_leaf': self.config.min_samples_leaf,\n",
    "            'min_samples_split': self.config.min_samples_split,\n",
    "            'n_estimators': self.config.n_estimators,\n",
    "            'subsample': self.config.subsample,\n",
    "            'random_state': self.config.random_state\n",
    "        }\n",
    "\n",
    "        # Train the model with the best parameters\n",
    "        gb_model = GradientBoostingRegressor(**best_params)\n",
    "        gb_model.fit(X_train, y_train)\n",
    "\n",
    "        # Save the trained model\n",
    "        model_save_path = os.path.join(self.config.root_dir, self.config.model_name)\n",
    "        joblib.dump(gb_model, model_save_path)\n",
    "        logger.info(f\"Model saved successfully to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-16 22:46:52,433: 42: predict_publications_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2023-10-16 22:46:52,437: 42: predict_publications_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2023-10-16 22:46:52,441: 42: predict_publications_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2023-10-16 22:46:52,445: 42: predict_publications_logger: INFO: common:  yaml file: feature_engineered_schema.yaml loaded successfully]\n",
      "[2023-10-16 22:46:52,447: 65: predict_publications_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2023-10-16 22:46:52,448: 53: predict_publications_logger: INFO: 2940486009:  Starting the Model Training Pipeline.]\n",
      "[2023-10-16 22:46:52,449: 54: predict_publications_logger: INFO: 2940486009:  >>>>>> Stage: Model Training Pipeline started <<<<<<]\n",
      "[2023-10-16 22:46:52,449: 30: predict_publications_logger: INFO: 2940486009:  Fetching model training configuration...]\n",
      "[2023-10-16 22:46:52,450: 65: predict_publications_logger: INFO: common:  Created directory at: artifacts/model_trainer]\n",
      "[2023-10-16 22:46:52,453: 33: predict_publications_logger: INFO: 2940486009:  Initializing model training process...]\n",
      "[2023-10-16 22:46:52,454: 36: predict_publications_logger: INFO: 2940486009:  Executing model training...]\n",
      "[2023-10-16 23:07:56,903: 109: predict_publications_logger: INFO: 3229952103:  Model saved successfully to artifacts/model_trainer/model.joblib]\n",
      "[2023-10-16 23:07:56,940: 39: predict_publications_logger: INFO: 2940486009:  Model Training Pipeline completed successfully.]\n",
      "[2023-10-16 23:07:56,942: 56: predict_publications_logger: INFO: 2940486009:  >>>>>> Stage Model Training Pipeline completed <<<<<< \n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "from predicting_publications import logger\n",
    "\n",
    "class ModelTrainerPipeline:\n",
    "    \"\"\"\n",
    "    This pipeline handles the model training process.\n",
    "\n",
    "    After the data transformation stage, this class orchestrates the training of the model\n",
    "    using the GradientBoostingRegressor and saves the trained model for future use.\n",
    "\n",
    "    Attributes:\n",
    "        STAGE_NAME (str): The name of this pipeline stage.\n",
    "    \"\"\"\n",
    "    \n",
    "    STAGE_NAME = \"Model Training Pipeline\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the pipeline with a configuration manager.\n",
    "        \"\"\"\n",
    "        self.config_manager = ConfigurationManager()\n",
    "\n",
    "    def run_model_training(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the model training process.\n",
    "\n",
    "        Fetches configurations, initializes the model training process, trains the model,\n",
    "        and logs the successful completion of the training.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Fetching model training configuration...\")\n",
    "            model_training_configuration = self.config_manager.get_model_trainer_config()\n",
    "\n",
    "            logger.info(\"Initializing model training process...\")\n",
    "            model_training = ModelTrainer(config=model_training_configuration)\n",
    "\n",
    "            logger.info(\"Executing model training...\")\n",
    "            model_training.train()\n",
    "\n",
    "            logger.info(\"Model Training Pipeline completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the model training: {e}\")\n",
    "\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the entire Model Training Pipeline.\n",
    "\n",
    "        This method orchestrates the process of model training and provides logs for each stage \n",
    "        of the pipeline.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting the Model Training Pipeline.\")\n",
    "            logger.info(f\">>>>>> Stage: {ModelTrainerPipeline.STAGE_NAME} started <<<<<<\")\n",
    "            self.run_model_training()\n",
    "            logger.info(f\">>>>>> Stage {ModelTrainerPipeline.STAGE_NAME} completed <<<<<< \\n\\nx==========x\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the {ModelTrainerPipeline.STAGE_NAME}: {e}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = ModelTrainerPipeline()\n",
    "    pipeline.run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_publications_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
