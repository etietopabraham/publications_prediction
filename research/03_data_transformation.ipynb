{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/predict_publications/publications_prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration related to data transformation\n",
    "data_transformation:\n",
    "  # Directory where data transformation results and artifacts are stored\n",
    "  root_dir: artifacts/data_transformation\n",
    "  \n",
    "  # Path to the ingested data file that will be used for validation\n",
    "  data_source_file: artifacts/data_ingestion/train_data.csv\n",
    "\n",
    "  # Path to data validation status\n",
    "  data_validation: artifacts/initial_data_validation/status.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Update Schema (Not required in this stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup Params (Not Required at this stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the data transformation process.\n",
    "    \n",
    "    This configuration class captures the necessary paths and directories \n",
    "    required for the transformation of data post-ingestion and pre-model training.\n",
    "    \n",
    "    Attributes:\n",
    "    - root_dir: Directory where data transformation results and artifacts are stored.\n",
    "    - data_source_file: Path to the file where the ingested data is stored that needs to be transformed.\n",
    "    \"\"\"\n",
    "    \n",
    "    root_dir: Path  # Directory for storing transformation results and related artifacts\n",
    "    data_source_file: Path  # Path to the ingested data file for transformation\n",
    "    data_validation: Path # Path to the validated output file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Setup Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicting_publications.constants import *\n",
    "from predicting_publications.utils.common import read_yaml, create_directories\n",
    "from predicting_publications import logger\n",
    "from predicting_publications.entity.config_entity import (DataIngestionConfig, \n",
    "                                                          DataValidationConfig)\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager manages configurations needed for the data pipeline.\n",
    "\n",
    "    The class reads configuration, parameter, and schema settings from specified files\n",
    "    and provides a set of methods to access these settings. It also takes care of\n",
    "    creating necessary directories defined in the configurations.\n",
    "\n",
    "    Attributes:\n",
    "    - config (dict): Configuration settings.\n",
    "    - params (dict): Parameters for the pipeline.\n",
    "    - schema (dict): Schema information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH, \n",
    "                 params_filepath = PARAMS_FILE_PATH, \n",
    "                 schema_filepath = SCHEMA_FILE_PATH,\n",
    "                 feature_schema_filepath = FEATURE_SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"initial_schema\")\n",
    "        self.feature_schema_filepath = self._read_config_file(feature_schema_filepath, \"feature_engineered_schema\")\n",
    "\n",
    "        # Create the directory for storing artifacts if it doesn't exist\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data ingestion configurations as a DataIngestionConfig object.\n",
    "\n",
    "        This method fetches settings related to data ingestion, like directories and file paths,\n",
    "        and returns them as a DataIngestionConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataIngestionConfig: Object containing data ingestion configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_ingestion' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_ingestion\n",
    "            # Create the root directory for data ingestion if it doesn't already exist\n",
    "            create_directories([config.root_dir])\n",
    "            \n",
    "            return DataIngestionConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                local_data_file=Path(config.local_data_file),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            logger.error(\"The 'data_ingestion' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data validation configurations as a DataValidationConfig object.\n",
    "\n",
    "        This method fetches settings related to data validation, like directories, file paths,\n",
    "        and schema, and returns them as a DataValidationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataValidationConfig: Object containing data validation configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_validation' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract data validation configurations\n",
    "            config = self.config.data_validation\n",
    "            \n",
    "            # Extract schema for data validation\n",
    "            schema = self.schema.columns\n",
    "            \n",
    "            # Ensure the parent directory for the status file exists\n",
    "            create_directories([os.path.dirname(config.status_file)])\n",
    "\n",
    "            \n",
    "            # Construct and return the DataValidationConfig object\n",
    "            return DataValidationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                status_file=Path(config.status_file),\n",
    "                initial_schema=schema\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_validation' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data transformation configurations as a DataTransformationConfig object.\n",
    "\n",
    "        This method fetches settings related to data transformation, like directories and file paths,\n",
    "        and returns them as a DataTransformationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataTransformationConfig: Object containing data transformation configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_transformation' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_transformation\n",
    "            \n",
    "            # Ensure the root directory for data transformation exists\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            # Construct and return the DataTransformationConfig object\n",
    "            return DataTransformationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                data_validation=Path(config.data_validation),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_transformation' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicting_publications import logger\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "class DataTransformation:\n",
    "    \"\"\"\n",
    "    Handles the transformation of the ingested dataset, generating temporal features, \n",
    "    aggregating the data, and splitting it into training and validation sets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataTransformation component by reading the data source file \n",
    "        specified in the config.\n",
    "\n",
    "        Args:\n",
    "        - config (DataTransformationConfig): Configuration settings for data transformation.\n",
    "\n",
    "        Attributes:\n",
    "        - df (pd.DataFrame): The data to be transformed.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.config.data_source_file)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {self.config.data_source_file}\")\n",
    "            raise\n",
    "\n",
    "    def generate_temporal_features_and_aggregate(self):\n",
    "        # Convert the 'timestamp' column to a datetime format if it's not already\n",
    "        if self.df['timestamp'].dtype != 'datetime64[ns]':\n",
    "            self.df['timestamp'] = pd.to_datetime(self.df['timestamp'], unit='s')\n",
    "\n",
    "        \"\"\"\n",
    "        Generate temporal features and aggregate the dataset.\n",
    "        \"\"\"\n",
    "        # Generating temporal features\n",
    "        self.df['hour'] = self.df['timestamp'].dt.hour\n",
    "        self.df['day'] = self.df['timestamp'].dt.day\n",
    "        self.df['dayofweek'] = self.df['timestamp'].dt.dayofweek\n",
    "        self.df['month'] = self.df['timestamp'].dt.month\n",
    "\n",
    "        # Aggregating data by hour and location\n",
    "        agg_columns = {\n",
    "            'likescount': 'mean',\n",
    "            'commentscount': 'mean',\n",
    "            'symbols_cnt': 'mean',\n",
    "            'words_cnt': 'mean',\n",
    "            'hashtags_cnt': 'mean',\n",
    "            'mentions_cnt': 'mean',\n",
    "            'links_cnt': 'mean',\n",
    "            'emoji_cnt': 'mean'\n",
    "        }\n",
    "\n",
    "        logger.info(\"Grouping data by timestamp, lon, lat, hour, day, day of week, and month\")\n",
    "        self.grouped_data = self.df.groupby(['timestamp', 'lon', 'lat', 'hour', 'day', 'dayofweek', 'month']).agg(agg_columns).reset_index()\n",
    "        \n",
    "        logger.info(\"Setting publication count grouped by timestamp, lon, and lat\")\n",
    "        self.grouped_data['publication_count'] = self.df.groupby(['timestamp', 'lon', 'lat']).size().values\n",
    "\n",
    "    def split_data_into_train_and_test(self):\n",
    "        \"\"\"\n",
    "        Split the aggregated data into training and test sets.\n",
    "        \"\"\"\n",
    "        # Drop 'timestamp' as it's strongly correlated with other time features and may cause data leakage\n",
    "        X = self.grouped_data.drop(['publication_count', 'timestamp'], axis=1)\n",
    "        y = self.grouped_data['publication_count']\n",
    "\n",
    "        # Split the data into training and validation sets and set them as class attributes\n",
    "        logger.info(\"Splitting data into train and test values\")\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        logger.info(f\"Training data shape: {self.X_train.shape}, Validation data shape: {self.X_val.shape}\")\n",
    "        print(f\"Training data shape: {self.X_train.shape}, Validation data shape: {self.X_val.shape}\")\n",
    "\n",
    "    def _save_datasets(self, train_filename: str, test_filename: str):\n",
    "        \"\"\"\n",
    "        Save the train and test datasets to the output path specified in the configuration.\n",
    "\n",
    "        Args:\n",
    "        - train_filename (str): Name of the file to save the training data.\n",
    "        - test_filename (str): Name of the file to save the test data.\n",
    "        \"\"\"\n",
    "        train_output_path = self.config.root_dir / train_filename\n",
    "        test_output_path = self.config.root_dir / test_filename\n",
    "        \n",
    "        try:\n",
    "            # Save training data\n",
    "            train_data = pd.concat([self.X_train, self.y_train], axis=1)\n",
    "            train_data.to_csv(train_output_path, index=False)\n",
    "            logger.info(f\"Training Data saved successfully to {train_output_path}\")\n",
    "\n",
    "            # Save test data\n",
    "            test_data = pd.concat([self.X_val, self.y_val], axis=1)\n",
    "            test_data.to_csv(test_output_path, index=False)\n",
    "            logger.info(f\"Test Data saved successfully to {test_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while saving the datasets: {e}\")\n",
    "            raise\n",
    "\n",
    "    def orchestrate_transformation(self, train_filename: str = \"train_data.csv\", test_filename: str = \"test_data.csv\"):\n",
    "        \"\"\"\n",
    "        Orchestrates the data transformation process by:\n",
    "        1. Generating temporal features and aggregating the data.\n",
    "        2. Splitting data into training and test sets.\n",
    "        3. Saving the training and test datasets.\n",
    "\n",
    "        Args:\n",
    "        - train_filename (str): Name of the file to save the training data. Default is \"train_data.csv\".\n",
    "        - test_filename (str): Name of the file to save the test data. Default is \"test_data.csv\".\n",
    "        \"\"\"\n",
    "        self.generate_temporal_features_and_aggregate()\n",
    "        self.split_data_into_train_and_test()\n",
    "        self._save_datasets(train_filename, test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-16 17:09:02,459: 42: predict_publications_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2023-10-16 17:09:02,461: 42: predict_publications_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2023-10-16 17:09:02,464: 42: predict_publications_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2023-10-16 17:09:02,465: 42: predict_publications_logger: INFO: common:  yaml file: feature_engineered_schema.yaml loaded successfully]\n",
      "[2023-10-16 17:09:02,467: 65: predict_publications_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2023-10-16 17:09:02,468: 65: predict_publications_logger: INFO: common:  Created directory at: artifacts/data_transformation]\n",
      "[2023-10-16 17:09:02,468: 57: predict_publications_logger: INFO: 3209470565:  Starting the Data Transformation Pipeline.]\n",
      "[2023-10-16 17:09:02,469: 58: predict_publications_logger: INFO: 3209470565:  >>>>>> Stage: Data Transformation Pipeline started <<<<<<]\n",
      "[2023-10-16 17:09:02,469: 31: predict_publications_logger: INFO: 3209470565:  Fetching data transformation configuration...]\n",
      "[2023-10-16 17:09:02,470: 65: predict_publications_logger: INFO: common:  Created directory at: artifacts/data_transformation]\n",
      "[2023-10-16 17:09:02,470: 34: predict_publications_logger: INFO: 3209470565:  Initializing data transformation process...]\n",
      "[2023-10-16 17:09:09,055: 37: predict_publications_logger: INFO: 3209470565:  Executing data transformation...]\n",
      "[2023-10-16 17:09:09,703: 56: predict_publications_logger: INFO: 645990762:  Grouping data by timestamp, lon, lat, hour, day, day of week, and month]\n",
      "[2023-10-16 17:09:11,485: 59: predict_publications_logger: INFO: 645990762:  Setting publication count grouped by timestamp, lon, and lat]\n",
      "[2023-10-16 17:09:12,547: 71: predict_publications_logger: INFO: 645990762:  Splitting data into train and test values]\n",
      "[2023-10-16 17:09:13,174: 73: predict_publications_logger: INFO: 645990762:  Training data shape: (2908432, 14), Validation data shape: (727109, 14)]\n",
      "Training data shape: (2908432, 14), Validation data shape: (727109, 14)\n",
      "[2023-10-16 17:09:24,866: 91: predict_publications_logger: INFO: 645990762:  Training Data saved successfully to artifacts/data_transformation/train_data.csv]\n",
      "[2023-10-16 17:09:27,910: 96: predict_publications_logger: INFO: 645990762:  Test Data saved successfully to artifacts/data_transformation/test_data.csv]\n",
      "[2023-10-16 17:09:27,920: 40: predict_publications_logger: INFO: 3209470565:  Data Transformation Pipeline completed successfully.]\n",
      "[2023-10-16 17:09:27,988: 60: predict_publications_logger: INFO: 3209470565:  >>>>>> Stage Data Transformation Pipeline completed <<<<<< \n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "from predicting_publications import logger\n",
    "from pathlib import Path\n",
    "\n",
    "class DataTransformationPipeline:\n",
    "    \"\"\"\n",
    "    This pipeline handles the data transformation steps.\n",
    "\n",
    "    After the data validation stage, it's essential to transform the data before moving on \n",
    "    to model training. This class orchestrates the transformation by generating temporal \n",
    "    features, aggregating data, and splitting it into training and test sets.\n",
    "\n",
    "    Attributes:\n",
    "        STAGE_NAME (str): The name of this pipeline stage.\n",
    "    \"\"\"\n",
    "    \n",
    "    STAGE_NAME = \"Data Transformation Pipeline\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the pipeline with a configuration manager.\n",
    "        \"\"\"\n",
    "        self.config_manager = ConfigurationManager()\n",
    "\n",
    "    def run_data_transformation(self):\n",
    "        \"\"\"\n",
    "        Run the data transformation steps.\n",
    "        \n",
    "        This method orchestrates the data transformation functions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Fetching data transformation configuration...\")\n",
    "            data_transformation_config = self.config_manager.get_data_transformation_config()\n",
    "\n",
    "            logger.info(\"Initializing data transformation process...\")\n",
    "            data_transformation = DataTransformation(config=data_transformation_config)\n",
    "\n",
    "            logger.info(\"Executing data transformation...\")\n",
    "            data_transformation.orchestrate_transformation()\n",
    "\n",
    "            logger.info(\"Data Transformation Pipeline completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the data transformation: {e}\")\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the entire Data Transformation Pipeline.\n",
    "        \n",
    "        This method orchestrates the process of the data transformation and\n",
    "        provides logs for each stage of the pipeline.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.config_manager.get_data_transformation_config().data_validation, \"r\") as f:\n",
    "                content = f.read()\n",
    "\n",
    "            if \"Overall Validation Status: All validations passed.\" in content:\n",
    "                logger.info(\"Starting the Data Transformation Pipeline.\")\n",
    "                logger.info(f\">>>>>> Stage: {DataTransformationPipeline.STAGE_NAME} started <<<<<<\")\n",
    "                self.run_data_transformation()\n",
    "                logger.info(f\">>>>>> Stage {DataTransformationPipeline.STAGE_NAME} completed <<<<<< \\n\\nx==========x\")\n",
    "            else:\n",
    "                logger.error(\"Data Transformation Pipeline aborted due to validation errors.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the {DataTransformationPipeline.STAGE_NAME}: {e}\")\n",
    "            raise e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = DataTransformationPipeline()\n",
    "    pipeline.run_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_publications_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
