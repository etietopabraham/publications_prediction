{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/predict_publications/publications_prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Export MLFLow Env Variables in Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export MLFLOW_TRACKING_URI=https://dagshub.com/etietopabraham/publications_prediction.mlflow\n",
    "export MLFLOW_TRACKING_USERNAME=etietopabraham \n",
    "export MLFLOW_TRACKING_PASSWORD=324bb2aaa6fc82dbfce509eac2ce2cd6a016a869"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Export MLFlow Env Variables for Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/etietopabraham/publications_prediction.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"etietopabraham\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"324bb2aaa6fc82dbfce509eac2ce2cd6a016a869\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration for Model Evaluation\n",
    "\n",
    "model_evaluation:\n",
    "  # Root directory for saving model evaluation artifacts\n",
    "  root_dir: artifacts/model_evaluation\n",
    "  \n",
    "  # Path to the test data used for evaluation\n",
    "  test_data_path: artifacts/data_transformation/test_data.csv\n",
    "  \n",
    "  # Path to the trained model saved during the training step\n",
    "  model_path: artifacts/model_trainer/model.joblib\n",
    "  \n",
    "  # Path to save the evaluation metrics in JSON format\n",
    "  metric_file_name: artifacts/model_evaluation/metrics.json\n",
    "\n",
    "  # MLFlow URI\n",
    "  mlflow_uri: 'https://dagshub.com/etietopabraham/publications_prediction.mlflow'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    \"\"\"\n",
    "    Data class for storing configuration related to model evaluation.\n",
    "\n",
    "    Attributes:\n",
    "    - root_dir: Root directory for saving model evaluation artifacts.\n",
    "    - test_data_path: Path to the test data used for evaluation.\n",
    "    - model_path: Path to the trained model saved during the training step.\n",
    "    - metric_file_name: Name (or path) to save the evaluation metrics.\n",
    "    - all_params: Dictionary containing other relevant parameters.\n",
    "    - target_column: Column name of the target variable in the dataset.\n",
    "    - mlflow_uri: URI for MLflow tracking server.\n",
    "\n",
    "    Note: The `frozen=True` argument makes instances of this class immutable, \n",
    "    ensuring that once an instance is created, its attributes cannot be modified.\n",
    "    \"\"\"\n",
    "\n",
    "    root_dir: Path          # Directory for saving model evaluation artifacts\n",
    "    test_data_path: Path    # Path to the test dataset\n",
    "    model_path: Path        # Path to the saved model\n",
    "    metric_file_name: str   # Filename to save evaluation metrics\n",
    "    all_params: dict        # Other relevant parameters for evaluation\n",
    "    target_column: str      # Name of the target column in the dataset\n",
    "    mlflow_uri: str         # URI for MLflow tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicting_publications.constants import *\n",
    "from predicting_publications.utils.common import read_yaml, create_directories\n",
    "from predicting_publications import logger\n",
    "from predicting_publications.entity.config_entity import (DataIngestionConfig, \n",
    "                                                          DataValidationConfig,\n",
    "                                                          DataTransformationConfig,\n",
    "                                                          ModelTrainerConfig)\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager manages configurations needed for the data pipeline.\n",
    "\n",
    "    The class reads configuration, parameter, and schema settings from specified files\n",
    "    and provides a set of methods to access these settings. It also takes care of\n",
    "    creating necessary directories defined in the configurations.\n",
    "\n",
    "    Attributes:\n",
    "    - config (dict): Configuration settings.\n",
    "    - params (dict): Parameters for the pipeline.\n",
    "    - schema (dict): Schema information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH, \n",
    "                 params_filepath = PARAMS_FILE_PATH, \n",
    "                 schema_filepath = SCHEMA_FILE_PATH,\n",
    "                 feature_schema_filepath = FEATURE_SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"initial_schema\")\n",
    "        self.feature_schema_filepath = self._read_config_file(feature_schema_filepath, \"feature_engineered_schema\")\n",
    "\n",
    "        # Create the directory for storing artifacts if it doesn't exist\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data ingestion configurations as a DataIngestionConfig object.\n",
    "\n",
    "        This method fetches settings related to data ingestion, like directories and file paths,\n",
    "        and returns them as a DataIngestionConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataIngestionConfig: Object containing data ingestion configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_ingestion' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_ingestion\n",
    "            # Create the root directory for data ingestion if it doesn't already exist\n",
    "            create_directories([config.root_dir])\n",
    "            \n",
    "            return DataIngestionConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                local_data_file=Path(config.local_data_file),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            logger.error(\"The 'data_ingestion' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data validation configurations as a DataValidationConfig object.\n",
    "\n",
    "        This method fetches settings related to data validation, like directories, file paths,\n",
    "        and schema, and returns them as a DataValidationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataValidationConfig: Object containing data validation configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_validation' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract data validation configurations\n",
    "            config = self.config.data_validation\n",
    "            \n",
    "            # Extract schema for data validation\n",
    "            schema = self.schema.columns\n",
    "            \n",
    "            # Ensure the parent directory for the status file exists\n",
    "            create_directories([os.path.dirname(config.status_file)])\n",
    "\n",
    "            \n",
    "            # Construct and return the DataValidationConfig object\n",
    "            return DataValidationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                status_file=Path(config.status_file),\n",
    "                initial_schema=schema\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_validation' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data transformation configurations as a DataTransformationConfig object.\n",
    "\n",
    "        This method fetches settings related to data transformation, like directories and file paths,\n",
    "        and returns them as a DataTransformationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataTransformationConfig: Object containing data transformation configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_transformation' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_transformation\n",
    "            \n",
    "            # Ensure the root directory for data transformation exists\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            # Construct and return the DataTransformationConfig object\n",
    "            return DataTransformationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                data_validation=Path(config.data_validation),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_transformation' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        \"\"\"\n",
    "        Extract and return model training configurations as a ModelTrainerConfig object.\n",
    "\n",
    "        This method fetches settings related to model training, like directories, file paths,\n",
    "        and hyperparameters, and returns them as a ModelTrainerConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - ModelTrainerConfig: Object containing model training configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the necessary attributes do not exist in the config or params files.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.model_training\n",
    "            params = self.params.GradientBoostingRegressor\n",
    "            \n",
    "            # The feature schema is a dictionary, extracting the target column\n",
    "            target_col = self.feature_schema_filepath.get(\"target_column\", \"\")\n",
    "\n",
    "\n",
    "            # Ensure the root directory for model training exists\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            # Construct and return the ModelTrainerConfig object\n",
    "            return ModelTrainerConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                train_data_path=Path(config.train_data_path),\n",
    "                test_data_path=Path(config.test_data_path),\n",
    "                model_name=config.model_name,\n",
    "                target_column=target_col,\n",
    "                n_estimators=params.n_estimators,\n",
    "                max_depth=params.max_depth,\n",
    "                learning_rate=params.learning_rate,\n",
    "                random_state=params.random_state,\n",
    "                subsample=params.subsample,\n",
    "                max_features=params.max_features,\n",
    "                min_samples_split=params.min_samples_split,\n",
    "                min_samples_leaf=params.min_samples_leaf\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"An expected attribute does not exist in the config or params files.\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        \"\"\"\n",
    "        Retrieve the configuration related to model evaluation.\n",
    "\n",
    "        This method:\n",
    "        1. Extracts model evaluation configuration from the main configuration.\n",
    "        2. Extracts GradientBoostingRegressor parameters from the params configuration.\n",
    "        3. Retrieves the target column from the feature schema.\n",
    "        4. Ensures the root directory for saving model evaluation artifacts exists.\n",
    "        5. Constructs and returns a ModelEvaluationConfig object.\n",
    "\n",
    "        Returns:\n",
    "            ModelEvaluationConfig: Dataclass object containing configurations for model evaluation.\n",
    "\n",
    "        Raises:\n",
    "            AttributeError: If an expected attribute does not exist in the config or params files.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            config = self.config.model_evaluation\n",
    "            params = self.params.GradientBoostingRegressor\n",
    "\n",
    "            # Extract the target column from the feature schema\n",
    "            target_col = self.feature_schema_filepath.get(\"target_column\", \"\")\n",
    "\n",
    "            # Ensure the root directory for model evaluation exists\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            # Construct and return the ModelEvaluationConfig object\n",
    "            return ModelEvaluationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                test_data_path=Path(config.test_data_path),\n",
    "                model_path=config.model_path,\n",
    "                metric_file_name=config.metric_file_name,\n",
    "                all_params=params,\n",
    "                target_column=target_col,\n",
    "                mlflow_uri=config.mlflow_uri,\n",
    "            )\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"An expected attribute does not exist in the config or params files.\")\n",
    "            raise e \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import mlflow\n",
    "from predicting_publications.utils.common import save_json\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        \"\"\"\n",
    "        Initialize the ModelEvaluation class.\n",
    "\n",
    "        Parameters:\n",
    "        - config: Configuration parameters for model evaluation.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def eval_metrics(self, actual, pred):\n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics for the model predictions.\n",
    "\n",
    "        Parameters:\n",
    "        - actual: Ground truth values.\n",
    "        - pred: Predicted values by the model.\n",
    "\n",
    "        Returns:\n",
    "        - rmse: Root Mean Squared Error.\n",
    "        - mae: Mean Absolute Error.\n",
    "        - r2: R2 Score.\n",
    "        - average_relative_error: Average Relative Error.\n",
    "        \"\"\"\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        epsilon = 1e-10\n",
    "        relative_error = np.abs(pred - actual) / (pred + epsilon)\n",
    "        average_relative_error = relative_error.mean()\n",
    "\n",
    "        return rmse, mae, r2, average_relative_error\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load test data and the trained model.\n",
    "        \"\"\"\n",
    "        self.test_data = pd.read_csv(self.config.test_data_path)\n",
    "        self.model = joblib.load(self.config.model_path)\n",
    "        self.X_test = self.test_data.drop([self.config.target_column], axis=1)\n",
    "        self.y_test = self.test_data[self.config.target_column]\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        \"\"\"\n",
    "        Log model parameters, metrics, and the model itself into MLflow.\n",
    "        \"\"\"\n",
    "        self.load_data()\n",
    "\n",
    "        # Set the MLflow registry URI\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_score = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        # Start an MLflow tracking session\n",
    "        with mlflow.start_run():\n",
    "            predicted_qualities = self.model.predict(self.X_test)\n",
    "            (rmse, mae, r2, average_relative_error) = self.eval_metrics(self.y_test, predicted_qualities)\n",
    "            scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"average_relative_error\": average_relative_error}\n",
    "\n",
    "            # Save evaluation metrics to a JSON file\n",
    "            save_json(path=Path(self.config.metric_file_name), data=scores)\n",
    "\n",
    "            # Log parameters and metrics into MLflow\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metric(\"rmse\", rmse)\n",
    "            mlflow.log_metric(\"mae\", mae)\n",
    "            mlflow.log_metric(\"r2\", r2)\n",
    "            mlflow.log_metric(\"average_relative_error\", average_relative_error)\n",
    "\n",
    "\n",
    "            # Log the model into MLflow based on the type of tracking URL\n",
    "            if tracking_url_type_score != \"file\":\n",
    "                mlflow.sklearn.log_model(self.model, \"model\", registered_model_name=\"GradientBoostingRegressor\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(self.model, \"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-17 01:46:32,727: 42: predict_publications_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2023-10-17 01:46:32,730: 42: predict_publications_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2023-10-17 01:46:32,733: 42: predict_publications_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2023-10-17 01:46:32,738: 42: predict_publications_logger: INFO: common:  yaml file: feature_engineered_schema.yaml loaded successfully]\n",
      "[2023-10-17 01:46:32,738: 65: predict_publications_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2023-10-17 01:46:32,739: 13: predict_publications_logger: INFO: 3544983170:  Fetching model evaluation configuration...]\n",
      "[2023-10-17 01:46:32,740: 65: predict_publications_logger: INFO: common:  Created directory at: artifacts/model_evaluation]\n",
      "[2023-10-17 01:46:32,741: 16: predict_publications_logger: INFO: 3544983170:  Initializing model evaluation process...]\n",
      "[2023-10-17 01:46:32,742: 19: predict_publications_logger: INFO: 3544983170:  Logging model evaluation into MLFlow...]\n",
      "[2023-10-17 01:46:38,518: 86: predict_publications_logger: INFO: common:  json file saved at: artifacts/model_evaluation/metrics.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'GradientBoostingRegressor'.\n",
      "2023/10/17 01:46:47 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: GradientBoostingRegressor, version 1\n",
      "Created version '1' of model 'GradientBoostingRegressor'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-17 01:46:48,086: 22: predict_publications_logger: INFO: 3544983170:  Model Evaluation Pipeline completed successfully.]\n"
     ]
    }
   ],
   "source": [
    "from predicting_publications import logger\n",
    "\n",
    "class ModelEvaluationPipeline:\n",
    "\n",
    "    STAGE_NAME = \"Model Evaluation Pipeline\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config_manager = ConfigurationManager()\n",
    "\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        try:\n",
    "            logger.info(\"Fetching model evaluation configuration...\")\n",
    "            model_evaluation_configuration = self.config_manager.get_model_evaluation_config()\n",
    "\n",
    "            logger.info(\"Initializing model evaluation process...\")\n",
    "            model_evaluation = ModelEvaluation(config=model_evaluation_configuration)\n",
    "            \n",
    "            logger.info(\"Logging model evaluation into MLFlow...\")\n",
    "            model_evaluation.log_into_mlflow()\n",
    "            \n",
    "            logger.info(\"Model Evaluation Pipeline completed successfully.\")\n",
    "       \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the model evaluation: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = ModelEvaluationPipeline()\n",
    "    pipeline.run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_publications_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
