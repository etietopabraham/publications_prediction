{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/predict_publications/publications_prediction'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('/Users/macbookpro/Documents/predict_publications/publications_prediction/data/test_data.csv')\n",
    "train_data = pd.read_csv('/Users/macbookpro/Documents/predict_publications/publications_prediction/data/train_data.csv')\n",
    "validation_data = pd.read_csv('/Users/macbookpro/Documents/predict_publications/publications_prediction/data/validation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>point</th>\n",
       "      <th>hour</th>\n",
       "      <th>likescount</th>\n",
       "      <th>commentscount</th>\n",
       "      <th>symbols_cnt</th>\n",
       "      <th>words_cnt</th>\n",
       "      <th>hashtags_cnt</th>\n",
       "      <th>mentions_cnt</th>\n",
       "      <th>links_cnt</th>\n",
       "      <th>emoji_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0101000020E61000000000000000000000000000000000...</td>\n",
       "      <td>0</td>\n",
       "      <td>31.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>51.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.136232</td>\n",
       "      <td>60.000054</td>\n",
       "      <td>0101000020E6100000B8E59619E0223E40ABB649C80100...</td>\n",
       "      <td>0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.138478</td>\n",
       "      <td>59.835705</td>\n",
       "      <td>0101000020E610000077D0A94773233E4097654065F8EA...</td>\n",
       "      <td>0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.142969</td>\n",
       "      <td>60.023627</td>\n",
       "      <td>0101000020E6100000F5A5CFA399243E400B9A5B330603...</td>\n",
       "      <td>0</td>\n",
       "      <td>77.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>34.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.142969</td>\n",
       "      <td>60.030359</td>\n",
       "      <td>0101000020E6100000F5A5CFA399243E40854A58CAE203...</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp        lon        lat  \\\n",
       "0  1546300800   0.000000   0.000000   \n",
       "1  1546300800  30.136232  60.000054   \n",
       "2  1546300800  30.138478  59.835705   \n",
       "3  1546300800  30.142969  60.023627   \n",
       "4  1546300800  30.142969  60.030359   \n",
       "\n",
       "                                               point  hour  likescount  \\\n",
       "0  0101000020E61000000000000000000000000000000000...     0   31.666667   \n",
       "1  0101000020E6100000B8E59619E0223E40ABB649C80100...     0   52.000000   \n",
       "2  0101000020E610000077D0A94773233E4097654065F8EA...     0   32.000000   \n",
       "3  0101000020E6100000F5A5CFA399243E400B9A5B330603...     0   77.666667   \n",
       "4  0101000020E6100000F5A5CFA399243E40854A58CAE203...     0   19.000000   \n",
       "\n",
       "   commentscount  symbols_cnt  words_cnt  hashtags_cnt  mentions_cnt  \\\n",
       "0       1.666667    51.333333   2.000000      2.000000           0.0   \n",
       "1       1.000000    28.000000   0.500000      2.000000           0.0   \n",
       "2       0.333333    46.000000   2.333333      3.000000           0.0   \n",
       "3       3.333333    34.666667   2.666667      0.666667           0.0   \n",
       "4       3.000000     0.000000   0.000000      0.000000           0.0   \n",
       "\n",
       "   links_cnt  emoji_cnt  \n",
       "0        0.0   0.000000  \n",
       "1        0.0   0.500000  \n",
       "2        0.0   1.333333  \n",
       "3        0.0   1.666667  \n",
       "4        0.0   0.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'timestamp' to a datetime format\n",
    "train_data['date'] = pd.to_datetime(train_data['timestamp'], unit='s')\n",
    "\n",
    "# Extracting the hour from the 'date' column\n",
    "train_data['hour'] = train_data['date'].dt.hour\n",
    "\n",
    "# Aggregate data based on 'hour', 'lon', and 'lat'\n",
    "agg_columns = {\n",
    "    'likescount': 'mean',\n",
    "    'commentscount': 'mean',\n",
    "    'symbols_cnt': 'mean',\n",
    "    'words_cnt': 'mean',\n",
    "    'hashtags_cnt': 'mean',\n",
    "    'mentions_cnt': 'mean',\n",
    "    'links_cnt': 'mean',\n",
    "    'emoji_cnt': 'mean',\n",
    "}\n",
    "\n",
    "grouped_data = train_data.groupby(['timestamp', 'lon', 'lat', 'point', 'hour']).agg(agg_columns).reset_index()\n",
    "grouped_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>point</th>\n",
       "      <th>hour</th>\n",
       "      <th>likescount</th>\n",
       "      <th>commentscount</th>\n",
       "      <th>symbols_cnt</th>\n",
       "      <th>words_cnt</th>\n",
       "      <th>hashtags_cnt</th>\n",
       "      <th>mentions_cnt</th>\n",
       "      <th>links_cnt</th>\n",
       "      <th>emoji_cnt</th>\n",
       "      <th>publication_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0101000020E61000000000000000000000000000000000...</td>\n",
       "      <td>0</td>\n",
       "      <td>31.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>51.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.136232</td>\n",
       "      <td>60.000054</td>\n",
       "      <td>0101000020E6100000B8E59619E0223E40ABB649C80100...</td>\n",
       "      <td>0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.138478</td>\n",
       "      <td>59.835705</td>\n",
       "      <td>0101000020E610000077D0A94773233E4097654065F8EA...</td>\n",
       "      <td>0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.142969</td>\n",
       "      <td>60.023627</td>\n",
       "      <td>0101000020E6100000F5A5CFA399243E400B9A5B330603...</td>\n",
       "      <td>0</td>\n",
       "      <td>77.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>34.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1546300800</td>\n",
       "      <td>30.142969</td>\n",
       "      <td>60.030359</td>\n",
       "      <td>0101000020E6100000F5A5CFA399243E40854A58CAE203...</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp        lon        lat  \\\n",
       "0  1546300800   0.000000   0.000000   \n",
       "1  1546300800  30.136232  60.000054   \n",
       "2  1546300800  30.138478  59.835705   \n",
       "3  1546300800  30.142969  60.023627   \n",
       "4  1546300800  30.142969  60.030359   \n",
       "\n",
       "                                               point  hour  likescount  \\\n",
       "0  0101000020E61000000000000000000000000000000000...     0   31.666667   \n",
       "1  0101000020E6100000B8E59619E0223E40ABB649C80100...     0   52.000000   \n",
       "2  0101000020E610000077D0A94773233E4097654065F8EA...     0   32.000000   \n",
       "3  0101000020E6100000F5A5CFA399243E400B9A5B330603...     0   77.666667   \n",
       "4  0101000020E6100000F5A5CFA399243E40854A58CAE203...     0   19.000000   \n",
       "\n",
       "   commentscount  symbols_cnt  words_cnt  hashtags_cnt  mentions_cnt  \\\n",
       "0       1.666667    51.333333   2.000000      2.000000           0.0   \n",
       "1       1.000000    28.000000   0.500000      2.000000           0.0   \n",
       "2       0.333333    46.000000   2.333333      3.000000           0.0   \n",
       "3       3.333333    34.666667   2.666667      0.666667           0.0   \n",
       "4       3.000000     0.000000   0.000000      0.000000           0.0   \n",
       "\n",
       "   links_cnt  emoji_cnt  publication_count  \n",
       "0        0.0   0.000000                  3  \n",
       "1        0.0   0.500000                  2  \n",
       "2        0.0   1.333333                  3  \n",
       "3        0.0   1.666667                  3  \n",
       "4        0.0   0.000000                  1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data['publication_count'] = train_data.groupby(['timestamp', 'hour', 'lon', 'lat', 'point']).size().values\n",
    "grouped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3635541 entries, 0 to 3635540\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   timestamp          int64  \n",
      " 1   lon                float64\n",
      " 2   lat                float64\n",
      " 3   point              object \n",
      " 4   hour               int32  \n",
      " 5   likescount         float64\n",
      " 6   commentscount      float64\n",
      " 7   symbols_cnt        float64\n",
      " 8   words_cnt          float64\n",
      " 9   hashtags_cnt       float64\n",
      " 10  mentions_cnt       float64\n",
      " 11  links_cnt          float64\n",
      " 12  emoji_cnt          float64\n",
      " 13  publication_count  int64  \n",
      "dtypes: float64(10), int32(1), int64(2), object(1)\n",
      "memory usage: 374.4+ MB\n"
     ]
    }
   ],
   "source": [
    "grouped_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'timestamp' as it's strongly correlated with other time features and may cause data leakage\n",
    "X_train = grouped_data.drop(['publication_count', 'timestamp', 'point'], axis=1)\n",
    "y_train = grouped_data['publication_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3635541, 11), (3635541,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3635541 entries, 0 to 3635540\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   lon            float64\n",
      " 1   lat            float64\n",
      " 2   hour           int32  \n",
      " 3   likescount     float64\n",
      " 4   commentscount  float64\n",
      " 5   symbols_cnt    float64\n",
      " 6   words_cnt      float64\n",
      " 7   hashtags_cnt   float64\n",
      " 8   mentions_cnt   float64\n",
      " 9   links_cnt      float64\n",
      " 10  emoji_cnt      float64\n",
      "dtypes: float64(10), int32(1)\n",
      "memory usage: 291.2 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'hour' column to a datetime format\n",
    "test_data['date'] = pd.to_datetime(test_data['hour'], unit='s')\n",
    "\n",
    "# Drop the original 'hour' column which contains the timestamp\n",
    "test_data.drop(columns=['hour'], inplace=True)\n",
    "\n",
    "# Extract the datetime features from the 'date' column\n",
    "test_data['hour'] = test_data['date'].dt.hour\n",
    "test_data['day'] = test_data['date'].dt.day\n",
    "test_data['dayofweek'] = test_data['date'].dt.dayofweek\n",
    "test_data['month'] = test_data['date'].dt.month\n",
    "\n",
    "# Drop the 'date' column as it's not needed for prediction\n",
    "test_data.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Set 'point' as the index for both datasets\n",
    "train_data.set_index('point', inplace=True)\n",
    "test_data.set_index('point', inplace=True)\n",
    "\n",
    "# List of features to create in the test dataset\n",
    "features_to_create = ['likescount', 'commentscount', 'symbols_cnt', 'words_cnt', \n",
    "                      'hashtags_cnt', 'mentions_cnt', 'links_cnt', 'emoji_cnt']\n",
    "\n",
    "# Aggregate the training dataset based on 'point' and compute the median for each feature\n",
    "aggregated_data = train_data[features_to_create].groupby('point').median()\n",
    "\n",
    "# Merge the test dataset with the aggregated training data on 'point'\n",
    "test_data = test_data.join(aggregated_data, on='point', how='left')\n",
    "\n",
    "# Reset index for both datasets after the operations\n",
    "train_data.reset_index(inplace=True)\n",
    "test_data.reset_index(inplace=True)\n",
    "\n",
    "X_test = test_data.drop(['sum', 'point', 'error'], axis=1)\n",
    "y_test = test_data['sum']\n",
    "X_test = X_test[X_train.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 11), (700,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 700 entries, 0 to 699\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   lon            700 non-null    float64\n",
      " 1   lat            700 non-null    float64\n",
      " 2   hour           700 non-null    int32  \n",
      " 3   likescount     700 non-null    float64\n",
      " 4   commentscount  700 non-null    float64\n",
      " 5   symbols_cnt    700 non-null    float64\n",
      " 6   words_cnt      700 non-null    float64\n",
      " 7   hashtags_cnt   700 non-null    float64\n",
      " 8   mentions_cnt   700 non-null    float64\n",
      " 9   links_cnt      700 non-null    float64\n",
      " 10  emoji_cnt      700 non-null    float64\n",
      "dtypes: float64(10), int32(1)\n",
      "memory usage: 57.5 KB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>hour</th>\n",
       "      <th>likescount</th>\n",
       "      <th>commentscount</th>\n",
       "      <th>symbols_cnt</th>\n",
       "      <th>words_cnt</th>\n",
       "      <th>hashtags_cnt</th>\n",
       "      <th>mentions_cnt</th>\n",
       "      <th>links_cnt</th>\n",
       "      <th>emoji_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.331616</td>\n",
       "      <td>59.934863</td>\n",
       "      <td>10</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.329370</td>\n",
       "      <td>59.940488</td>\n",
       "      <td>11</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.297929</td>\n",
       "      <td>59.905597</td>\n",
       "      <td>16</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.356319</td>\n",
       "      <td>59.921358</td>\n",
       "      <td>13</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.315895</td>\n",
       "      <td>59.939363</td>\n",
       "      <td>13</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lon        lat  hour  likescount  commentscount  symbols_cnt  \\\n",
       "0  30.331616  59.934863    10        44.0            1.0         73.0   \n",
       "1  30.329370  59.940488    11        41.0            1.0         47.0   \n",
       "2  30.297929  59.905597    16        28.0            0.0         42.0   \n",
       "3  30.356319  59.921358    13        48.0            1.0        110.0   \n",
       "4  30.315895  59.939363    13        47.0            1.0         49.0   \n",
       "\n",
       "   words_cnt  hashtags_cnt  mentions_cnt  links_cnt  emoji_cnt  \n",
       "0        4.0           0.0           0.0        0.0        1.0  \n",
       "1        2.0           0.0           0.0        0.0        0.0  \n",
       "2        2.0           0.0           0.0        0.0        0.0  \n",
       "3        6.0           0.0           0.0        0.0        1.0  \n",
       "4        2.0           0.0           0.0        0.0        0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    3\n",
       " 1    2\n",
       " 2    3\n",
       " 3    3\n",
       " 4    1\n",
       " Name: publication_count, dtype: int64,\n",
       " 0     7\n",
       " 1     6\n",
       " 2     5\n",
       " 3    16\n",
       " 4    10\n",
       " Name: sum, dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(), y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, n_jobs=-1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARE: 7.192154745845674\n",
      "RMSE: 10.642302552301723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Predictions\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
    "\n",
    "# Calculating the relative error for the Gradient Boosting model predictions\n",
    "rf_relative_errors = np.abs(rf_predictions - y_test) / rf_predictions\n",
    "rf_average_relative_error = rf_relative_errors.mean()\n",
    "\n",
    "\n",
    "print(f\"ARE: {rf_average_relative_error}\")\n",
    "print(f\"RMSE: {rf_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.0895527640998049, max_depth=8,\n",
       "                          min_samples_leaf=4, min_samples_split=20,\n",
       "                          random_state=42, subsample=0.5970487514167024)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.0895527640998049, max_depth=8,\n",
       "                          min_samples_leaf=4, min_samples_split=20,\n",
       "                          random_state=42, subsample=0.5970487514167024)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.0895527640998049, max_depth=8,\n",
       "                          min_samples_leaf=4, min_samples_split=20,\n",
       "                          random_state=42, subsample=0.5970487514167024)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize and train the Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, max_depth=8, learning_rate=0.0895527640998049, subsample=0.5970487514167024, min_samples_split=20, min_samples_leaf=4, random_state=42)\n",
    "\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.1216475882308625"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict using the Gradient Boosting model\n",
    "gb_predictions = gb_model.predict(X_test)  # gb_model is already loaded and trained\n",
    "\n",
    "# Evaluate the Gradient Boosting model\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_predictions))\n",
    "\n",
    "# Calculating the relative error for the Gradient Boosting model predictions\n",
    "gb_relative_errors = np.abs(gb_predictions - y_test) / gb_predictions\n",
    "gb_average_relative_error = gb_relative_errors.mean()\n",
    "gb_average_relative_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshaping data for LSTM [samples, time steps, features]\n",
    "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18936/18936 - 38s - loss: 5.9658 - val_loss: 15.1784 - 38s/epoch - 2ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18936/18936 - 31s - loss: 5.5741 - val_loss: 15.0742 - 31s/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "18936/18936 - 33s - loss: 5.5027 - val_loss: 15.0109 - 33s/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "18936/18936 - 46s - loss: 5.4651 - val_loss: 15.0175 - 46s/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "18936/18936 - 35s - loss: 5.4414 - val_loss: 14.9771 - 35s/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "18936/18936 - 31s - loss: 5.2663 - val_loss: 14.0115 - 31s/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "18936/18936 - 31s - loss: 4.9157 - val_loss: 13.2343 - 31s/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "18936/18936 - 32s - loss: 4.6967 - val_loss: 12.8222 - 32s/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "18936/18936 - 31s - loss: 4.5588 - val_loss: 12.2603 - 31s/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "18936/18936 - 31s - loss: 4.4451 - val_loss: 11.9213 - 31s/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37871/37871 - 60s - loss: 9.7862 - val_loss: 19.5447 - 60s/epoch - 2ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37871/37871 - 56s - loss: 9.4276 - val_loss: 19.2339 - 56s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "37871/37871 - 55s - loss: 8.9569 - val_loss: 16.5924 - 55s/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "37871/37871 - 55s - loss: 7.9509 - val_loss: 15.3307 - 55s/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "37871/37871 - 55s - loss: 7.4873 - val_loss: 14.8721 - 55s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "37871/37871 - 55s - loss: 7.1752 - val_loss: 14.5492 - 55s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "37871/37871 - 56s - loss: 6.9342 - val_loss: 14.2609 - 56s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "37871/37871 - 55s - loss: 6.8051 - val_loss: 14.1141 - 55s/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "37871/37871 - 53s - loss: 6.7203 - val_loss: 13.9541 - 53s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "37871/37871 - 54s - loss: 6.6575 - val_loss: 13.8668 - 54s/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56806/56806 - 81s - loss: 12.8137 - val_loss: 13.8385 - 81s/epoch - 1ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56806/56806 - 77s - loss: 12.2011 - val_loss: 12.7327 - 77s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "56806/56806 - 77s - loss: 10.5365 - val_loss: 11.1570 - 77s/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "56806/56806 - 77s - loss: 9.7456 - val_loss: 10.5111 - 77s/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "56806/56806 - 76s - loss: 9.3535 - val_loss: 10.1647 - 76s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "56806/56806 - 77s - loss: 9.1093 - val_loss: 9.9362 - 77s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "56806/56806 - 76s - loss: 8.9857 - val_loss: 9.9428 - 76s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "56806/56806 - 76s - loss: 8.8926 - val_loss: 9.7164 - 76s/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "56806/56806 - 77s - loss: 8.8172 - val_loss: 9.6767 - 77s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "56806/56806 - 76s - loss: 8.7491 - val_loss: 9.6162 - 76s/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75741/75741 - 104s - loss: 12.9510 - val_loss: 16.6935 - 104s/epoch - 1ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75741/75741 - 101s - loss: 10.6282 - val_loss: 15.0068 - 101s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "75741/75741 - 99s - loss: 9.7165 - val_loss: 14.2457 - 99s/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "75741/75741 - 99s - loss: 9.3560 - val_loss: 13.8898 - 99s/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "75741/75741 - 98s - loss: 9.1539 - val_loss: 13.7768 - 98s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "75741/75741 - 98s - loss: 9.0186 - val_loss: 13.6712 - 98s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "75741/75741 - 97s - loss: 8.8941 - val_loss: 13.4719 - 97s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "75741/75741 - 98s - loss: 8.7916 - val_loss: 13.3750 - 98s/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "75741/75741 - 98s - loss: 8.7053 - val_loss: 13.4600 - 98s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "75741/75741 - 102s - loss: 8.6456 - val_loss: 13.2777 - 102s/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94676/94676 - 135s - loss: 13.7394 - val_loss: 23.1767 - 135s/epoch - 1ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94676/94676 - 130s - loss: 11.5196 - val_loss: 18.9118 - 130s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "94676/94676 - 129s - loss: 10.5470 - val_loss: 17.9301 - 129s/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "94676/94676 - 129s - loss: 10.1772 - val_loss: 17.5167 - 129s/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "94676/94676 - 129s - loss: 10.0112 - val_loss: 17.0610 - 129s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "94676/94676 - 125s - loss: 9.8601 - val_loss: 16.9060 - 125s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "94676/94676 - 129s - loss: 9.7162 - val_loss: 16.4801 - 129s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "94676/94676 - 130s - loss: 9.6193 - val_loss: 16.4171 - 130s/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "94676/94676 - 128s - loss: 9.5583 - val_loss: 16.7494 - 128s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "94676/94676 - 122s - loss: 9.5530 - val_loss: 16.5753 - 122s/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, clone_model, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "def create_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "best_model = None\n",
    "lowest_val_loss = float('inf')\n",
    "\n",
    "# Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, val_index in tscv.split(X_train_reshaped):\n",
    "    X_train_fold, X_val_fold = X_train_reshaped[train_index], X_train_reshaped[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model = create_lstm_model()\n",
    "    \n",
    "    # Use ModelCheckpoint to save the model with the lowest validation loss\n",
    "    checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold), callbacks=[checkpoint], verbose=2, shuffle=False)\n",
    "    \n",
    "    # Check if this fold's model is better than the previous best model\n",
    "    val_loss = model.history.history['val_loss'][-1]\n",
    "    if val_loss < lowest_val_loss:\n",
    "        best_model = clone_model(model)\n",
    "        best_model.load_weights('best_model.h5')  # Corrected this line\n",
    "        best_model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        lowest_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 2s 1ms/step\n",
      "Average Relative Error: 3.674325794533714\n",
      "RMSE on the validation set: 9.896537503524\n"
     ]
    }
   ],
   "source": [
    "# Reshape X_test for the LSTM model\n",
    "X_test_reshaped = scaler.transform(X_test).reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# After training, evaluate the best model on the test data\n",
    "y_pred = best_model.predict(X_test_reshaped)\n",
    "\n",
    "# Calculate the relative error\n",
    "epsilon = 1e-10\n",
    "errors = np.abs(y_pred.flatten() - y_test) / (y_pred.flatten() + epsilon)\n",
    "avg_relative_error = np.mean(errors)\n",
    "print(f\"Average Relative Error: {avg_relative_error}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE on the validation set: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18936/18936 - 32s - loss: 5.9615 - val_loss: 15.2243 - 32s/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "18936/18936 - 30s - loss: 5.5880 - val_loss: 15.0707 - 30s/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "18936/18936 - 30s - loss: 5.5292 - val_loss: 15.0041 - 30s/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "18936/18936 - 31s - loss: 5.4966 - val_loss: 14.9716 - 31s/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "18936/18936 - 31s - loss: 5.4781 - val_loss: 14.9749 - 31s/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "18936/18936 - 31s - loss: 5.4631 - val_loss: 14.8930 - 31s/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "18936/18936 - 38s - loss: 5.4417 - val_loss: 14.8688 - 38s/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "18936/18936 - 38s - loss: 5.3454 - val_loss: 14.1421 - 38s/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "18936/18936 - 31s - loss: 4.9543 - val_loss: 13.1718 - 31s/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "18936/18936 - 31s - loss: 4.7198 - val_loss: 12.5983 - 31s/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37871/37871 - 64s - loss: 9.8291 - val_loss: 19.4817 - 64s/epoch - 2ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37871/37871 - 51s - loss: 9.4728 - val_loss: 19.2719 - 51s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "37871/37871 - 66s - loss: 9.3085 - val_loss: 18.1317 - 66s/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "37871/37871 - 51s - loss: 8.2534 - val_loss: 15.3076 - 51s/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "37871/37871 - 53s - loss: 7.6689 - val_loss: 14.6772 - 53s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "37871/37871 - 60s - loss: 7.3655 - val_loss: 14.5348 - 60s/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "37871/37871 - 53s - loss: 7.2187 - val_loss: 14.4023 - 53s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "37871/37871 - 62s - loss: 7.0928 - val_loss: 14.2518 - 62s/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "37871/37871 - 51s - loss: 7.0231 - val_loss: 14.0726 - 51s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "37871/37871 - 68s - loss: 6.9043 - val_loss: 13.9735 - 68s/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56806/56806 - 91s - loss: 12.8515 - val_loss: 13.7797 - 91s/epoch - 2ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56806/56806 - 78s - loss: 12.4294 - val_loss: 13.2335 - 78s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "56806/56806 - 77s - loss: 10.6412 - val_loss: 10.7689 - 77s/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "56806/56806 - 88s - loss: 9.8124 - val_loss: 10.6095 - 88s/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "56806/56806 - 79s - loss: 9.5421 - val_loss: 10.2087 - 79s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "56806/56806 - 82s - loss: 9.3648 - val_loss: 10.1065 - 82s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "56806/56806 - 74s - loss: 9.2274 - val_loss: 9.9700 - 74s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "56806/56806 - 72s - loss: 9.0937 - val_loss: 9.9691 - 72s/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "56806/56806 - 72s - loss: 9.0414 - val_loss: 9.9301 - 72s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "56806/56806 - 73s - loss: 8.9594 - val_loss: 9.8095 - 73s/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75741/75741 - 98s - loss: 13.0328 - val_loss: 17.2151 - 98s/epoch - 1ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75741/75741 - 96s - loss: 11.5333 - val_loss: 15.0897 - 96s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "75741/75741 - 92s - loss: 10.0696 - val_loss: 14.4075 - 92s/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "75741/75741 - 96s - loss: 9.6727 - val_loss: 14.0819 - 96s/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "75741/75741 - 92s - loss: 9.4150 - val_loss: 13.8236 - 92s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "75741/75741 - 92s - loss: 9.2674 - val_loss: 13.7950 - 92s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "75741/75741 - 92s - loss: 9.1961 - val_loss: 13.6800 - 92s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "75741/75741 - 92s - loss: 9.1162 - val_loss: 13.7105 - 92s/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "75741/75741 - 93s - loss: 9.0600 - val_loss: 13.6993 - 93s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "75741/75741 - 93s - loss: 8.9941 - val_loss: 13.6014 - 93s/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94676/94676 - 139s - loss: 13.7726 - val_loss: 22.5734 - 139s/epoch - 1ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94676/94676 - 116s - loss: 11.4625 - val_loss: 19.0441 - 116s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "94676/94676 - 116s - loss: 10.6460 - val_loss: 18.1820 - 116s/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "94676/94676 - 117s - loss: 10.3327 - val_loss: 17.7362 - 117s/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "94676/94676 - 120s - loss: 10.1548 - val_loss: 17.7018 - 120s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "94676/94676 - 117s - loss: 10.0647 - val_loss: 17.3863 - 117s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "94676/94676 - 117s - loss: 9.9832 - val_loss: 17.2374 - 117s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "94676/94676 - 115s - loss: 9.9330 - val_loss: 17.2350 - 115s/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "94676/94676 - 116s - loss: 9.8331 - val_loss: 17.0533 - 116s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "94676/94676 - 116s - loss: 9.7562 - val_loss: 17.2087 - 116s/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "def create_gru_model():\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "best_gru_model = None\n",
    "lowest_val_loss = float('inf')\n",
    "\n",
    "# Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, val_index in tscv.split(X_train_reshaped):\n",
    "    X_train_fold, X_val_fold = X_train_reshaped[train_index], X_train_reshaped[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model = create_gru_model()\n",
    "    \n",
    "    # Use ModelCheckpoint to save the model with the lowest validation loss\n",
    "    checkpoint = ModelCheckpoint('best_gru_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold), callbacks=[checkpoint], verbose=2, shuffle=False)\n",
    "    \n",
    "    # Check if this fold's model is better than the previous best model\n",
    "    val_loss = model.history.history['val_loss'][-1]\n",
    "    if val_loss < lowest_val_loss:\n",
    "        best_gru_model = clone_model(model)\n",
    "        best_gru_model.load_weights('best_gru_model.h5')\n",
    "        best_gru_model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        lowest_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 518us/step\n",
      "Average Relative Error: 3.718872882014978\n",
      "RMSE on the validation set: 9.903534816594535\n"
     ]
    }
   ],
   "source": [
    "# Reshape X_test for the LSTM model\n",
    "X_test_reshaped = scaler.transform(X_test).reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# After training, evaluate the best model on the test data\n",
    "y_pred = best_gru_model.predict(X_test_reshaped)\n",
    "\n",
    "# Calculate the relative error\n",
    "epsilon = 1e-10\n",
    "errors = np.abs(y_pred.flatten() - y_test) / (y_pred.flatten() + epsilon)\n",
    "avg_relative_error = np.mean(errors)\n",
    "print(f\"Average Relative Error: {avg_relative_error}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE on the validation set: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18936/18936 - 21s - loss: 0.1048 - val_loss: 0.1044 - 21s/epoch - 1ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18936/18936 - 16s - loss: 0.1047 - val_loss: 0.1044 - 16s/epoch - 854us/step\n",
      "Epoch 3/10\n",
      "18936/18936 - 16s - loss: 0.1047 - val_loss: 0.1045 - 16s/epoch - 850us/step\n",
      "Epoch 4/10\n",
      "18936/18936 - 16s - loss: 0.1047 - val_loss: 0.1045 - 16s/epoch - 831us/step\n",
      "Epoch 5/10\n",
      "18936/18936 - 16s - loss: 0.1047 - val_loss: 0.1044 - 16s/epoch - 860us/step\n",
      "Epoch 6/10\n",
      "18936/18936 - 16s - loss: 0.1047 - val_loss: 0.1045 - 16s/epoch - 835us/step\n",
      "Epoch 7/10\n",
      "18936/18936 - 16s - loss: 0.1047 - val_loss: 0.1045 - 16s/epoch - 862us/step\n",
      "Epoch 8/10\n",
      "18936/18936 - 17s - loss: 0.1047 - val_loss: 0.1045 - 17s/epoch - 889us/step\n",
      "Epoch 9/10\n",
      "18936/18936 - 16s - loss: 0.1047 - val_loss: 0.1045 - 16s/epoch - 853us/step\n",
      "Epoch 10/10\n",
      "18936/18936 - 17s - loss: 0.1047 - val_loss: 0.1044 - 17s/epoch - 889us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37871/37871 - 28s - loss: 0.1046 - val_loss: 0.1050 - 28s/epoch - 744us/step\n",
      "Epoch 2/10\n",
      "37871/37871 - 29s - loss: 0.1045 - val_loss: 0.1049 - 29s/epoch - 756us/step\n",
      "Epoch 3/10\n",
      "37871/37871 - 29s - loss: 0.1045 - val_loss: 0.1050 - 29s/epoch - 761us/step\n",
      "Epoch 4/10\n",
      "37871/37871 - 29s - loss: 0.1045 - val_loss: 0.1050 - 29s/epoch - 760us/step\n",
      "Epoch 5/10\n",
      "37871/37871 - 29s - loss: 0.1045 - val_loss: 0.1050 - 29s/epoch - 759us/step\n",
      "Epoch 6/10\n",
      "37871/37871 - 28s - loss: 0.1045 - val_loss: 0.1050 - 28s/epoch - 734us/step\n",
      "Epoch 7/10\n",
      "37871/37871 - 29s - loss: 0.1045 - val_loss: 0.1049 - 29s/epoch - 754us/step\n",
      "Epoch 8/10\n",
      "37871/37871 - 28s - loss: 0.1045 - val_loss: 0.1050 - 28s/epoch - 739us/step\n",
      "Epoch 9/10\n",
      "37871/37871 - 28s - loss: 0.1045 - val_loss: 0.1050 - 28s/epoch - 745us/step\n",
      "Epoch 10/10\n",
      "37871/37871 - 28s - loss: 0.1045 - val_loss: 0.1050 - 28s/epoch - 752us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56806/56806 - 41s - loss: 0.1047 - val_loss: 0.1045 - 41s/epoch - 720us/step\n",
      "Epoch 2/10\n",
      "56806/56806 - 40s - loss: 0.1047 - val_loss: 0.1045 - 40s/epoch - 702us/step\n",
      "Epoch 3/10\n",
      "56806/56806 - 39s - loss: 0.1047 - val_loss: 0.1045 - 39s/epoch - 694us/step\n",
      "Epoch 4/10\n",
      "56806/56806 - 40s - loss: 0.1047 - val_loss: 0.1045 - 40s/epoch - 697us/step\n",
      "Epoch 5/10\n",
      "56806/56806 - 40s - loss: 0.1047 - val_loss: 0.1045 - 40s/epoch - 700us/step\n",
      "Epoch 6/10\n",
      "56806/56806 - 40s - loss: 0.1047 - val_loss: 0.1045 - 40s/epoch - 696us/step\n",
      "Epoch 7/10\n",
      "56806/56806 - 40s - loss: 0.1047 - val_loss: 0.1045 - 40s/epoch - 701us/step\n",
      "Epoch 8/10\n",
      "56806/56806 - 40s - loss: 0.1047 - val_loss: 0.1045 - 40s/epoch - 700us/step\n",
      "Epoch 9/10\n",
      "56806/56806 - 40s - loss: 0.1047 - val_loss: 0.1045 - 40s/epoch - 699us/step\n",
      "Epoch 10/10\n",
      "56806/56806 - 39s - loss: 0.1047 - val_loss: 0.1045 - 39s/epoch - 691us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75741/75741 - 54s - loss: 0.1046 - val_loss: 0.1047 - 54s/epoch - 710us/step\n",
      "Epoch 2/10\n",
      "75741/75741 - 50s - loss: 0.1046 - val_loss: 0.1047 - 50s/epoch - 665us/step\n",
      "Epoch 3/10\n",
      "75741/75741 - 51s - loss: 0.1046 - val_loss: 0.1047 - 51s/epoch - 673us/step\n",
      "Epoch 4/10\n",
      "75741/75741 - 57s - loss: 0.1046 - val_loss: 0.1047 - 57s/epoch - 753us/step\n",
      "Epoch 5/10\n",
      "75741/75741 - 54s - loss: 0.1046 - val_loss: 0.1047 - 54s/epoch - 713us/step\n",
      "Epoch 6/10\n",
      "75741/75741 - 51s - loss: 0.1046 - val_loss: 0.1047 - 51s/epoch - 674us/step\n",
      "Epoch 7/10\n",
      "75741/75741 - 51s - loss: 0.1046 - val_loss: 0.1047 - 51s/epoch - 669us/step\n",
      "Epoch 8/10\n",
      "75741/75741 - 50s - loss: 0.1046 - val_loss: 0.1047 - 50s/epoch - 659us/step\n",
      "Epoch 9/10\n",
      "75741/75741 - 50s - loss: 0.1046 - val_loss: 0.1047 - 50s/epoch - 665us/step\n",
      "Epoch 10/10\n",
      "75741/75741 - 58s - loss: 0.1046 - val_loss: 0.1047 - 58s/epoch - 770us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94676/94676 - 64s - loss: 0.1047 - val_loss: 0.1055 - 64s/epoch - 676us/step\n",
      "Epoch 2/10\n",
      "94676/94676 - 58s - loss: 0.1046 - val_loss: 0.1055 - 58s/epoch - 612us/step\n",
      "Epoch 3/10\n",
      "94676/94676 - 58s - loss: 0.1046 - val_loss: 0.1056 - 58s/epoch - 615us/step\n",
      "Epoch 4/10\n",
      "94676/94676 - 59s - loss: 0.1046 - val_loss: 0.1055 - 59s/epoch - 626us/step\n",
      "Epoch 5/10\n",
      "94676/94676 - 59s - loss: 0.1046 - val_loss: 0.1055 - 59s/epoch - 620us/step\n",
      "Epoch 6/10\n",
      "94676/94676 - 59s - loss: 0.1046 - val_loss: 0.1055 - 59s/epoch - 622us/step\n",
      "Epoch 7/10\n",
      "94676/94676 - 61s - loss: 0.1046 - val_loss: 0.1055 - 61s/epoch - 646us/step\n",
      "Epoch 8/10\n",
      "94676/94676 - 60s - loss: 0.1046 - val_loss: 0.1055 - 60s/epoch - 638us/step\n",
      "Epoch 9/10\n",
      "94676/94676 - 60s - loss: 0.1046 - val_loss: 0.1055 - 60s/epoch - 633us/step\n",
      "Epoch 10/10\n",
      "94676/94676 - 60s - loss: 0.1046 - val_loss: 0.1055 - 60s/epoch - 629us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, clone_model, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def create_windowed_dataset(data, window_size):\n",
    "    \"\"\"\n",
    "    Transforms the data into a windowed dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The original dataset.\n",
    "    - window_size: The size of the window to take for each sample.\n",
    "    \n",
    "    Returns:\n",
    "    - X, y: Windowed data and labels.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i : i + window_size])\n",
    "        y.append(data[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Choose a window size\n",
    "window_size = 10\n",
    "\n",
    "# Create windowed datasets\n",
    "X_train_windows, y_train_windows = create_windowed_dataset(X_train_scaled, window_size)\n",
    "X_test_windows, y_test_windows = create_windowed_dataset(X_test_scaled, window_size)\n",
    "\n",
    "X_train_windows.shape, y_train_windows.shape\n",
    "\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "def create_cnn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adjust the input shape to account for the window size\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(window_size, X_train_scaled.shape[1])))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "best_cnn_model = None\n",
    "lowest_val_loss = float('inf')\n",
    "\n",
    "# Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, val_index in tscv.split(X_train_windows):\n",
    "    X_train_fold, X_val_fold = X_train_windows[train_index], X_train_windows[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_windows[train_index], y_train_windows[val_index]\n",
    "    \n",
    "    model = create_cnn_model()\n",
    "    \n",
    "    # Use ModelCheckpoint to save the model with the lowest validation loss\n",
    "    checkpoint = ModelCheckpoint('best_cnn_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold), callbacks=[checkpoint], verbose=2, shuffle=False)\n",
    "    \n",
    "    # Check if this fold's model is better than the previous best model\n",
    "    val_loss = model.history.history['val_loss'][-1]\n",
    "    if val_loss < lowest_val_loss:\n",
    "        best_cnn_model = clone_model(model)\n",
    "        best_cnn_model.load_weights('best_cnn_model.h5')\n",
    "        best_cnn_model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        lowest_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 335us/step\n",
      "Average Relative Error: 1.1120157978057397\n",
      "RMSE on the validation set: 0.22331234648308465\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Predict on the windowed test data\n",
    "y_pred = best_cnn_model.predict(X_test_windows)\n",
    "\n",
    "# 2. Extract the publication count from y_test_windows for comparison\n",
    "y_test_publication_counts = y_test_windows[:, 0]  # Adjust the index if the publication count is not the first column\n",
    "\n",
    "# 3. Calculate the relative error\n",
    "epsilon = 1e-10\n",
    "errors = np.abs(y_pred.flatten() - y_test_publication_counts) / (y_pred.flatten() + epsilon)\n",
    "avg_relative_error = np.mean(errors)\n",
    "print(f\"Average Relative Error: {avg_relative_error}\")\n",
    "\n",
    "# 4. Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test_publication_counts, y_pred.flatten()))\n",
    "print(f\"RMSE on the validation set: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Convolutional Networks (TCNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tcn\n",
      "  Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: numpy in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from keras-tcn) (1.24.3)\n",
      "Requirement already satisfied: tensorflow in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from keras-tcn) (2.13.0)\n",
      "Collecting tensorflow-addons (from keras-tcn)\n",
      "  Obtaining dependency information for tensorflow-addons from https://files.pythonhosted.org/packages/9d/ce/e6705610d63bd892c11e9cec0a2f9f6367e7e3e83cc6819f7611bb7c699a/tensorflow_addons-0.21.0-cp38-cp38-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading tensorflow_addons-0.21.0-cp38-cp38-macosx_11_0_arm64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow->keras-tcn) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (4.24.4)\n",
      "Requirement already satisfied: setuptools in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow->keras-tcn) (2.13.1)\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->keras-tcn)\n",
      "  Using cached typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (3.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (3.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (6.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->keras-tcn) (3.2.2)\n",
      "Downloading tensorflow_addons-0.21.0-cp38-cp38-macosx_11_0_arm64.whl (11.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: typeguard, tensorflow-addons, keras-tcn\n",
      "Successfully installed keras-tcn-3.5.0 tensorflow-addons-0.21.0 typeguard-2.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras-tcn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18936/18936 - 122s - loss: 5.6866 - val_loss: 15.3842 - 122s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "18936/18936 - 184s - loss: 5.4568 - val_loss: 15.0455 - 184s/epoch - 10ms/step\n",
      "Epoch 3/10\n",
      "18936/18936 - 148s - loss: 5.1637 - val_loss: 14.6009 - 148s/epoch - 8ms/step\n",
      "Epoch 4/10\n",
      "18936/18936 - 108s - loss: 4.6682 - val_loss: 12.3049 - 108s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "18936/18936 - 110s - loss: 4.5483 - val_loss: 11.7323 - 110s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "18936/18936 - 103s - loss: 4.3968 - val_loss: 11.9610 - 103s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "18936/18936 - 103s - loss: 4.3341 - val_loss: 11.0282 - 103s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "18936/18936 - 103s - loss: 4.2905 - val_loss: 12.6126 - 103s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "18936/18936 - 102s - loss: 4.2764 - val_loss: 10.8018 - 102s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "18936/18936 - 104s - loss: 4.1162 - val_loss: 10.4772 - 104s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37871/37871 - 186s - loss: 9.4208 - val_loss: 18.2735 - 186s/epoch - 5ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37871/37871 - 178s - loss: 8.3921 - val_loss: 18.6002 - 178s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "37871/37871 - 181s - loss: 7.7739 - val_loss: 14.7808 - 181s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "37871/37871 - 200s - loss: 7.3957 - val_loss: 14.9846 - 200s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "37871/37871 - 208s - loss: 7.0706 - val_loss: 14.3214 - 208s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "37871/37871 - 202s - loss: 7.0947 - val_loss: 14.0807 - 202s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "37871/37871 - 198s - loss: 7.0572 - val_loss: 13.9873 - 198s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "37871/37871 - 195s - loss: 6.9471 - val_loss: 13.9578 - 195s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "37871/37871 - 196s - loss: 6.8268 - val_loss: 13.7348 - 196s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "37871/37871 - 184s - loss: 6.8735 - val_loss: 14.0503 - 184s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56806/56806 - 284s - loss: 11.4507 - val_loss: 10.8435 - 284s/epoch - 5ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56806/56806 - 283s - loss: 10.0269 - val_loss: 10.5164 - 283s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "56806/56806 - 278s - loss: 9.5847 - val_loss: 12.5892 - 278s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "56806/56806 - 286s - loss: 9.2934 - val_loss: 10.4817 - 286s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "56806/56806 - 260s - loss: 9.1251 - val_loss: 10.1246 - 260s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "56806/56806 - 258s - loss: 9.0989 - val_loss: 10.9168 - 258s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "56806/56806 - 263s - loss: 8.8325 - val_loss: 10.3787 - 263s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "56806/56806 - 263s - loss: 8.7685 - val_loss: 9.8130 - 263s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "56806/56806 - 255s - loss: 8.5873 - val_loss: 9.7296 - 255s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "56806/56806 - 261s - loss: 8.4240 - val_loss: 9.5456 - 261s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75741/75741 - 346s - loss: 11.2651 - val_loss: 14.6112 - 346s/epoch - 5ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75741/75741 - 344s - loss: 9.8292 - val_loss: 14.0353 - 344s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "75741/75741 - 332s - loss: 9.6352 - val_loss: 14.1674 - 332s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "75741/75741 - 339s - loss: 9.4693 - val_loss: 14.1129 - 339s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "75741/75741 - 336s - loss: 9.3706 - val_loss: 14.5285 - 336s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "75741/75741 - 342s - loss: 9.1362 - val_loss: 13.5633 - 342s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "75741/75741 - 338s - loss: 8.9006 - val_loss: 13.7945 - 338s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "75741/75741 - 349s - loss: 8.7643 - val_loss: 13.5207 - 349s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "75741/75741 - 358s - loss: 8.8064 - val_loss: 14.7512 - 358s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "75741/75741 - 351s - loss: 8.7609 - val_loss: 13.4583 - 351s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94676/94676 - 442s - loss: 12.6320 - val_loss: 19.4011 - 442s/epoch - 5ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94676/94676 - 434s - loss: 10.6171 - val_loss: 17.0671 - 434s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "94676/94676 - 445s - loss: 10.3460 - val_loss: 17.3281 - 445s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "94676/94676 - 446s - loss: 10.1485 - val_loss: 17.6157 - 446s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "94676/94676 - 448s - loss: 10.1503 - val_loss: 17.2467 - 448s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "94676/94676 - 439s - loss: 9.9150 - val_loss: 16.7388 - 439s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "94676/94676 - 437s - loss: 9.7506 - val_loss: 16.7383 - 437s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "94676/94676 - 422s - loss: 9.6490 - val_loss: 16.5475 - 422s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "94676/94676 - 431s - loss: 9.4759 - val_loss: 15.8699 - 431s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "94676/94676 - 421s - loss: 9.4037 - val_loss: 16.9566 - 421s/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "from tcn import TCN, tcn_full_summary\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def create_tcn_model():\n",
    "    i = Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]))\n",
    "    o = TCN()(i)  # The TCN layers are here.\n",
    "    o = Dense(1)(o)\n",
    "\n",
    "    model = Model(inputs=[i], outputs=[o])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "best_tcn_model = None\n",
    "lowest_val_loss = float('inf')\n",
    "\n",
    "# Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, val_index in tscv.split(X_train_reshaped):\n",
    "    X_train_fold, X_val_fold = X_train_reshaped[train_index], X_train_reshaped[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model = create_tcn_model()\n",
    "    \n",
    "    # Use ModelCheckpoint to save the model with the lowest validation loss\n",
    "    checkpoint = ModelCheckpoint('best_tcn_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold), callbacks=[checkpoint], verbose=2, shuffle=False)\n",
    "    \n",
    "    # Check if this fold's model is better than the previous best model\n",
    "    val_loss = model.history.history['val_loss'][-1]\n",
    "    if val_loss < lowest_val_loss:\n",
    "        best_tcn_model = clone_model(model)\n",
    "        best_tcn_model.load_weights('best_tcn_model.h5')\n",
    "        best_tcn_model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        lowest_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step\n",
      "Average Relative Error: 3.8958285340656285\n",
      "RMSE on the validation set: 9.940727456157882\n"
     ]
    }
   ],
   "source": [
    "# Reshape X_test for the  model\n",
    "X_test_reshaped = scaler.transform(X_test).reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# After training, evaluate the best model on the test data\n",
    "y_pred = best_tcn_model.predict(X_test_reshaped)\n",
    "\n",
    "# Calculate the relative error\n",
    "epsilon = 1e-10\n",
    "errors = np.abs(y_pred.flatten() - y_test) / (y_pred.flatten() + epsilon)\n",
    "avg_relative_error = np.mean(errors)\n",
    "print(f\"Average Relative Error: {avg_relative_error}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE on the validation set: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/6d/d1/3e954de1d492129710e8625349a7b86eb287a4f413c5b5c15522f89a6c04/xgboost-2.0.0-py3-none-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading xgboost-2.0.0-py3-none-macosx_12_0_arm64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages (from xgboost) (1.10.1)\n",
      "Downloading xgboost-2.0.0-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m779.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.24259\teval-rmse:3.70757\n",
      "[1]\ttrain-rmse:2.07789\teval-rmse:3.52295\n",
      "[2]\ttrain-rmse:1.95434\teval-rmse:3.37590\n",
      "[3]\ttrain-rmse:1.88458\teval-rmse:3.28406\n",
      "[4]\ttrain-rmse:1.83964\teval-rmse:3.23960\n",
      "[5]\ttrain-rmse:1.81673\teval-rmse:3.20787\n",
      "[6]\ttrain-rmse:1.79549\teval-rmse:3.17433\n",
      "[7]\ttrain-rmse:1.74220\teval-rmse:3.08370\n",
      "[8]\ttrain-rmse:1.69331\teval-rmse:3.02406\n",
      "[9]\ttrain-rmse:1.67180\teval-rmse:3.00868\n",
      "[10]\ttrain-rmse:1.64855\teval-rmse:2.96799\n",
      "[11]\ttrain-rmse:1.63207\teval-rmse:2.95008\n",
      "[12]\ttrain-rmse:1.61228\teval-rmse:2.92715\n",
      "[13]\ttrain-rmse:1.60322\teval-rmse:2.91775\n",
      "[14]\ttrain-rmse:1.59025\teval-rmse:2.89504\n",
      "[15]\ttrain-rmse:1.57409\teval-rmse:2.86584\n",
      "[16]\ttrain-rmse:1.56421\teval-rmse:2.85464\n",
      "[17]\ttrain-rmse:1.55609\teval-rmse:2.84564\n",
      "[18]\ttrain-rmse:1.55042\teval-rmse:2.83917\n",
      "[19]\ttrain-rmse:1.53864\teval-rmse:2.83186\n",
      "[20]\ttrain-rmse:1.52876\teval-rmse:2.81935\n",
      "[21]\ttrain-rmse:1.52024\teval-rmse:2.80171\n",
      "[22]\ttrain-rmse:1.51203\teval-rmse:2.78994\n",
      "[23]\ttrain-rmse:1.50072\teval-rmse:2.78149\n",
      "[24]\ttrain-rmse:1.49199\teval-rmse:2.77258\n",
      "[25]\ttrain-rmse:1.48318\teval-rmse:2.77078\n",
      "[26]\ttrain-rmse:1.45663\teval-rmse:2.74236\n",
      "[27]\ttrain-rmse:1.45186\teval-rmse:2.73853\n",
      "[28]\ttrain-rmse:1.45057\teval-rmse:2.73859\n",
      "[29]\ttrain-rmse:1.44563\teval-rmse:2.73621\n",
      "[30]\ttrain-rmse:1.43258\teval-rmse:2.71940\n",
      "[31]\ttrain-rmse:1.42859\teval-rmse:2.71511\n",
      "[32]\ttrain-rmse:1.42537\teval-rmse:2.71491\n",
      "[33]\ttrain-rmse:1.42463\teval-rmse:2.71452\n",
      "[34]\ttrain-rmse:1.41749\teval-rmse:2.70474\n",
      "[35]\ttrain-rmse:1.41391\teval-rmse:2.70223\n",
      "[36]\ttrain-rmse:1.41091\teval-rmse:2.70047\n",
      "[37]\ttrain-rmse:1.40799\teval-rmse:2.69797\n",
      "[38]\ttrain-rmse:1.40109\teval-rmse:2.69158\n",
      "[39]\ttrain-rmse:1.39735\teval-rmse:2.68908\n",
      "[40]\ttrain-rmse:1.39312\teval-rmse:2.68515\n",
      "[41]\ttrain-rmse:1.39177\teval-rmse:2.68462\n",
      "[42]\ttrain-rmse:1.38564\teval-rmse:2.68348\n",
      "[43]\ttrain-rmse:1.37326\teval-rmse:2.67216\n",
      "[44]\ttrain-rmse:1.36999\teval-rmse:2.66743\n",
      "[45]\ttrain-rmse:1.36696\teval-rmse:2.66588\n",
      "[46]\ttrain-rmse:1.36487\teval-rmse:2.66554\n",
      "[47]\ttrain-rmse:1.35925\teval-rmse:2.66094\n",
      "[48]\ttrain-rmse:1.35499\teval-rmse:2.66130\n",
      "[49]\ttrain-rmse:1.34744\teval-rmse:2.65603\n",
      "[50]\ttrain-rmse:1.34515\teval-rmse:2.65130\n",
      "[51]\ttrain-rmse:1.34150\teval-rmse:2.64871\n",
      "[52]\ttrain-rmse:1.33966\teval-rmse:2.64692\n",
      "[53]\ttrain-rmse:1.33705\teval-rmse:2.64484\n",
      "[54]\ttrain-rmse:1.33427\teval-rmse:2.64004\n",
      "[55]\ttrain-rmse:1.32785\teval-rmse:2.63551\n",
      "[56]\ttrain-rmse:1.32509\teval-rmse:2.63535\n",
      "[57]\ttrain-rmse:1.32177\teval-rmse:2.63512\n",
      "[58]\ttrain-rmse:1.32017\teval-rmse:2.63412\n",
      "[59]\ttrain-rmse:1.31833\teval-rmse:2.63308\n",
      "[60]\ttrain-rmse:1.31483\teval-rmse:2.62751\n",
      "[61]\ttrain-rmse:1.31328\teval-rmse:2.62736\n",
      "[62]\ttrain-rmse:1.31199\teval-rmse:2.62651\n",
      "[63]\ttrain-rmse:1.30996\teval-rmse:2.62298\n",
      "[64]\ttrain-rmse:1.30875\teval-rmse:2.62337\n",
      "[65]\ttrain-rmse:1.30576\teval-rmse:2.62101\n",
      "[66]\ttrain-rmse:1.30247\teval-rmse:2.61765\n",
      "[67]\ttrain-rmse:1.30149\teval-rmse:2.61732\n",
      "[68]\ttrain-rmse:1.29949\teval-rmse:2.61697\n",
      "[69]\ttrain-rmse:1.29873\teval-rmse:2.61622\n",
      "[70]\ttrain-rmse:1.29761\teval-rmse:2.61608\n",
      "[71]\ttrain-rmse:1.29671\teval-rmse:2.61540\n",
      "[72]\ttrain-rmse:1.29519\teval-rmse:2.61446\n",
      "[73]\ttrain-rmse:1.29393\teval-rmse:2.61449\n",
      "[74]\ttrain-rmse:1.29091\teval-rmse:2.61386\n",
      "[75]\ttrain-rmse:1.28929\teval-rmse:2.61226\n",
      "[76]\ttrain-rmse:1.28679\teval-rmse:2.61030\n",
      "[77]\ttrain-rmse:1.28589\teval-rmse:2.60963\n",
      "[78]\ttrain-rmse:1.28320\teval-rmse:2.61099\n",
      "[79]\ttrain-rmse:1.27891\teval-rmse:2.60733\n",
      "[80]\ttrain-rmse:1.27785\teval-rmse:2.60644\n",
      "[81]\ttrain-rmse:1.27475\teval-rmse:2.60024\n",
      "[82]\ttrain-rmse:1.27155\teval-rmse:2.59993\n",
      "[83]\ttrain-rmse:1.26804\teval-rmse:2.59505\n",
      "[84]\ttrain-rmse:1.26511\teval-rmse:2.59434\n",
      "[85]\ttrain-rmse:1.26321\teval-rmse:2.59515\n",
      "[86]\ttrain-rmse:1.25945\teval-rmse:2.59464\n",
      "[87]\ttrain-rmse:1.25765\teval-rmse:2.59271\n",
      "[88]\ttrain-rmse:1.25636\teval-rmse:2.59255\n",
      "[89]\ttrain-rmse:1.25212\teval-rmse:2.59074\n",
      "[90]\ttrain-rmse:1.25173\teval-rmse:2.59067\n",
      "[91]\ttrain-rmse:1.25127\teval-rmse:2.59052\n",
      "[92]\ttrain-rmse:1.25040\teval-rmse:2.59009\n",
      "[93]\ttrain-rmse:1.24692\teval-rmse:2.58885\n",
      "[94]\ttrain-rmse:1.24540\teval-rmse:2.58810\n",
      "[95]\ttrain-rmse:1.24466\teval-rmse:2.58800\n",
      "[96]\ttrain-rmse:1.24422\teval-rmse:2.58793\n",
      "[97]\ttrain-rmse:1.24096\teval-rmse:2.58515\n",
      "[98]\ttrain-rmse:1.23953\teval-rmse:2.58424\n",
      "[99]\ttrain-rmse:1.23797\teval-rmse:2.58387\n",
      "[100]\ttrain-rmse:1.23756\teval-rmse:2.58383\n",
      "[101]\ttrain-rmse:1.23460\teval-rmse:2.58095\n",
      "[102]\ttrain-rmse:1.23263\teval-rmse:2.57732\n",
      "[103]\ttrain-rmse:1.23065\teval-rmse:2.57652\n",
      "[104]\ttrain-rmse:1.22956\teval-rmse:2.57648\n",
      "[105]\ttrain-rmse:1.22829\teval-rmse:2.57531\n",
      "[106]\ttrain-rmse:1.22128\teval-rmse:2.57210\n",
      "[107]\ttrain-rmse:1.21921\teval-rmse:2.57087\n",
      "[108]\ttrain-rmse:1.21646\teval-rmse:2.56881\n",
      "[109]\ttrain-rmse:1.21383\teval-rmse:2.56597\n",
      "[110]\ttrain-rmse:1.21258\teval-rmse:2.56545\n",
      "[111]\ttrain-rmse:1.21205\teval-rmse:2.56541\n",
      "[112]\ttrain-rmse:1.21141\teval-rmse:2.56525\n",
      "[113]\ttrain-rmse:1.21028\teval-rmse:2.56437\n",
      "[114]\ttrain-rmse:1.20936\teval-rmse:2.56438\n",
      "[115]\ttrain-rmse:1.20836\teval-rmse:2.56364\n",
      "[116]\ttrain-rmse:1.20757\teval-rmse:2.56338\n",
      "[117]\ttrain-rmse:1.20658\teval-rmse:2.56369\n",
      "[118]\ttrain-rmse:1.20581\teval-rmse:2.56340\n",
      "[119]\ttrain-rmse:1.20492\teval-rmse:2.56263\n",
      "[120]\ttrain-rmse:1.20444\teval-rmse:2.56241\n",
      "[121]\ttrain-rmse:1.20247\teval-rmse:2.56067\n",
      "[122]\ttrain-rmse:1.20128\teval-rmse:2.56084\n",
      "[123]\ttrain-rmse:1.19973\teval-rmse:2.55994\n",
      "[124]\ttrain-rmse:1.19927\teval-rmse:2.55975\n",
      "[125]\ttrain-rmse:1.19914\teval-rmse:2.55971\n",
      "[126]\ttrain-rmse:1.19859\teval-rmse:2.55998\n",
      "[127]\ttrain-rmse:1.19800\teval-rmse:2.55994\n",
      "[128]\ttrain-rmse:1.19709\teval-rmse:2.55920\n",
      "[129]\ttrain-rmse:1.19633\teval-rmse:2.55909\n",
      "[130]\ttrain-rmse:1.19520\teval-rmse:2.55842\n",
      "[131]\ttrain-rmse:1.19284\teval-rmse:2.55242\n",
      "[132]\ttrain-rmse:1.19204\teval-rmse:2.55225\n",
      "[133]\ttrain-rmse:1.18895\teval-rmse:2.55104\n",
      "[134]\ttrain-rmse:1.18705\teval-rmse:2.55150\n",
      "[135]\ttrain-rmse:1.18680\teval-rmse:2.55130\n",
      "[136]\ttrain-rmse:1.18657\teval-rmse:2.55120\n",
      "[137]\ttrain-rmse:1.18486\teval-rmse:2.55247\n",
      "[138]\ttrain-rmse:1.18422\teval-rmse:2.55186\n",
      "[139]\ttrain-rmse:1.18277\teval-rmse:2.55050\n",
      "[140]\ttrain-rmse:1.18227\teval-rmse:2.55061\n",
      "[141]\ttrain-rmse:1.18086\teval-rmse:2.55068\n",
      "[142]\ttrain-rmse:1.17898\teval-rmse:2.54939\n",
      "[143]\ttrain-rmse:1.17837\teval-rmse:2.54911\n",
      "[144]\ttrain-rmse:1.17777\teval-rmse:2.54901\n",
      "[145]\ttrain-rmse:1.17553\teval-rmse:2.54797\n",
      "[146]\ttrain-rmse:1.17503\teval-rmse:2.54791\n",
      "[147]\ttrain-rmse:1.17477\teval-rmse:2.54788\n",
      "[148]\ttrain-rmse:1.17386\teval-rmse:2.54801\n",
      "[149]\ttrain-rmse:1.17277\teval-rmse:2.54788\n",
      "[150]\ttrain-rmse:1.17112\teval-rmse:2.54472\n",
      "[151]\ttrain-rmse:1.17071\teval-rmse:2.54456\n",
      "[152]\ttrain-rmse:1.16701\teval-rmse:2.54385\n",
      "[153]\ttrain-rmse:1.16638\teval-rmse:2.54363\n",
      "[154]\ttrain-rmse:1.16492\teval-rmse:2.54184\n",
      "[155]\ttrain-rmse:1.16388\teval-rmse:2.54208\n",
      "[156]\ttrain-rmse:1.16263\teval-rmse:2.54246\n",
      "[157]\ttrain-rmse:1.16137\teval-rmse:2.54203\n",
      "[158]\ttrain-rmse:1.16111\teval-rmse:2.54202\n",
      "[159]\ttrain-rmse:1.15904\teval-rmse:2.54181\n",
      "[160]\ttrain-rmse:1.15870\teval-rmse:2.54176\n",
      "[161]\ttrain-rmse:1.15730\teval-rmse:2.54141\n",
      "[162]\ttrain-rmse:1.15713\teval-rmse:2.54135\n",
      "[163]\ttrain-rmse:1.15564\teval-rmse:2.53935\n",
      "[164]\ttrain-rmse:1.15449\teval-rmse:2.53886\n",
      "[165]\ttrain-rmse:1.15298\teval-rmse:2.53854\n",
      "[166]\ttrain-rmse:1.15129\teval-rmse:2.53783\n",
      "[167]\ttrain-rmse:1.15075\teval-rmse:2.53708\n",
      "[168]\ttrain-rmse:1.15037\teval-rmse:2.53714\n",
      "[169]\ttrain-rmse:1.14980\teval-rmse:2.53709\n",
      "[170]\ttrain-rmse:1.14851\teval-rmse:2.53598\n",
      "[171]\ttrain-rmse:1.14820\teval-rmse:2.53590\n",
      "[172]\ttrain-rmse:1.14792\teval-rmse:2.53575\n",
      "[173]\ttrain-rmse:1.14679\teval-rmse:2.53595\n",
      "[174]\ttrain-rmse:1.14640\teval-rmse:2.53588\n",
      "[175]\ttrain-rmse:1.14523\teval-rmse:2.53607\n",
      "[176]\ttrain-rmse:1.14440\teval-rmse:2.53596\n",
      "[177]\ttrain-rmse:1.14387\teval-rmse:2.53608\n",
      "[178]\ttrain-rmse:1.14336\teval-rmse:2.53612\n",
      "[179]\ttrain-rmse:1.14212\teval-rmse:2.53544\n",
      "[180]\ttrain-rmse:1.14101\teval-rmse:2.53521\n",
      "[181]\ttrain-rmse:1.14080\teval-rmse:2.53518\n",
      "[182]\ttrain-rmse:1.13778\teval-rmse:2.53480\n",
      "[183]\ttrain-rmse:1.13672\teval-rmse:2.53442\n",
      "[184]\ttrain-rmse:1.13615\teval-rmse:2.53453\n",
      "[185]\ttrain-rmse:1.13577\teval-rmse:2.53456\n",
      "[186]\ttrain-rmse:1.13484\teval-rmse:2.53426\n",
      "[187]\ttrain-rmse:1.13461\teval-rmse:2.53426\n",
      "[188]\ttrain-rmse:1.13432\teval-rmse:2.53419\n",
      "[189]\ttrain-rmse:1.13270\teval-rmse:2.53299\n",
      "[190]\ttrain-rmse:1.13246\teval-rmse:2.53297\n",
      "[191]\ttrain-rmse:1.13146\teval-rmse:2.53115\n",
      "[192]\ttrain-rmse:1.13057\teval-rmse:2.53116\n",
      "[193]\ttrain-rmse:1.12964\teval-rmse:2.53050\n",
      "[194]\ttrain-rmse:1.12920\teval-rmse:2.53048\n",
      "[195]\ttrain-rmse:1.12802\teval-rmse:2.53022\n",
      "[196]\ttrain-rmse:1.12728\teval-rmse:2.53032\n",
      "[197]\ttrain-rmse:1.12685\teval-rmse:2.53020\n",
      "[198]\ttrain-rmse:1.12612\teval-rmse:2.53045\n",
      "[199]\ttrain-rmse:1.12435\teval-rmse:2.53018\n",
      "[200]\ttrain-rmse:1.12279\teval-rmse:2.52977\n",
      "[201]\ttrain-rmse:1.12168\teval-rmse:2.52873\n",
      "[202]\ttrain-rmse:1.12120\teval-rmse:2.52867\n",
      "[203]\ttrain-rmse:1.12098\teval-rmse:2.52862\n",
      "[204]\ttrain-rmse:1.11934\teval-rmse:2.52837\n",
      "[205]\ttrain-rmse:1.11921\teval-rmse:2.52837\n",
      "[206]\ttrain-rmse:1.11914\teval-rmse:2.52834\n",
      "[207]\ttrain-rmse:1.11787\teval-rmse:2.52795\n",
      "[208]\ttrain-rmse:1.11759\teval-rmse:2.52789\n",
      "[209]\ttrain-rmse:1.11680\teval-rmse:2.52686\n",
      "[210]\ttrain-rmse:1.11566\teval-rmse:2.52699\n",
      "[211]\ttrain-rmse:1.11518\teval-rmse:2.52694\n",
      "[212]\ttrain-rmse:1.11472\teval-rmse:2.52692\n",
      "[213]\ttrain-rmse:1.11349\teval-rmse:2.52632\n",
      "[214]\ttrain-rmse:1.11319\teval-rmse:2.52624\n",
      "[215]\ttrain-rmse:1.11276\teval-rmse:2.52618\n",
      "[216]\ttrain-rmse:1.11239\teval-rmse:2.52604\n",
      "[217]\ttrain-rmse:1.11158\teval-rmse:2.52420\n",
      "[218]\ttrain-rmse:1.11111\teval-rmse:2.52405\n",
      "[219]\ttrain-rmse:1.11083\teval-rmse:2.52391\n",
      "[220]\ttrain-rmse:1.11036\teval-rmse:2.52401\n",
      "[221]\ttrain-rmse:1.10984\teval-rmse:2.52376\n",
      "[222]\ttrain-rmse:1.10927\teval-rmse:2.52342\n",
      "[223]\ttrain-rmse:1.10876\teval-rmse:2.52320\n",
      "[224]\ttrain-rmse:1.10836\teval-rmse:2.52312\n",
      "[225]\ttrain-rmse:1.10819\teval-rmse:2.52313\n",
      "[226]\ttrain-rmse:1.10805\teval-rmse:2.52301\n",
      "[227]\ttrain-rmse:1.10701\teval-rmse:2.52344\n",
      "[228]\ttrain-rmse:1.10674\teval-rmse:2.52330\n",
      "[229]\ttrain-rmse:1.10601\teval-rmse:2.52285\n",
      "[230]\ttrain-rmse:1.10581\teval-rmse:2.52282\n",
      "[231]\ttrain-rmse:1.10549\teval-rmse:2.52298\n",
      "[232]\ttrain-rmse:1.10534\teval-rmse:2.52297\n",
      "[233]\ttrain-rmse:1.10504\teval-rmse:2.52296\n",
      "[234]\ttrain-rmse:1.10425\teval-rmse:2.52304\n",
      "[235]\ttrain-rmse:1.10391\teval-rmse:2.52285\n",
      "[236]\ttrain-rmse:1.10379\teval-rmse:2.52285\n",
      "[237]\ttrain-rmse:1.10284\teval-rmse:2.52261\n",
      "[238]\ttrain-rmse:1.10174\teval-rmse:2.52233\n",
      "[239]\ttrain-rmse:1.10137\teval-rmse:2.52230\n",
      "[240]\ttrain-rmse:1.10097\teval-rmse:2.52198\n",
      "[241]\ttrain-rmse:1.10046\teval-rmse:2.52167\n",
      "[242]\ttrain-rmse:1.09946\teval-rmse:2.52167\n",
      "[243]\ttrain-rmse:1.09859\teval-rmse:2.52135\n",
      "[244]\ttrain-rmse:1.09812\teval-rmse:2.52125\n",
      "[245]\ttrain-rmse:1.09784\teval-rmse:2.52125\n",
      "[246]\ttrain-rmse:1.09609\teval-rmse:2.51996\n",
      "[247]\ttrain-rmse:1.09568\teval-rmse:2.51981\n",
      "[248]\ttrain-rmse:1.09555\teval-rmse:2.51980\n",
      "[249]\ttrain-rmse:1.09506\teval-rmse:2.51971\n",
      "[250]\ttrain-rmse:1.09432\teval-rmse:2.51957\n",
      "[251]\ttrain-rmse:1.09393\teval-rmse:2.51954\n",
      "[252]\ttrain-rmse:1.09271\teval-rmse:2.51938\n",
      "[253]\ttrain-rmse:1.09238\teval-rmse:2.51963\n",
      "[254]\ttrain-rmse:1.09203\teval-rmse:2.51952\n",
      "[255]\ttrain-rmse:1.09186\teval-rmse:2.51942\n",
      "[256]\ttrain-rmse:1.09175\teval-rmse:2.51941\n",
      "[257]\ttrain-rmse:1.09081\teval-rmse:2.51951\n",
      "[258]\ttrain-rmse:1.09040\teval-rmse:2.51939\n",
      "[259]\ttrain-rmse:1.09006\teval-rmse:2.51935\n",
      "[260]\ttrain-rmse:1.08917\teval-rmse:2.51916\n",
      "[261]\ttrain-rmse:1.08868\teval-rmse:2.51935\n",
      "[262]\ttrain-rmse:1.08834\teval-rmse:2.51935\n",
      "[263]\ttrain-rmse:1.08815\teval-rmse:2.51933\n",
      "[264]\ttrain-rmse:1.08580\teval-rmse:2.51910\n",
      "[265]\ttrain-rmse:1.08556\teval-rmse:2.51911\n",
      "[266]\ttrain-rmse:1.08536\teval-rmse:2.51908\n",
      "[267]\ttrain-rmse:1.08415\teval-rmse:2.51882\n",
      "[268]\ttrain-rmse:1.08375\teval-rmse:2.51879\n",
      "[269]\ttrain-rmse:1.08354\teval-rmse:2.51879\n",
      "[270]\ttrain-rmse:1.08277\teval-rmse:2.51895\n",
      "[271]\ttrain-rmse:1.08165\teval-rmse:2.51917\n",
      "[272]\ttrain-rmse:1.08073\teval-rmse:2.51945\n",
      "[273]\ttrain-rmse:1.07953\teval-rmse:2.51885\n",
      "[274]\ttrain-rmse:1.07912\teval-rmse:2.51829\n",
      "[275]\ttrain-rmse:1.07889\teval-rmse:2.51819\n",
      "[276]\ttrain-rmse:1.07856\teval-rmse:2.51806\n",
      "[277]\ttrain-rmse:1.07825\teval-rmse:2.51727\n",
      "[278]\ttrain-rmse:1.07641\teval-rmse:2.51746\n",
      "[279]\ttrain-rmse:1.07534\teval-rmse:2.51689\n",
      "[280]\ttrain-rmse:1.07474\teval-rmse:2.51698\n",
      "[281]\ttrain-rmse:1.07450\teval-rmse:2.51704\n",
      "[282]\ttrain-rmse:1.07403\teval-rmse:2.51660\n",
      "[283]\ttrain-rmse:1.07314\teval-rmse:2.51655\n",
      "[284]\ttrain-rmse:1.07257\teval-rmse:2.51670\n",
      "[285]\ttrain-rmse:1.07219\teval-rmse:2.51687\n",
      "[286]\ttrain-rmse:1.07190\teval-rmse:2.51683\n",
      "[287]\ttrain-rmse:1.07124\teval-rmse:2.51672\n",
      "[288]\ttrain-rmse:1.07109\teval-rmse:2.51676\n",
      "[289]\ttrain-rmse:1.07096\teval-rmse:2.51671\n",
      "[290]\ttrain-rmse:1.07064\teval-rmse:2.51632\n",
      "[291]\ttrain-rmse:1.07058\teval-rmse:2.51630\n",
      "[292]\ttrain-rmse:1.06986\teval-rmse:2.51663\n",
      "[293]\ttrain-rmse:1.06955\teval-rmse:2.51665\n",
      "[294]\ttrain-rmse:1.06888\teval-rmse:2.51684\n",
      "[295]\ttrain-rmse:1.06831\teval-rmse:2.51674\n",
      "[296]\ttrain-rmse:1.06808\teval-rmse:2.51673\n",
      "[297]\ttrain-rmse:1.06774\teval-rmse:2.51653\n",
      "[298]\ttrain-rmse:1.06758\teval-rmse:2.51692\n",
      "[299]\ttrain-rmse:1.06735\teval-rmse:2.51679\n",
      "[300]\ttrain-rmse:1.06587\teval-rmse:2.51669\n",
      "[301]\ttrain-rmse:1.06523\teval-rmse:2.51713\n",
      "[0]\ttrain-rmse:2.97099\teval-rmse:4.29854\n",
      "[1]\ttrain-rmse:2.73568\teval-rmse:4.04586\n",
      "[2]\ttrain-rmse:2.59287\teval-rmse:3.87256\n",
      "[3]\ttrain-rmse:2.51468\teval-rmse:3.77911\n",
      "[4]\ttrain-rmse:2.43877\teval-rmse:3.69746\n",
      "[5]\ttrain-rmse:2.36367\teval-rmse:3.57818\n",
      "[6]\ttrain-rmse:2.30846\teval-rmse:3.53960\n",
      "[7]\ttrain-rmse:2.25934\teval-rmse:3.43399\n",
      "[8]\ttrain-rmse:2.24333\teval-rmse:3.42220\n",
      "[9]\ttrain-rmse:2.20502\teval-rmse:3.37457\n",
      "[10]\ttrain-rmse:2.17283\teval-rmse:3.31565\n",
      "[11]\ttrain-rmse:2.14718\teval-rmse:3.29547\n",
      "[12]\ttrain-rmse:2.13764\teval-rmse:3.28169\n",
      "[13]\ttrain-rmse:2.12752\teval-rmse:3.27062\n",
      "[14]\ttrain-rmse:2.11738\teval-rmse:3.25628\n",
      "[15]\ttrain-rmse:2.10853\teval-rmse:3.24637\n",
      "[16]\ttrain-rmse:2.09367\teval-rmse:3.21787\n",
      "[17]\ttrain-rmse:2.08375\teval-rmse:3.21564\n",
      "[18]\ttrain-rmse:2.05627\teval-rmse:3.19780\n",
      "[19]\ttrain-rmse:2.05127\teval-rmse:3.19330\n",
      "[20]\ttrain-rmse:2.04064\teval-rmse:3.18182\n",
      "[21]\ttrain-rmse:2.02518\teval-rmse:3.17676\n",
      "[22]\ttrain-rmse:2.02193\teval-rmse:3.17334\n",
      "[23]\ttrain-rmse:2.01173\teval-rmse:3.16970\n",
      "[24]\ttrain-rmse:2.00579\teval-rmse:3.15128\n",
      "[25]\ttrain-rmse:1.99156\teval-rmse:3.13975\n",
      "[26]\ttrain-rmse:1.98748\teval-rmse:3.13688\n",
      "[27]\ttrain-rmse:1.98484\teval-rmse:3.13630\n",
      "[28]\ttrain-rmse:1.97904\teval-rmse:3.13062\n",
      "[29]\ttrain-rmse:1.97715\teval-rmse:3.12945\n",
      "[30]\ttrain-rmse:1.96325\teval-rmse:3.11296\n",
      "[31]\ttrain-rmse:1.95970\teval-rmse:3.10952\n",
      "[32]\ttrain-rmse:1.95617\teval-rmse:3.10828\n",
      "[33]\ttrain-rmse:1.95160\teval-rmse:3.10472\n",
      "[34]\ttrain-rmse:1.94582\teval-rmse:3.09774\n",
      "[35]\ttrain-rmse:1.93844\teval-rmse:3.09480\n",
      "[36]\ttrain-rmse:1.92642\teval-rmse:3.08688\n",
      "[37]\ttrain-rmse:1.92206\teval-rmse:3.08296\n",
      "[38]\ttrain-rmse:1.91602\teval-rmse:3.07959\n",
      "[39]\ttrain-rmse:1.91070\teval-rmse:3.07576\n",
      "[40]\ttrain-rmse:1.90746\teval-rmse:3.07428\n",
      "[41]\ttrain-rmse:1.90664\teval-rmse:3.07300\n",
      "[42]\ttrain-rmse:1.90453\teval-rmse:3.07329\n",
      "[43]\ttrain-rmse:1.90103\teval-rmse:3.07007\n",
      "[44]\ttrain-rmse:1.88994\teval-rmse:3.06214\n",
      "[45]\ttrain-rmse:1.88481\teval-rmse:3.06094\n",
      "[46]\ttrain-rmse:1.88213\teval-rmse:3.05920\n",
      "[47]\ttrain-rmse:1.87575\teval-rmse:3.05597\n",
      "[48]\ttrain-rmse:1.87153\teval-rmse:3.05218\n",
      "[49]\ttrain-rmse:1.86705\teval-rmse:3.04914\n",
      "[50]\ttrain-rmse:1.86356\teval-rmse:3.04568\n",
      "[51]\ttrain-rmse:1.85744\teval-rmse:3.04271\n",
      "[52]\ttrain-rmse:1.85181\teval-rmse:3.03832\n",
      "[53]\ttrain-rmse:1.84954\teval-rmse:3.03605\n",
      "[54]\ttrain-rmse:1.84773\teval-rmse:3.03482\n",
      "[55]\ttrain-rmse:1.84513\teval-rmse:3.03416\n",
      "[56]\ttrain-rmse:1.84393\teval-rmse:3.03310\n",
      "[57]\ttrain-rmse:1.84313\teval-rmse:3.03270\n",
      "[58]\ttrain-rmse:1.84255\teval-rmse:3.03237\n",
      "[59]\ttrain-rmse:1.84082\teval-rmse:3.03175\n",
      "[60]\ttrain-rmse:1.83751\teval-rmse:3.03114\n",
      "[61]\ttrain-rmse:1.83480\teval-rmse:3.02772\n",
      "[62]\ttrain-rmse:1.83243\teval-rmse:3.02488\n",
      "[63]\ttrain-rmse:1.83086\teval-rmse:3.02387\n",
      "[64]\ttrain-rmse:1.82980\teval-rmse:3.02329\n",
      "[65]\ttrain-rmse:1.82751\teval-rmse:3.02182\n",
      "[66]\ttrain-rmse:1.82477\teval-rmse:3.02130\n",
      "[67]\ttrain-rmse:1.81677\teval-rmse:3.01566\n",
      "[68]\ttrain-rmse:1.81394\teval-rmse:3.01307\n",
      "[69]\ttrain-rmse:1.81285\teval-rmse:3.01072\n",
      "[70]\ttrain-rmse:1.81158\teval-rmse:3.00757\n",
      "[71]\ttrain-rmse:1.80968\teval-rmse:3.00640\n",
      "[72]\ttrain-rmse:1.80475\teval-rmse:3.00259\n",
      "[73]\ttrain-rmse:1.79968\teval-rmse:3.00005\n",
      "[74]\ttrain-rmse:1.79893\teval-rmse:2.99949\n",
      "[75]\ttrain-rmse:1.79638\teval-rmse:2.99869\n",
      "[76]\ttrain-rmse:1.79558\teval-rmse:2.99857\n",
      "[77]\ttrain-rmse:1.79164\teval-rmse:2.99703\n",
      "[78]\ttrain-rmse:1.79035\teval-rmse:2.99641\n",
      "[79]\ttrain-rmse:1.78794\teval-rmse:2.99630\n",
      "[80]\ttrain-rmse:1.78743\teval-rmse:2.99636\n",
      "[81]\ttrain-rmse:1.78357\teval-rmse:2.99229\n",
      "[82]\ttrain-rmse:1.78315\teval-rmse:2.99262\n",
      "[83]\ttrain-rmse:1.78157\teval-rmse:2.99125\n",
      "[84]\ttrain-rmse:1.77987\teval-rmse:2.98895\n",
      "[85]\ttrain-rmse:1.77902\teval-rmse:2.98768\n",
      "[86]\ttrain-rmse:1.77813\teval-rmse:2.98756\n",
      "[87]\ttrain-rmse:1.77761\teval-rmse:2.98744\n",
      "[88]\ttrain-rmse:1.77589\teval-rmse:2.98629\n",
      "[89]\ttrain-rmse:1.77418\teval-rmse:2.98485\n",
      "[90]\ttrain-rmse:1.77293\teval-rmse:2.98427\n",
      "[91]\ttrain-rmse:1.77082\teval-rmse:2.98303\n",
      "[92]\ttrain-rmse:1.76984\teval-rmse:2.98262\n",
      "[93]\ttrain-rmse:1.76577\teval-rmse:2.97519\n",
      "[94]\ttrain-rmse:1.76435\teval-rmse:2.97585\n",
      "[95]\ttrain-rmse:1.76311\teval-rmse:2.97532\n",
      "[96]\ttrain-rmse:1.76112\teval-rmse:2.97368\n",
      "[97]\ttrain-rmse:1.76060\teval-rmse:2.97352\n",
      "[98]\ttrain-rmse:1.75976\teval-rmse:2.97211\n",
      "[99]\ttrain-rmse:1.75885\teval-rmse:2.97156\n",
      "[100]\ttrain-rmse:1.75445\teval-rmse:2.96818\n",
      "[101]\ttrain-rmse:1.75191\teval-rmse:2.96178\n",
      "[102]\ttrain-rmse:1.74938\teval-rmse:2.96133\n",
      "[103]\ttrain-rmse:1.74922\teval-rmse:2.96129\n",
      "[104]\ttrain-rmse:1.74703\teval-rmse:2.96187\n",
      "[105]\ttrain-rmse:1.74290\teval-rmse:2.96076\n",
      "[106]\ttrain-rmse:1.74190\teval-rmse:2.96055\n",
      "[107]\ttrain-rmse:1.74096\teval-rmse:2.95726\n",
      "[108]\ttrain-rmse:1.73908\teval-rmse:2.95526\n",
      "[109]\ttrain-rmse:1.73668\teval-rmse:2.95297\n",
      "[110]\ttrain-rmse:1.73568\teval-rmse:2.95224\n",
      "[111]\ttrain-rmse:1.73507\teval-rmse:2.95213\n",
      "[112]\ttrain-rmse:1.73254\teval-rmse:2.95029\n",
      "[113]\ttrain-rmse:1.72731\teval-rmse:2.94966\n",
      "[114]\ttrain-rmse:1.72610\teval-rmse:2.94821\n",
      "[115]\ttrain-rmse:1.72433\teval-rmse:2.94747\n",
      "[116]\ttrain-rmse:1.72197\teval-rmse:2.94703\n",
      "[117]\ttrain-rmse:1.72014\teval-rmse:2.94685\n",
      "[118]\ttrain-rmse:1.71953\teval-rmse:2.94649\n",
      "[119]\ttrain-rmse:1.71476\teval-rmse:2.94295\n",
      "[120]\ttrain-rmse:1.71219\teval-rmse:2.94186\n",
      "[121]\ttrain-rmse:1.71170\teval-rmse:2.94191\n",
      "[122]\ttrain-rmse:1.71038\teval-rmse:2.94070\n",
      "[123]\ttrain-rmse:1.70724\teval-rmse:2.93964\n",
      "[124]\ttrain-rmse:1.70472\teval-rmse:2.93728\n",
      "[125]\ttrain-rmse:1.70325\teval-rmse:2.93708\n",
      "[126]\ttrain-rmse:1.70259\teval-rmse:2.93679\n",
      "[127]\ttrain-rmse:1.70203\teval-rmse:2.93646\n",
      "[128]\ttrain-rmse:1.70166\teval-rmse:2.93669\n",
      "[129]\ttrain-rmse:1.69756\teval-rmse:2.93466\n",
      "[130]\ttrain-rmse:1.69680\teval-rmse:2.93301\n",
      "[131]\ttrain-rmse:1.69440\teval-rmse:2.93185\n",
      "[132]\ttrain-rmse:1.69418\teval-rmse:2.93186\n",
      "[133]\ttrain-rmse:1.69367\teval-rmse:2.93169\n",
      "[134]\ttrain-rmse:1.69339\teval-rmse:2.93188\n",
      "[135]\ttrain-rmse:1.69273\teval-rmse:2.93171\n",
      "[136]\ttrain-rmse:1.69253\teval-rmse:2.93183\n",
      "[137]\ttrain-rmse:1.69219\teval-rmse:2.93175\n",
      "[138]\ttrain-rmse:1.69013\teval-rmse:2.93207\n",
      "[139]\ttrain-rmse:1.68975\teval-rmse:2.93204\n",
      "[140]\ttrain-rmse:1.68849\teval-rmse:2.93201\n",
      "[141]\ttrain-rmse:1.68759\teval-rmse:2.93151\n",
      "[142]\ttrain-rmse:1.68690\teval-rmse:2.93122\n",
      "[143]\ttrain-rmse:1.68665\teval-rmse:2.93122\n",
      "[144]\ttrain-rmse:1.68531\teval-rmse:2.93079\n",
      "[145]\ttrain-rmse:1.68363\teval-rmse:2.92989\n",
      "[146]\ttrain-rmse:1.68296\teval-rmse:2.92762\n",
      "[147]\ttrain-rmse:1.68257\teval-rmse:2.92725\n",
      "[148]\ttrain-rmse:1.68242\teval-rmse:2.92723\n",
      "[149]\ttrain-rmse:1.68095\teval-rmse:2.92606\n",
      "[150]\ttrain-rmse:1.68030\teval-rmse:2.92568\n",
      "[151]\ttrain-rmse:1.67992\teval-rmse:2.92569\n",
      "[152]\ttrain-rmse:1.67789\teval-rmse:2.92530\n",
      "[153]\ttrain-rmse:1.67743\teval-rmse:2.92515\n",
      "[154]\ttrain-rmse:1.67614\teval-rmse:2.92364\n",
      "[155]\ttrain-rmse:1.67569\teval-rmse:2.92405\n",
      "[156]\ttrain-rmse:1.67540\teval-rmse:2.92400\n",
      "[157]\ttrain-rmse:1.67421\teval-rmse:2.92381\n",
      "[158]\ttrain-rmse:1.67293\teval-rmse:2.92154\n",
      "[159]\ttrain-rmse:1.67192\teval-rmse:2.92084\n",
      "[160]\ttrain-rmse:1.67145\teval-rmse:2.92056\n",
      "[161]\ttrain-rmse:1.67105\teval-rmse:2.92059\n",
      "[162]\ttrain-rmse:1.67019\teval-rmse:2.92004\n",
      "[163]\ttrain-rmse:1.66996\teval-rmse:2.91994\n",
      "[164]\ttrain-rmse:1.66915\teval-rmse:2.91996\n",
      "[165]\ttrain-rmse:1.66669\teval-rmse:2.91840\n",
      "[166]\ttrain-rmse:1.66589\teval-rmse:2.91813\n",
      "[167]\ttrain-rmse:1.66501\teval-rmse:2.91722\n",
      "[168]\ttrain-rmse:1.66410\teval-rmse:2.91682\n",
      "[169]\ttrain-rmse:1.66343\teval-rmse:2.91643\n",
      "[170]\ttrain-rmse:1.66058\teval-rmse:2.91600\n",
      "[171]\ttrain-rmse:1.65897\teval-rmse:2.91582\n",
      "[172]\ttrain-rmse:1.65860\teval-rmse:2.91564\n",
      "[173]\ttrain-rmse:1.65404\teval-rmse:2.91592\n",
      "[174]\ttrain-rmse:1.65113\teval-rmse:2.91515\n",
      "[175]\ttrain-rmse:1.65079\teval-rmse:2.91513\n",
      "[176]\ttrain-rmse:1.64912\teval-rmse:2.91440\n",
      "[177]\ttrain-rmse:1.64848\teval-rmse:2.91390\n",
      "[178]\ttrain-rmse:1.64837\teval-rmse:2.91391\n",
      "[179]\ttrain-rmse:1.64644\teval-rmse:2.91438\n",
      "[180]\ttrain-rmse:1.64628\teval-rmse:2.91435\n",
      "[181]\ttrain-rmse:1.64574\teval-rmse:2.91425\n",
      "[182]\ttrain-rmse:1.64418\teval-rmse:2.91345\n",
      "[183]\ttrain-rmse:1.64343\teval-rmse:2.91311\n",
      "[184]\ttrain-rmse:1.64190\teval-rmse:2.91355\n",
      "[185]\ttrain-rmse:1.64086\teval-rmse:2.91199\n",
      "[186]\ttrain-rmse:1.63829\teval-rmse:2.91270\n",
      "[187]\ttrain-rmse:1.63795\teval-rmse:2.91263\n",
      "[188]\ttrain-rmse:1.63693\teval-rmse:2.91243\n",
      "[189]\ttrain-rmse:1.63643\teval-rmse:2.91263\n",
      "[190]\ttrain-rmse:1.63615\teval-rmse:2.91276\n",
      "[191]\ttrain-rmse:1.63457\teval-rmse:2.91258\n",
      "[192]\ttrain-rmse:1.63414\teval-rmse:2.91253\n",
      "[193]\ttrain-rmse:1.63377\teval-rmse:2.91262\n",
      "[194]\ttrain-rmse:1.63351\teval-rmse:2.91260\n",
      "[0]\ttrain-rmse:3.44429\teval-rmse:3.54427\n",
      "[1]\ttrain-rmse:3.19115\teval-rmse:3.30240\n",
      "[2]\ttrain-rmse:3.02733\teval-rmse:3.13352\n",
      "[3]\ttrain-rmse:2.90849\teval-rmse:3.01875\n",
      "[4]\ttrain-rmse:2.83558\teval-rmse:2.95059\n",
      "[5]\ttrain-rmse:2.76103\teval-rmse:2.88307\n",
      "[6]\ttrain-rmse:2.71657\teval-rmse:2.84388\n",
      "[7]\ttrain-rmse:2.65884\teval-rmse:2.81350\n",
      "[8]\ttrain-rmse:2.64099\teval-rmse:2.79562\n",
      "[9]\ttrain-rmse:2.60813\teval-rmse:2.76581\n",
      "[10]\ttrain-rmse:2.58575\teval-rmse:2.74969\n",
      "[11]\ttrain-rmse:2.55968\teval-rmse:2.72771\n",
      "[12]\ttrain-rmse:2.54707\teval-rmse:2.71802\n",
      "[13]\ttrain-rmse:2.53144\teval-rmse:2.70544\n",
      "[14]\ttrain-rmse:2.50358\teval-rmse:2.70153\n",
      "[15]\ttrain-rmse:2.49390\teval-rmse:2.69300\n",
      "[16]\ttrain-rmse:2.47107\teval-rmse:2.66276\n",
      "[17]\ttrain-rmse:2.46372\teval-rmse:2.65593\n",
      "[18]\ttrain-rmse:2.45829\teval-rmse:2.65177\n",
      "[19]\ttrain-rmse:2.43868\teval-rmse:2.64066\n",
      "[20]\ttrain-rmse:2.43380\teval-rmse:2.63601\n",
      "[21]\ttrain-rmse:2.41822\teval-rmse:2.62546\n",
      "[22]\ttrain-rmse:2.41395\teval-rmse:2.62296\n",
      "[23]\ttrain-rmse:2.39259\teval-rmse:2.60051\n",
      "[24]\ttrain-rmse:2.38350\teval-rmse:2.59446\n",
      "[25]\ttrain-rmse:2.37561\teval-rmse:2.59069\n",
      "[26]\ttrain-rmse:2.36401\teval-rmse:2.58374\n",
      "[27]\ttrain-rmse:2.34092\teval-rmse:2.57339\n",
      "[28]\ttrain-rmse:2.31509\teval-rmse:2.56064\n",
      "[29]\ttrain-rmse:2.31359\teval-rmse:2.55959\n",
      "[30]\ttrain-rmse:2.30601\teval-rmse:2.55348\n",
      "[31]\ttrain-rmse:2.30416\teval-rmse:2.55260\n",
      "[32]\ttrain-rmse:2.29499\teval-rmse:2.54199\n",
      "[33]\ttrain-rmse:2.28745\teval-rmse:2.53694\n",
      "[34]\ttrain-rmse:2.28560\teval-rmse:2.53606\n",
      "[35]\ttrain-rmse:2.28390\teval-rmse:2.53505\n",
      "[36]\ttrain-rmse:2.27338\teval-rmse:2.53323\n",
      "[37]\ttrain-rmse:2.27083\teval-rmse:2.53114\n",
      "[38]\ttrain-rmse:2.26589\teval-rmse:2.52839\n",
      "[39]\ttrain-rmse:2.25715\teval-rmse:2.52640\n",
      "[40]\ttrain-rmse:2.25269\teval-rmse:2.52487\n",
      "[41]\ttrain-rmse:2.25077\teval-rmse:2.52374\n",
      "[42]\ttrain-rmse:2.24787\teval-rmse:2.52224\n",
      "[43]\ttrain-rmse:2.24482\teval-rmse:2.52089\n",
      "[44]\ttrain-rmse:2.24385\teval-rmse:2.52050\n",
      "[45]\ttrain-rmse:2.24279\teval-rmse:2.51980\n",
      "[46]\ttrain-rmse:2.24111\teval-rmse:2.51970\n",
      "[47]\ttrain-rmse:2.23712\teval-rmse:2.51919\n",
      "[48]\ttrain-rmse:2.23308\teval-rmse:2.51869\n",
      "[49]\ttrain-rmse:2.23253\teval-rmse:2.51827\n",
      "[50]\ttrain-rmse:2.23036\teval-rmse:2.51707\n",
      "[51]\ttrain-rmse:2.21968\teval-rmse:2.51832\n",
      "[52]\ttrain-rmse:2.21815\teval-rmse:2.51749\n",
      "[53]\ttrain-rmse:2.21727\teval-rmse:2.51715\n",
      "[54]\ttrain-rmse:2.21431\teval-rmse:2.51600\n",
      "[55]\ttrain-rmse:2.20706\teval-rmse:2.51380\n",
      "[56]\ttrain-rmse:2.20383\teval-rmse:2.51118\n",
      "[57]\ttrain-rmse:2.20301\teval-rmse:2.51054\n",
      "[58]\ttrain-rmse:2.20173\teval-rmse:2.51013\n",
      "[59]\ttrain-rmse:2.19954\teval-rmse:2.50835\n",
      "[60]\ttrain-rmse:2.19735\teval-rmse:2.50692\n",
      "[61]\ttrain-rmse:2.19448\teval-rmse:2.50613\n",
      "[62]\ttrain-rmse:2.19222\teval-rmse:2.50446\n",
      "[63]\ttrain-rmse:2.18478\teval-rmse:2.50379\n",
      "[64]\ttrain-rmse:2.17939\teval-rmse:2.50485\n",
      "[65]\ttrain-rmse:2.17630\teval-rmse:2.50243\n",
      "[66]\ttrain-rmse:2.17414\teval-rmse:2.50143\n",
      "[67]\ttrain-rmse:2.16876\teval-rmse:2.50031\n",
      "[68]\ttrain-rmse:2.16767\teval-rmse:2.49964\n",
      "[69]\ttrain-rmse:2.16580\teval-rmse:2.49936\n",
      "[70]\ttrain-rmse:2.16425\teval-rmse:2.49831\n",
      "[71]\ttrain-rmse:2.16386\teval-rmse:2.49814\n",
      "[72]\ttrain-rmse:2.16242\teval-rmse:2.49747\n",
      "[73]\ttrain-rmse:2.15922\teval-rmse:2.49665\n",
      "[74]\ttrain-rmse:2.15795\teval-rmse:2.49591\n",
      "[75]\ttrain-rmse:2.15588\teval-rmse:2.49550\n",
      "[76]\ttrain-rmse:2.15301\teval-rmse:2.49512\n",
      "[77]\ttrain-rmse:2.15090\teval-rmse:2.49330\n",
      "[78]\ttrain-rmse:2.14458\teval-rmse:2.49005\n",
      "[79]\ttrain-rmse:2.14243\teval-rmse:2.48954\n",
      "[80]\ttrain-rmse:2.14150\teval-rmse:2.48879\n",
      "[81]\ttrain-rmse:2.14055\teval-rmse:2.48818\n",
      "[82]\ttrain-rmse:2.14009\teval-rmse:2.48786\n",
      "[83]\ttrain-rmse:2.13459\teval-rmse:2.48810\n",
      "[84]\ttrain-rmse:2.13292\teval-rmse:2.48722\n",
      "[85]\ttrain-rmse:2.12976\teval-rmse:2.48669\n",
      "[86]\ttrain-rmse:2.12570\teval-rmse:2.48629\n",
      "[87]\ttrain-rmse:2.12419\teval-rmse:2.48596\n",
      "[88]\ttrain-rmse:2.12366\teval-rmse:2.48567\n",
      "[89]\ttrain-rmse:2.12283\teval-rmse:2.48441\n",
      "[90]\ttrain-rmse:2.12230\teval-rmse:2.48397\n",
      "[91]\ttrain-rmse:2.12162\teval-rmse:2.48355\n",
      "[92]\ttrain-rmse:2.12120\teval-rmse:2.48314\n",
      "[93]\ttrain-rmse:2.11888\teval-rmse:2.48378\n",
      "[94]\ttrain-rmse:2.11428\teval-rmse:2.47925\n",
      "[95]\ttrain-rmse:2.11337\teval-rmse:2.47897\n",
      "[96]\ttrain-rmse:2.11219\teval-rmse:2.47861\n",
      "[97]\ttrain-rmse:2.11069\teval-rmse:2.47765\n",
      "[98]\ttrain-rmse:2.11044\teval-rmse:2.47748\n",
      "[99]\ttrain-rmse:2.10829\teval-rmse:2.47701\n",
      "[100]\ttrain-rmse:2.10789\teval-rmse:2.47696\n",
      "[101]\ttrain-rmse:2.10397\teval-rmse:2.47613\n",
      "[102]\ttrain-rmse:2.09842\teval-rmse:2.47554\n",
      "[103]\ttrain-rmse:2.09804\teval-rmse:2.47533\n",
      "[104]\ttrain-rmse:2.09640\teval-rmse:2.47469\n",
      "[105]\ttrain-rmse:2.09601\teval-rmse:2.47437\n",
      "[106]\ttrain-rmse:2.09250\teval-rmse:2.47646\n",
      "[107]\ttrain-rmse:2.08973\teval-rmse:2.47393\n",
      "[108]\ttrain-rmse:2.08826\teval-rmse:2.47364\n",
      "[109]\ttrain-rmse:2.08668\teval-rmse:2.47326\n",
      "[110]\ttrain-rmse:2.08627\teval-rmse:2.47329\n",
      "[111]\ttrain-rmse:2.08515\teval-rmse:2.47273\n",
      "[112]\ttrain-rmse:2.08466\teval-rmse:2.47248\n",
      "[113]\ttrain-rmse:2.08415\teval-rmse:2.47237\n",
      "[114]\ttrain-rmse:2.08385\teval-rmse:2.47231\n",
      "[115]\ttrain-rmse:2.08295\teval-rmse:2.47184\n",
      "[116]\ttrain-rmse:2.08150\teval-rmse:2.47038\n",
      "[117]\ttrain-rmse:2.07804\teval-rmse:2.46822\n",
      "[118]\ttrain-rmse:2.07598\teval-rmse:2.46788\n",
      "[119]\ttrain-rmse:2.07530\teval-rmse:2.46764\n",
      "[120]\ttrain-rmse:2.07350\teval-rmse:2.46871\n",
      "[121]\ttrain-rmse:2.07315\teval-rmse:2.46852\n",
      "[122]\ttrain-rmse:2.07188\teval-rmse:2.46813\n",
      "[123]\ttrain-rmse:2.07032\teval-rmse:2.46740\n",
      "[124]\ttrain-rmse:2.06807\teval-rmse:2.46694\n",
      "[125]\ttrain-rmse:2.06448\teval-rmse:2.46678\n",
      "[126]\ttrain-rmse:2.06397\teval-rmse:2.46657\n",
      "[127]\ttrain-rmse:2.06339\teval-rmse:2.46630\n",
      "[128]\ttrain-rmse:2.06220\teval-rmse:2.46397\n",
      "[129]\ttrain-rmse:2.06112\teval-rmse:2.46357\n",
      "[130]\ttrain-rmse:2.05881\teval-rmse:2.46124\n",
      "[131]\ttrain-rmse:2.05552\teval-rmse:2.46192\n",
      "[132]\ttrain-rmse:2.05512\teval-rmse:2.46196\n",
      "[133]\ttrain-rmse:2.05338\teval-rmse:2.46313\n",
      "[134]\ttrain-rmse:2.05279\teval-rmse:2.46256\n",
      "[135]\ttrain-rmse:2.05126\teval-rmse:2.46426\n",
      "[136]\ttrain-rmse:2.04927\teval-rmse:2.46448\n",
      "[137]\ttrain-rmse:2.04527\teval-rmse:2.46320\n",
      "[138]\ttrain-rmse:2.04435\teval-rmse:2.46323\n",
      "[139]\ttrain-rmse:2.04384\teval-rmse:2.46293\n",
      "[0]\ttrain-rmse:3.47333\teval-rmse:3.98610\n",
      "[1]\ttrain-rmse:3.22372\teval-rmse:3.80025\n",
      "[2]\ttrain-rmse:3.06985\teval-rmse:3.68277\n",
      "[3]\ttrain-rmse:2.94940\teval-rmse:3.59544\n",
      "[4]\ttrain-rmse:2.87253\teval-rmse:3.54692\n",
      "[5]\ttrain-rmse:2.80556\teval-rmse:3.50689\n",
      "[6]\ttrain-rmse:2.77087\teval-rmse:3.48496\n",
      "[7]\ttrain-rmse:2.72611\teval-rmse:3.47074\n",
      "[8]\ttrain-rmse:2.67693\teval-rmse:3.43587\n",
      "[9]\ttrain-rmse:2.65694\teval-rmse:3.42261\n",
      "[10]\ttrain-rmse:2.63490\teval-rmse:3.40833\n",
      "[11]\ttrain-rmse:2.58904\teval-rmse:3.37772\n",
      "[12]\ttrain-rmse:2.58015\teval-rmse:3.37167\n",
      "[13]\ttrain-rmse:2.56800\teval-rmse:3.36405\n",
      "[14]\ttrain-rmse:2.54531\teval-rmse:3.35763\n",
      "[15]\ttrain-rmse:2.53666\teval-rmse:3.35072\n",
      "[16]\ttrain-rmse:2.52954\teval-rmse:3.34526\n",
      "[17]\ttrain-rmse:2.51635\teval-rmse:3.34179\n",
      "[18]\ttrain-rmse:2.48938\teval-rmse:3.32203\n",
      "[19]\ttrain-rmse:2.48081\teval-rmse:3.32027\n",
      "[20]\ttrain-rmse:2.47449\teval-rmse:3.31735\n",
      "[21]\ttrain-rmse:2.46371\teval-rmse:3.31780\n",
      "[22]\ttrain-rmse:2.45547\teval-rmse:3.31719\n",
      "[23]\ttrain-rmse:2.44584\teval-rmse:3.30979\n",
      "[24]\ttrain-rmse:2.44305\teval-rmse:3.30864\n",
      "[25]\ttrain-rmse:2.43579\teval-rmse:3.30172\n",
      "[26]\ttrain-rmse:2.42724\teval-rmse:3.29652\n",
      "[27]\ttrain-rmse:2.41410\teval-rmse:3.27164\n",
      "[28]\ttrain-rmse:2.41052\teval-rmse:3.27003\n",
      "[29]\ttrain-rmse:2.40416\teval-rmse:3.26582\n",
      "[30]\ttrain-rmse:2.40025\teval-rmse:3.26364\n",
      "[31]\ttrain-rmse:2.39617\teval-rmse:3.26115\n",
      "[32]\ttrain-rmse:2.39250\teval-rmse:3.25870\n",
      "[33]\ttrain-rmse:2.38421\teval-rmse:3.25630\n",
      "[34]\ttrain-rmse:2.36011\teval-rmse:3.19007\n",
      "[35]\ttrain-rmse:2.35255\teval-rmse:3.18860\n",
      "[36]\ttrain-rmse:2.35137\teval-rmse:3.18793\n",
      "[37]\ttrain-rmse:2.34604\teval-rmse:3.18617\n",
      "[38]\ttrain-rmse:2.34134\teval-rmse:3.17839\n",
      "[39]\ttrain-rmse:2.33983\teval-rmse:3.17738\n",
      "[40]\ttrain-rmse:2.33680\teval-rmse:3.17569\n",
      "[41]\ttrain-rmse:2.33183\teval-rmse:3.17474\n",
      "[42]\ttrain-rmse:2.33050\teval-rmse:3.17459\n",
      "[43]\ttrain-rmse:2.32865\teval-rmse:3.17378\n",
      "[44]\ttrain-rmse:2.32461\teval-rmse:3.17299\n",
      "[45]\ttrain-rmse:2.32361\teval-rmse:3.17247\n",
      "[46]\ttrain-rmse:2.31729\teval-rmse:3.16929\n",
      "[47]\ttrain-rmse:2.31152\teval-rmse:3.15275\n",
      "[48]\ttrain-rmse:2.30646\teval-rmse:3.14783\n",
      "[49]\ttrain-rmse:2.28510\teval-rmse:3.14411\n",
      "[50]\ttrain-rmse:2.28244\teval-rmse:3.14364\n",
      "[51]\ttrain-rmse:2.28058\teval-rmse:3.14265\n",
      "[52]\ttrain-rmse:2.27429\teval-rmse:3.14081\n",
      "[53]\ttrain-rmse:2.27127\teval-rmse:3.13058\n",
      "[54]\ttrain-rmse:2.26754\teval-rmse:3.13013\n",
      "[55]\ttrain-rmse:2.26454\teval-rmse:3.12991\n",
      "[56]\ttrain-rmse:2.25873\teval-rmse:3.12888\n",
      "[57]\ttrain-rmse:2.25427\teval-rmse:3.12508\n",
      "[58]\ttrain-rmse:2.25390\teval-rmse:3.12483\n",
      "[59]\ttrain-rmse:2.25180\teval-rmse:3.12322\n",
      "[60]\ttrain-rmse:2.24693\teval-rmse:3.11624\n",
      "[61]\ttrain-rmse:2.24338\teval-rmse:3.11554\n",
      "[62]\ttrain-rmse:2.23989\teval-rmse:3.11436\n",
      "[63]\ttrain-rmse:2.23506\teval-rmse:3.11584\n",
      "[64]\ttrain-rmse:2.23351\teval-rmse:3.11571\n",
      "[65]\ttrain-rmse:2.23248\teval-rmse:3.11559\n",
      "[66]\ttrain-rmse:2.23180\teval-rmse:3.11504\n",
      "[67]\ttrain-rmse:2.22578\teval-rmse:3.10627\n",
      "[68]\ttrain-rmse:2.22436\teval-rmse:3.10529\n",
      "[69]\ttrain-rmse:2.22264\teval-rmse:3.10439\n",
      "[70]\ttrain-rmse:2.22195\teval-rmse:3.10406\n",
      "[71]\ttrain-rmse:2.21727\teval-rmse:3.07277\n",
      "[72]\ttrain-rmse:2.21520\teval-rmse:3.07185\n",
      "[73]\ttrain-rmse:2.21264\teval-rmse:3.07090\n",
      "[74]\ttrain-rmse:2.20979\teval-rmse:3.06131\n",
      "[75]\ttrain-rmse:2.20850\teval-rmse:3.06191\n",
      "[76]\ttrain-rmse:2.20790\teval-rmse:3.06159\n",
      "[77]\ttrain-rmse:2.20460\teval-rmse:3.05300\n",
      "[78]\ttrain-rmse:2.20357\teval-rmse:3.05267\n",
      "[79]\ttrain-rmse:2.20217\teval-rmse:3.05291\n",
      "[80]\ttrain-rmse:2.20103\teval-rmse:3.05245\n",
      "[81]\ttrain-rmse:2.19817\teval-rmse:3.05149\n",
      "[82]\ttrain-rmse:2.19540\teval-rmse:3.04460\n",
      "[83]\ttrain-rmse:2.19451\teval-rmse:3.04488\n",
      "[84]\ttrain-rmse:2.19382\teval-rmse:3.04472\n",
      "[85]\ttrain-rmse:2.19277\teval-rmse:3.04460\n",
      "[86]\ttrain-rmse:2.19110\teval-rmse:3.04383\n",
      "[87]\ttrain-rmse:2.18000\teval-rmse:3.04340\n",
      "[88]\ttrain-rmse:2.17960\teval-rmse:3.04320\n",
      "[89]\ttrain-rmse:2.17662\teval-rmse:3.03893\n",
      "[90]\ttrain-rmse:2.17441\teval-rmse:3.03855\n",
      "[91]\ttrain-rmse:2.17360\teval-rmse:3.03847\n",
      "[92]\ttrain-rmse:2.17284\teval-rmse:3.03789\n",
      "[93]\ttrain-rmse:2.17173\teval-rmse:3.03730\n",
      "[94]\ttrain-rmse:2.16978\teval-rmse:3.03459\n",
      "[95]\ttrain-rmse:2.16756\teval-rmse:3.03483\n",
      "[96]\ttrain-rmse:2.16378\teval-rmse:3.03182\n",
      "[97]\ttrain-rmse:2.16093\teval-rmse:3.02784\n",
      "[98]\ttrain-rmse:2.16018\teval-rmse:3.02738\n",
      "[99]\ttrain-rmse:2.15803\teval-rmse:3.02666\n",
      "[100]\ttrain-rmse:2.15580\teval-rmse:3.02449\n",
      "[101]\ttrain-rmse:2.14814\teval-rmse:3.02215\n",
      "[102]\ttrain-rmse:2.14596\teval-rmse:3.02137\n",
      "[103]\ttrain-rmse:2.14450\teval-rmse:3.02111\n",
      "[104]\ttrain-rmse:2.14350\teval-rmse:3.02118\n",
      "[105]\ttrain-rmse:2.14187\teval-rmse:3.02037\n",
      "[106]\ttrain-rmse:2.14092\teval-rmse:3.02056\n",
      "[107]\ttrain-rmse:2.13924\teval-rmse:3.02072\n",
      "[108]\ttrain-rmse:2.13801\teval-rmse:3.02135\n",
      "[109]\ttrain-rmse:2.13771\teval-rmse:3.02120\n",
      "[110]\ttrain-rmse:2.13654\teval-rmse:3.02074\n",
      "[111]\ttrain-rmse:2.13411\teval-rmse:3.01993\n",
      "[112]\ttrain-rmse:2.13236\teval-rmse:3.01894\n",
      "[113]\ttrain-rmse:2.13081\teval-rmse:3.01858\n",
      "[114]\ttrain-rmse:2.13007\teval-rmse:3.01809\n",
      "[115]\ttrain-rmse:2.12764\teval-rmse:3.01384\n",
      "[116]\ttrain-rmse:2.12657\teval-rmse:3.01323\n",
      "[117]\ttrain-rmse:2.12624\teval-rmse:3.01308\n",
      "[118]\ttrain-rmse:2.12418\teval-rmse:3.01190\n",
      "[119]\ttrain-rmse:2.12377\teval-rmse:3.01178\n",
      "[120]\ttrain-rmse:2.11896\teval-rmse:3.01006\n",
      "[121]\ttrain-rmse:2.11804\teval-rmse:3.00871\n",
      "[122]\ttrain-rmse:2.11427\teval-rmse:3.00920\n",
      "[123]\ttrain-rmse:2.11395\teval-rmse:3.00905\n",
      "[124]\ttrain-rmse:2.11286\teval-rmse:3.00742\n",
      "[125]\ttrain-rmse:2.11236\teval-rmse:3.00710\n",
      "[126]\ttrain-rmse:2.11179\teval-rmse:3.00635\n",
      "[127]\ttrain-rmse:2.11087\teval-rmse:3.00657\n",
      "[128]\ttrain-rmse:2.11049\teval-rmse:3.00597\n",
      "[129]\ttrain-rmse:2.10994\teval-rmse:3.00556\n",
      "[130]\ttrain-rmse:2.10776\teval-rmse:3.00457\n",
      "[131]\ttrain-rmse:2.10342\teval-rmse:3.00537\n",
      "[132]\ttrain-rmse:2.10282\teval-rmse:3.00505\n",
      "[133]\ttrain-rmse:2.10013\teval-rmse:3.00070\n",
      "[134]\ttrain-rmse:2.09927\teval-rmse:3.00052\n",
      "[135]\ttrain-rmse:2.09891\teval-rmse:3.00054\n",
      "[136]\ttrain-rmse:2.09850\teval-rmse:3.00028\n",
      "[137]\ttrain-rmse:2.09762\teval-rmse:2.99973\n",
      "[138]\ttrain-rmse:2.09728\teval-rmse:3.00021\n",
      "[139]\ttrain-rmse:2.09418\teval-rmse:2.99999\n",
      "[140]\ttrain-rmse:2.09360\teval-rmse:2.99995\n",
      "[141]\ttrain-rmse:2.09327\teval-rmse:2.99992\n",
      "[142]\ttrain-rmse:2.09258\teval-rmse:2.99966\n",
      "[143]\ttrain-rmse:2.09130\teval-rmse:2.99940\n",
      "[144]\ttrain-rmse:2.09071\teval-rmse:2.99904\n",
      "[145]\ttrain-rmse:2.08920\teval-rmse:2.99895\n",
      "[146]\ttrain-rmse:2.08784\teval-rmse:2.99891\n",
      "[147]\ttrain-rmse:2.08699\teval-rmse:2.99859\n",
      "[148]\ttrain-rmse:2.08645\teval-rmse:2.99843\n",
      "[149]\ttrain-rmse:2.08590\teval-rmse:2.99880\n",
      "[150]\ttrain-rmse:2.08365\teval-rmse:2.99824\n",
      "[151]\ttrain-rmse:2.08218\teval-rmse:2.99620\n",
      "[152]\ttrain-rmse:2.08145\teval-rmse:2.99539\n",
      "[153]\ttrain-rmse:2.08118\teval-rmse:2.99527\n",
      "[154]\ttrain-rmse:2.08074\teval-rmse:2.99551\n",
      "[155]\ttrain-rmse:2.07839\teval-rmse:2.99479\n",
      "[156]\ttrain-rmse:2.07816\teval-rmse:2.99469\n",
      "[157]\ttrain-rmse:2.07771\teval-rmse:2.99444\n",
      "[158]\ttrain-rmse:2.07602\teval-rmse:2.99430\n",
      "[159]\ttrain-rmse:2.07568\teval-rmse:2.99420\n",
      "[160]\ttrain-rmse:2.07531\teval-rmse:2.99405\n",
      "[161]\ttrain-rmse:2.07503\teval-rmse:2.99391\n",
      "[162]\ttrain-rmse:2.07476\teval-rmse:2.99417\n",
      "[163]\ttrain-rmse:2.07341\teval-rmse:2.99337\n",
      "[164]\ttrain-rmse:2.07259\teval-rmse:2.99312\n",
      "[165]\ttrain-rmse:2.07150\teval-rmse:2.99343\n",
      "[166]\ttrain-rmse:2.07123\teval-rmse:2.99341\n",
      "[167]\ttrain-rmse:2.07091\teval-rmse:2.99397\n",
      "[168]\ttrain-rmse:2.06853\teval-rmse:2.99219\n",
      "[169]\ttrain-rmse:2.06271\teval-rmse:2.96957\n",
      "[170]\ttrain-rmse:2.06171\teval-rmse:2.96923\n",
      "[171]\ttrain-rmse:2.06149\teval-rmse:2.96921\n",
      "[172]\ttrain-rmse:2.05551\teval-rmse:2.96892\n",
      "[173]\ttrain-rmse:2.05499\teval-rmse:2.96865\n",
      "[174]\ttrain-rmse:2.05415\teval-rmse:2.96826\n",
      "[175]\ttrain-rmse:2.05306\teval-rmse:2.96800\n",
      "[176]\ttrain-rmse:2.05179\teval-rmse:2.96483\n",
      "[177]\ttrain-rmse:2.05168\teval-rmse:2.96476\n",
      "[178]\ttrain-rmse:2.05119\teval-rmse:2.96465\n",
      "[179]\ttrain-rmse:2.05019\teval-rmse:2.96443\n",
      "[180]\ttrain-rmse:2.04885\teval-rmse:2.96401\n",
      "[181]\ttrain-rmse:2.04675\teval-rmse:2.95749\n",
      "[182]\ttrain-rmse:2.04595\teval-rmse:2.95735\n",
      "[183]\ttrain-rmse:2.04549\teval-rmse:2.95728\n",
      "[184]\ttrain-rmse:2.04456\teval-rmse:2.95698\n",
      "[185]\ttrain-rmse:2.04437\teval-rmse:2.95693\n",
      "[186]\ttrain-rmse:2.04417\teval-rmse:2.95678\n",
      "[187]\ttrain-rmse:2.04370\teval-rmse:2.95656\n",
      "[188]\ttrain-rmse:2.04332\teval-rmse:2.95633\n",
      "[189]\ttrain-rmse:2.03965\teval-rmse:2.95628\n",
      "[190]\ttrain-rmse:2.03875\teval-rmse:2.95601\n",
      "[191]\ttrain-rmse:2.03759\teval-rmse:2.95613\n",
      "[192]\ttrain-rmse:2.03563\teval-rmse:2.95366\n",
      "[193]\ttrain-rmse:2.03540\teval-rmse:2.95361\n",
      "[194]\ttrain-rmse:2.03494\teval-rmse:2.95357\n",
      "[195]\ttrain-rmse:2.03271\teval-rmse:2.95417\n",
      "[196]\ttrain-rmse:2.03225\teval-rmse:2.95406\n",
      "[197]\ttrain-rmse:2.02965\teval-rmse:2.95369\n",
      "[198]\ttrain-rmse:2.02865\teval-rmse:2.95290\n",
      "[199]\ttrain-rmse:2.02706\teval-rmse:2.95283\n",
      "[200]\ttrain-rmse:2.02701\teval-rmse:2.95278\n",
      "[201]\ttrain-rmse:2.02576\teval-rmse:2.95225\n",
      "[202]\ttrain-rmse:2.02545\teval-rmse:2.95272\n",
      "[203]\ttrain-rmse:2.02523\teval-rmse:2.95259\n",
      "[204]\ttrain-rmse:2.02453\teval-rmse:2.95246\n",
      "[205]\ttrain-rmse:2.02363\teval-rmse:2.95186\n",
      "[206]\ttrain-rmse:2.02269\teval-rmse:2.95102\n",
      "[207]\ttrain-rmse:2.02242\teval-rmse:2.95089\n",
      "[208]\ttrain-rmse:2.02214\teval-rmse:2.95079\n",
      "[209]\ttrain-rmse:2.02150\teval-rmse:2.95067\n",
      "[210]\ttrain-rmse:2.02083\teval-rmse:2.95053\n",
      "[211]\ttrain-rmse:2.01908\teval-rmse:2.95213\n",
      "[212]\ttrain-rmse:2.01884\teval-rmse:2.95208\n",
      "[213]\ttrain-rmse:2.01804\teval-rmse:2.95229\n",
      "[214]\ttrain-rmse:2.01774\teval-rmse:2.95199\n",
      "[215]\ttrain-rmse:2.01753\teval-rmse:2.95200\n",
      "[216]\ttrain-rmse:2.01683\teval-rmse:2.95177\n",
      "[217]\ttrain-rmse:2.01633\teval-rmse:2.95189\n",
      "[218]\ttrain-rmse:2.01599\teval-rmse:2.95204\n",
      "[219]\ttrain-rmse:2.01550\teval-rmse:2.95188\n",
      "[220]\ttrain-rmse:2.01519\teval-rmse:2.95173\n",
      "[0]\ttrain-rmse:3.57921\teval-rmse:4.67038\n",
      "[1]\ttrain-rmse:3.34515\teval-rmse:4.43186\n",
      "[2]\ttrain-rmse:3.19761\teval-rmse:4.28653\n",
      "[3]\ttrain-rmse:3.08987\teval-rmse:4.18093\n",
      "[4]\ttrain-rmse:3.02093\teval-rmse:4.10819\n",
      "[5]\ttrain-rmse:2.96099\teval-rmse:4.05348\n",
      "[6]\ttrain-rmse:2.90833\teval-rmse:3.99412\n",
      "[7]\ttrain-rmse:2.86923\teval-rmse:3.97585\n",
      "[8]\ttrain-rmse:2.84251\teval-rmse:3.95414\n",
      "[9]\ttrain-rmse:2.81969\teval-rmse:3.94346\n",
      "[10]\ttrain-rmse:2.79207\teval-rmse:3.92304\n",
      "[11]\ttrain-rmse:2.76847\teval-rmse:3.89076\n",
      "[12]\ttrain-rmse:2.70175\teval-rmse:3.84937\n",
      "[13]\ttrain-rmse:2.68521\teval-rmse:3.83619\n",
      "[14]\ttrain-rmse:2.67621\teval-rmse:3.82457\n",
      "[15]\ttrain-rmse:2.66460\teval-rmse:3.80871\n",
      "[16]\ttrain-rmse:2.65717\teval-rmse:3.80362\n",
      "[17]\ttrain-rmse:2.62811\teval-rmse:3.75244\n",
      "[18]\ttrain-rmse:2.61987\teval-rmse:3.75225\n",
      "[19]\ttrain-rmse:2.61294\teval-rmse:3.74535\n",
      "[20]\ttrain-rmse:2.60729\teval-rmse:3.74139\n",
      "[21]\ttrain-rmse:2.57017\teval-rmse:3.71595\n",
      "[22]\ttrain-rmse:2.55871\teval-rmse:3.71068\n",
      "[23]\ttrain-rmse:2.55268\teval-rmse:3.70482\n",
      "[24]\ttrain-rmse:2.53858\teval-rmse:3.69915\n",
      "[25]\ttrain-rmse:2.53592\teval-rmse:3.69676\n",
      "[26]\ttrain-rmse:2.53189\teval-rmse:3.69359\n",
      "[27]\ttrain-rmse:2.51927\teval-rmse:3.68317\n",
      "[28]\ttrain-rmse:2.51426\teval-rmse:3.68099\n",
      "[29]\ttrain-rmse:2.50676\teval-rmse:3.68017\n",
      "[30]\ttrain-rmse:2.49746\teval-rmse:3.65103\n",
      "[31]\ttrain-rmse:2.49575\teval-rmse:3.64862\n",
      "[32]\ttrain-rmse:2.48357\teval-rmse:3.64782\n",
      "[33]\ttrain-rmse:2.46608\teval-rmse:3.63511\n",
      "[34]\ttrain-rmse:2.46395\teval-rmse:3.62807\n",
      "[35]\ttrain-rmse:2.45462\teval-rmse:3.63045\n",
      "[36]\ttrain-rmse:2.45179\teval-rmse:3.63080\n",
      "[37]\ttrain-rmse:2.44797\teval-rmse:3.63039\n",
      "[38]\ttrain-rmse:2.43623\teval-rmse:3.61687\n",
      "[39]\ttrain-rmse:2.43448\teval-rmse:3.61540\n",
      "[40]\ttrain-rmse:2.43101\teval-rmse:3.61355\n",
      "[41]\ttrain-rmse:2.42820\teval-rmse:3.61283\n",
      "[42]\ttrain-rmse:2.42703\teval-rmse:3.61302\n",
      "[43]\ttrain-rmse:2.42596\teval-rmse:3.61175\n",
      "[44]\ttrain-rmse:2.42338\teval-rmse:3.60932\n",
      "[45]\ttrain-rmse:2.42196\teval-rmse:3.60819\n",
      "[46]\ttrain-rmse:2.41922\teval-rmse:3.60368\n",
      "[47]\ttrain-rmse:2.41692\teval-rmse:3.60089\n",
      "[48]\ttrain-rmse:2.41506\teval-rmse:3.59995\n",
      "[49]\ttrain-rmse:2.41195\teval-rmse:3.59665\n",
      "[50]\ttrain-rmse:2.40690\teval-rmse:3.58027\n",
      "[51]\ttrain-rmse:2.39913\teval-rmse:3.57983\n",
      "[52]\ttrain-rmse:2.39777\teval-rmse:3.57934\n",
      "[53]\ttrain-rmse:2.38920\teval-rmse:3.57349\n",
      "[54]\ttrain-rmse:2.38850\teval-rmse:3.57302\n",
      "[55]\ttrain-rmse:2.38482\teval-rmse:3.57161\n",
      "[56]\ttrain-rmse:2.38248\teval-rmse:3.56995\n",
      "[57]\ttrain-rmse:2.38055\teval-rmse:3.56719\n",
      "[58]\ttrain-rmse:2.37755\teval-rmse:3.56656\n",
      "[59]\ttrain-rmse:2.37558\teval-rmse:3.56323\n",
      "[60]\ttrain-rmse:2.37430\teval-rmse:3.56168\n",
      "[61]\ttrain-rmse:2.36891\teval-rmse:3.56069\n",
      "[62]\ttrain-rmse:2.36840\teval-rmse:3.56044\n",
      "[63]\ttrain-rmse:2.36772\teval-rmse:3.56004\n",
      "[64]\ttrain-rmse:2.36626\teval-rmse:3.56001\n",
      "[65]\ttrain-rmse:2.36296\teval-rmse:3.55733\n",
      "[66]\ttrain-rmse:2.36175\teval-rmse:3.55564\n",
      "[67]\ttrain-rmse:2.35960\teval-rmse:3.55478\n",
      "[68]\ttrain-rmse:2.35711\teval-rmse:3.55418\n",
      "[69]\ttrain-rmse:2.35587\teval-rmse:3.55390\n",
      "[70]\ttrain-rmse:2.34804\teval-rmse:3.55052\n",
      "[71]\ttrain-rmse:2.34517\teval-rmse:3.54816\n",
      "[72]\ttrain-rmse:2.33880\teval-rmse:3.54741\n",
      "[73]\ttrain-rmse:2.33285\teval-rmse:3.55082\n",
      "[74]\ttrain-rmse:2.33092\teval-rmse:3.54916\n",
      "[75]\ttrain-rmse:2.32968\teval-rmse:3.54834\n",
      "[76]\ttrain-rmse:2.32579\teval-rmse:3.54928\n",
      "[77]\ttrain-rmse:2.32307\teval-rmse:3.54880\n",
      "[78]\ttrain-rmse:2.32177\teval-rmse:3.54878\n",
      "[79]\ttrain-rmse:2.31758\teval-rmse:3.54412\n",
      "[80]\ttrain-rmse:2.31615\teval-rmse:3.54323\n",
      "[81]\ttrain-rmse:2.30591\teval-rmse:3.54379\n",
      "[82]\ttrain-rmse:2.30448\teval-rmse:3.54344\n",
      "[83]\ttrain-rmse:2.30387\teval-rmse:3.54307\n",
      "[84]\ttrain-rmse:2.30259\teval-rmse:3.54197\n",
      "[85]\ttrain-rmse:2.29934\teval-rmse:3.54176\n",
      "[86]\ttrain-rmse:2.29893\teval-rmse:3.54165\n",
      "[87]\ttrain-rmse:2.29861\teval-rmse:3.54141\n",
      "[88]\ttrain-rmse:2.29817\teval-rmse:3.54110\n",
      "[89]\ttrain-rmse:2.29801\teval-rmse:3.54106\n",
      "[90]\ttrain-rmse:2.29712\teval-rmse:3.54062\n",
      "[91]\ttrain-rmse:2.29460\teval-rmse:3.53940\n",
      "[92]\ttrain-rmse:2.29356\teval-rmse:3.53920\n",
      "[93]\ttrain-rmse:2.29139\teval-rmse:3.53807\n",
      "[94]\ttrain-rmse:2.28917\teval-rmse:3.53631\n",
      "[95]\ttrain-rmse:2.28662\teval-rmse:3.53560\n",
      "[96]\ttrain-rmse:2.28517\teval-rmse:3.53576\n",
      "[97]\ttrain-rmse:2.28480\teval-rmse:3.53548\n",
      "[98]\ttrain-rmse:2.28404\teval-rmse:3.53539\n",
      "[99]\ttrain-rmse:2.28383\teval-rmse:3.53526\n",
      "[100]\ttrain-rmse:2.28251\teval-rmse:3.53545\n",
      "[101]\ttrain-rmse:2.27935\teval-rmse:3.53558\n",
      "[102]\ttrain-rmse:2.27118\teval-rmse:3.52775\n",
      "[103]\ttrain-rmse:2.27066\teval-rmse:3.52736\n",
      "[104]\ttrain-rmse:2.26909\teval-rmse:3.52671\n",
      "[105]\ttrain-rmse:2.26822\teval-rmse:3.52662\n",
      "[106]\ttrain-rmse:2.25939\teval-rmse:3.52694\n",
      "[107]\ttrain-rmse:2.25904\teval-rmse:3.52662\n",
      "[108]\ttrain-rmse:2.25872\teval-rmse:3.52637\n",
      "[109]\ttrain-rmse:2.25819\teval-rmse:3.52617\n",
      "[110]\ttrain-rmse:2.25694\teval-rmse:3.52519\n",
      "[111]\ttrain-rmse:2.24888\teval-rmse:3.52356\n",
      "[112]\ttrain-rmse:2.24793\teval-rmse:3.52296\n",
      "[113]\ttrain-rmse:2.24666\teval-rmse:3.52433\n",
      "[114]\ttrain-rmse:2.24572\teval-rmse:3.52361\n",
      "[115]\ttrain-rmse:2.24493\teval-rmse:3.52331\n",
      "[116]\ttrain-rmse:2.24467\teval-rmse:3.52317\n",
      "[117]\ttrain-rmse:2.24325\teval-rmse:3.52313\n",
      "[118]\ttrain-rmse:2.24242\teval-rmse:3.52291\n",
      "[119]\ttrain-rmse:2.24096\teval-rmse:3.52362\n",
      "[120]\ttrain-rmse:2.23857\teval-rmse:3.52310\n",
      "[121]\ttrain-rmse:2.23780\teval-rmse:3.52295\n",
      "[122]\ttrain-rmse:2.23520\teval-rmse:3.52286\n",
      "[123]\ttrain-rmse:2.23475\teval-rmse:3.52260\n",
      "[124]\ttrain-rmse:2.23427\teval-rmse:3.52213\n",
      "[125]\ttrain-rmse:2.22101\teval-rmse:3.52064\n",
      "[126]\ttrain-rmse:2.21981\teval-rmse:3.51974\n",
      "[127]\ttrain-rmse:2.21464\teval-rmse:3.51888\n",
      "[128]\ttrain-rmse:2.20744\teval-rmse:3.52051\n",
      "[129]\ttrain-rmse:2.20499\teval-rmse:3.51910\n",
      "[130]\ttrain-rmse:2.20432\teval-rmse:3.51976\n",
      "[131]\ttrain-rmse:2.19995\teval-rmse:3.51637\n",
      "[132]\ttrain-rmse:2.19320\teval-rmse:3.51227\n",
      "[133]\ttrain-rmse:2.19130\teval-rmse:3.51208\n",
      "[134]\ttrain-rmse:2.19081\teval-rmse:3.51166\n",
      "[135]\ttrain-rmse:2.18984\teval-rmse:3.51118\n",
      "[136]\ttrain-rmse:2.18868\teval-rmse:3.51061\n",
      "[137]\ttrain-rmse:2.18638\teval-rmse:3.51077\n",
      "[138]\ttrain-rmse:2.18522\teval-rmse:3.51038\n",
      "[139]\ttrain-rmse:2.18471\teval-rmse:3.51009\n",
      "[140]\ttrain-rmse:2.18122\teval-rmse:3.50455\n",
      "[141]\ttrain-rmse:2.17971\teval-rmse:3.50255\n",
      "[142]\ttrain-rmse:2.17906\teval-rmse:3.50181\n",
      "[143]\ttrain-rmse:2.17823\teval-rmse:3.50208\n",
      "[144]\ttrain-rmse:2.17743\teval-rmse:3.50172\n",
      "[145]\ttrain-rmse:2.17715\teval-rmse:3.50165\n",
      "[146]\ttrain-rmse:2.17654\teval-rmse:3.50104\n",
      "[147]\ttrain-rmse:2.17630\teval-rmse:3.50084\n",
      "[148]\ttrain-rmse:2.17595\teval-rmse:3.50060\n",
      "[149]\ttrain-rmse:2.17574\teval-rmse:3.50053\n",
      "[150]\ttrain-rmse:2.17456\teval-rmse:3.50017\n",
      "[151]\ttrain-rmse:2.17413\teval-rmse:3.49994\n",
      "[152]\ttrain-rmse:2.17017\teval-rmse:3.49950\n",
      "[153]\ttrain-rmse:2.16923\teval-rmse:3.49904\n",
      "[154]\ttrain-rmse:2.16833\teval-rmse:3.49913\n",
      "[155]\ttrain-rmse:2.16700\teval-rmse:3.49893\n",
      "[156]\ttrain-rmse:2.16604\teval-rmse:3.49886\n",
      "[157]\ttrain-rmse:2.16526\teval-rmse:3.49929\n",
      "[158]\ttrain-rmse:2.16482\teval-rmse:3.49941\n",
      "[159]\ttrain-rmse:2.16459\teval-rmse:3.49936\n",
      "[160]\ttrain-rmse:2.16415\teval-rmse:3.49871\n",
      "[161]\ttrain-rmse:2.16392\teval-rmse:3.49876\n",
      "[162]\ttrain-rmse:2.16350\teval-rmse:3.49887\n",
      "[163]\ttrain-rmse:2.16131\teval-rmse:3.49902\n",
      "[164]\ttrain-rmse:2.16082\teval-rmse:3.49874\n",
      "[165]\ttrain-rmse:2.15846\teval-rmse:3.49935\n",
      "[166]\ttrain-rmse:2.15789\teval-rmse:3.49907\n",
      "[167]\ttrain-rmse:2.15425\teval-rmse:3.48691\n",
      "[168]\ttrain-rmse:2.15318\teval-rmse:3.48634\n",
      "[169]\ttrain-rmse:2.14895\teval-rmse:3.48153\n",
      "[170]\ttrain-rmse:2.14806\teval-rmse:3.48239\n",
      "[171]\ttrain-rmse:2.14360\teval-rmse:3.48405\n",
      "[172]\ttrain-rmse:2.14074\teval-rmse:3.48428\n",
      "[173]\ttrain-rmse:2.14031\teval-rmse:3.48439\n",
      "[174]\ttrain-rmse:2.13976\teval-rmse:3.48445\n",
      "[175]\ttrain-rmse:2.13829\teval-rmse:3.48355\n",
      "[176]\ttrain-rmse:2.13798\teval-rmse:3.48415\n",
      "[177]\ttrain-rmse:2.13633\teval-rmse:3.48412\n",
      "[178]\ttrain-rmse:2.13554\teval-rmse:3.48261\n",
      "[179]\ttrain-rmse:2.13408\teval-rmse:3.48303\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Convert data into DMatrix format for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "\n",
    "# Define parameters for the XGBoost model\n",
    "param = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "}\n",
    "\n",
    "# Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "lowest_val_rmse = float('inf')\n",
    "best_xgb_model = None\n",
    "\n",
    "for train_index, val_index in tscv.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    dtrain_fold = xgb.DMatrix(X_train_fold, label=y_train_fold)\n",
    "    dval_fold = xgb.DMatrix(X_val_fold, label=y_val_fold)\n",
    "    \n",
    "    evals = [(dtrain_fold, 'train'), (dval_fold, 'eval')]\n",
    "    \n",
    "    xgb_model = xgb.train(param, dtrain_fold, num_boost_round=1000, evals=evals, early_stopping_rounds=10, verbose_eval=True)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = xgb_model.predict(dval_fold)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "    \n",
    "    if rmse < lowest_val_rmse:\n",
    "        best_xgb_model = xgb_model\n",
    "        lowest_val_rmse = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Relative Error: 5.246789892928434\n",
      "RMSE on the validation set: 10.037060358012107\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set using the best model\n",
    "dtest = xgb.DMatrix(X_test_scaled)\n",
    "y_pred = best_xgb_model.predict(dtest)\n",
    "\n",
    "# Calculate the relative error\n",
    "epsilon = 1e-10\n",
    "errors = np.abs(y_pred - y_test) / (y_pred + epsilon)\n",
    "avg_relative_error = np.mean(errors)\n",
    "print(f\"Average Relative Error: {avg_relative_error}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE on the validation set: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stacking a 1D CNN with a GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18936/18936 - 33s - loss: 0.1049 - val_loss: 0.1048 - 33s/epoch - 2ms/step\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/predict_publications_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18936/18936 - 32s - loss: 0.1049 - val_loss: 0.1048 - 32s/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "18936/18936 - 42s - loss: 0.1049 - val_loss: 0.1048 - 42s/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "18936/18936 - 31s - loss: 0.1049 - val_loss: 0.1048 - 31s/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "18936/18936 - 31s - loss: 0.1049 - val_loss: 0.1048 - 31s/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "18936/18936 - 30s - loss: 0.1049 - val_loss: 0.1048 - 30s/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "18936/18936 - 30s - loss: 0.1049 - val_loss: 0.1048 - 30s/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "18936/18936 - 32s - loss: 0.1049 - val_loss: 0.1048 - 32s/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "18936/18936 - 32s - loss: 0.1049 - val_loss: 0.1048 - 32s/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "18936/18936 - 31s - loss: 0.1049 - val_loss: 0.1048 - 31s/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37871/37871 - 56s - loss: 0.1047 - val_loss: 0.1052 - 56s/epoch - 1ms/step\n",
      "Epoch 2/10\n",
      "37871/37871 - 54s - loss: 0.1047 - val_loss: 0.1052 - 54s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "37871/37871 - 55s - loss: 0.1047 - val_loss: 0.1052 - 55s/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "37871/37871 - 54s - loss: 0.1047 - val_loss: 0.1052 - 54s/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "37871/37871 - 53s - loss: 0.1047 - val_loss: 0.1052 - 53s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "37871/37871 - 53s - loss: 0.1047 - val_loss: 0.1052 - 53s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "37871/37871 - 52s - loss: 0.1047 - val_loss: 0.1052 - 52s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "37871/37871 - 52s - loss: 0.1047 - val_loss: 0.1052 - 52s/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "37871/37871 - 53s - loss: 0.1047 - val_loss: 0.1052 - 53s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "37871/37871 - 54s - loss: 0.1047 - val_loss: 0.1052 - 54s/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56806/56806 - 82s - loss: 0.1049 - val_loss: 0.1051 - 82s/epoch - 1ms/step\n",
      "Epoch 2/10\n",
      "56806/56806 - 77s - loss: 0.1049 - val_loss: 0.1051 - 77s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "56806/56806 - 77s - loss: 0.1049 - val_loss: 0.1051 - 77s/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "56806/56806 - 77s - loss: 0.1049 - val_loss: 0.1051 - 77s/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "56806/56806 - 76s - loss: 0.1049 - val_loss: 0.1051 - 76s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "56806/56806 - 77s - loss: 0.1049 - val_loss: 0.1051 - 77s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "56806/56806 - 81s - loss: 0.1049 - val_loss: 0.1051 - 81s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "56806/56806 - 106s - loss: 0.1049 - val_loss: 0.1051 - 106s/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "56806/56806 - 79s - loss: 0.1049 - val_loss: 0.1051 - 79s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "56806/56806 - 78s - loss: 0.1049 - val_loss: 0.1051 - 78s/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75741/75741 - 105s - loss: 0.1047 - val_loss: 0.1047 - 105s/epoch - 1ms/step\n",
      "Epoch 2/10\n",
      "75741/75741 - 102s - loss: 0.1046 - val_loss: 0.1047 - 102s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "75741/75741 - 108s - loss: 0.1046 - val_loss: 0.1047 - 108s/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "75741/75741 - 110s - loss: 0.1046 - val_loss: 0.1047 - 110s/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "75741/75741 - 103s - loss: 0.1046 - val_loss: 0.1047 - 103s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "75741/75741 - 101s - loss: 0.1046 - val_loss: 0.1047 - 101s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "75741/75741 - 104s - loss: 0.1046 - val_loss: 0.1047 - 104s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "75741/75741 - 119s - loss: 0.1046 - val_loss: 0.1047 - 119s/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "75741/75741 - 100s - loss: 0.1046 - val_loss: 0.1047 - 100s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "75741/75741 - 101s - loss: 0.1046 - val_loss: 0.1047 - 101s/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94676/94676 - 131s - loss: 0.1047 - val_loss: 0.1055 - 131s/epoch - 1ms/step\n",
      "Epoch 2/10\n",
      "94676/94676 - 124s - loss: 0.1047 - val_loss: 0.1055 - 124s/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "94676/94676 - 125s - loss: 0.1047 - val_loss: 0.1055 - 125s/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "94676/94676 - 123s - loss: 0.1047 - val_loss: 0.1055 - 123s/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "94676/94676 - 122s - loss: 0.1047 - val_loss: 0.1055 - 122s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "94676/94676 - 122s - loss: 0.1047 - val_loss: 0.1055 - 122s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "94676/94676 - 122s - loss: 0.1047 - val_loss: 0.1055 - 122s/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "94676/94676 - 118s - loss: 0.1047 - val_loss: 0.1055 - 118s/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "94676/94676 - 122s - loss: 0.1047 - val_loss: 0.1055 - 122s/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "94676/94676 - 121s - loss: 0.1047 - val_loss: 0.1055 - 121s/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, GRU, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def create_cnn_gru_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adjust the input shape to account for the window size\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(window_size, X_train_scaled.shape[1])))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(GRU(50, return_sequences=True))\n",
    "    model.add(GRU(25))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "best_cnn_gru_model = None\n",
    "lowest_val_loss = float('inf')\n",
    "\n",
    "# Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, val_index in tscv.split(X_train_windows):\n",
    "    X_train_fold, X_val_fold = X_train_windows[train_index], X_train_windows[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_windows[train_index], y_train_windows[val_index]\n",
    "    \n",
    "    model = create_cnn_gru_model()\n",
    "    \n",
    "    # Use ModelCheckpoint to save the model with the lowest validation loss\n",
    "    checkpoint = ModelCheckpoint('best_cnn_gru_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold), callbacks=[checkpoint], verbose=2, shuffle=False)\n",
    "    \n",
    "    # Check if this fold's model is better than the previous best model\n",
    "    val_loss = model.history.history['val_loss'][-1]\n",
    "    if val_loss < lowest_val_loss:\n",
    "        best_cnn_gru_model = clone_model(model)\n",
    "        best_cnn_gru_model.load_weights('best_cnn_gru_model.h5')\n",
    "        best_cnn_gru_model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        lowest_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 480us/step\n",
      "Average Relative Error: 0.9923043076604032\n",
      "RMSE on the validation set: 0.19128726335925672\n"
     ]
    }
   ],
   "source": [
    "# Extract the last column from y_test_windows\n",
    "y_test_windows_last_column = y_test_windows[:, -1]\n",
    "\n",
    "# After training, evaluate the best model on the test data\n",
    "y_pred = best_cnn_gru_model.predict(X_test_windows)\n",
    "\n",
    "# Ensure that y_pred and y_test_windows_last_column have the same shape\n",
    "y_pred_squeezed = np.squeeze(y_pred)\n",
    "assert y_pred_squeezed.shape == y_test_windows_last_column.shape, f\"Shape mismatch: y_pred_squeezed has shape {y_pred_squeezed.shape} while y_test_windows_last_column has shape {y_test_windows_last_column.shape}\"\n",
    "\n",
    "# Calculate the relative error\n",
    "epsilon = 1e-10\n",
    "errors = np.abs(y_pred_squeezed - y_test_windows_last_column) / (y_pred_squeezed + epsilon)\n",
    "avg_relative_error = np.mean(errors)\n",
    "print(f\"Average Relative Error: {avg_relative_error}\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test_windows_last_column, y_pred_squeezed))\n",
    "print(f\"RMSE on the validation set: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 547us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred = best_cnn_gru_model.predict(X_test_windows)\n",
    "\n",
    "# Fit a new scaler on the target variable only\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Inverse transform using the new scaler to get the original scale for both predicted and actual values\n",
    "y_pred_rescaled = target_scaler.inverse_transform(y_pred).flatten()\n",
    "y_test_rescaled = target_scaler.inverse_transform(y_test_windows_last_column.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_rescaled, label='Actual', color='blue')\n",
    "plt.plot(y_pred_rescaled, label='Predicted', color='red', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Values (Rescaled)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Publications')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.75062657, 1.75062657, 2.50125313, 1.75062657,\n",
       "       2.50125313, 1.        , 1.75062657, 2.50125313, 1.        ,\n",
       "       2.50125313, 4.00250627, 1.        , 1.75062657, 2.50125313,\n",
       "       1.75062657, 2.50125313, 1.75062657, 1.        , 1.75062657,\n",
       "       3.2518797 , 3.2518797 , 1.        , 1.        , 1.        ,\n",
       "       2.50125313, 2.50125313, 1.75062657, 1.75062657, 2.50125313,\n",
       "       1.        , 1.75062657, 1.75062657, 1.        , 1.75062657,\n",
       "       3.2518797 , 2.50125313, 1.75062657, 1.75062657, 2.50125313,\n",
       "       2.50125313, 1.        , 1.75062657, 1.75062657, 1.        ,\n",
       "       1.75062657, 3.2518797 , 2.50125313, 2.50125313, 1.75062657,\n",
       "       1.        , 2.50125313, 1.75062657, 1.75062657, 1.75062657,\n",
       "       1.75062657, 1.75062657, 1.        , 1.        , 1.75062657,\n",
       "       3.2518797 , 1.        , 1.75062657, 2.50125313, 1.75062657,\n",
       "       1.        , 1.        , 1.75062657, 1.75062657, 2.50125313,\n",
       "       1.75062657, 2.50125313, 2.50125313, 1.        , 1.        ,\n",
       "       1.        , 1.75062657, 1.75062657, 4.00250627, 1.75062657,\n",
       "       1.75062657, 1.        , 1.75062657, 1.75062657, 1.75062657,\n",
       "       2.50125313, 1.        , 1.        , 1.75062657, 1.75062657,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.75062657,\n",
       "       1.75062657, 1.75062657, 1.        , 1.75062657, 1.75062657,\n",
       "       3.2518797 , 1.75062657, 2.50125313, 2.50125313, 1.        ,\n",
       "       1.75062657, 1.75062657, 1.75062657, 2.50125313, 1.        ,\n",
       "       1.75062657, 1.75062657, 1.75062657, 2.50125313, 1.75062657,\n",
       "       2.50125313, 3.2518797 , 2.50125313, 1.        , 1.75062657,\n",
       "       1.75062657, 1.75062657, 1.75062657, 1.        , 1.        ,\n",
       "       2.50125313, 1.75062657, 1.75062657, 1.        , 3.2518797 ,\n",
       "       1.        , 1.        , 1.75062657, 1.        , 1.75062657,\n",
       "       1.        , 2.50125313, 1.        , 2.50125313, 3.2518797 ,\n",
       "       1.        , 2.50125313, 1.75062657, 1.75062657, 3.2518797 ,\n",
       "       1.75062657, 1.75062657, 1.        , 1.        , 1.75062657,\n",
       "       1.75062657, 1.75062657, 1.75062657, 2.50125313, 3.2518797 ,\n",
       "       1.75062657, 3.2518797 , 1.75062657, 1.75062657, 2.50125313,\n",
       "       1.        , 1.75062657, 1.75062657, 1.75062657, 1.        ,\n",
       "       1.75062657, 1.        , 1.        , 1.75062657, 2.50125313,\n",
       "       3.2518797 , 2.50125313, 1.75062657, 1.        , 2.50125313,\n",
       "       1.75062657, 1.75062657, 1.        , 1.75062657, 1.75062657,\n",
       "       1.75062657, 3.2518797 , 2.50125313, 1.75062657, 2.50125313,\n",
       "       1.75062657, 2.50125313, 1.75062657, 1.75062657, 1.        ,\n",
       "       2.50125313, 2.50125313, 2.50125313, 1.        , 2.50125313,\n",
       "       1.75062657, 2.50125313, 1.75062657, 1.75062657, 1.75062657,\n",
       "       1.75062657, 2.50125313, 1.75062657, 1.        , 1.        ,\n",
       "       1.        , 1.75062657, 2.50125313, 1.75062657, 1.        ,\n",
       "       1.75062657, 1.75062657, 2.50125313, 2.50125313, 1.75062657,\n",
       "       1.        , 1.        , 1.        , 1.75062657, 2.50125313,\n",
       "       1.75062657, 1.        , 1.75062657, 1.75062657, 1.75062657,\n",
       "       1.75062657, 1.75062657, 1.75062657, 1.75062657, 1.75062657,\n",
       "       1.75062657, 1.        , 1.        , 1.        , 3.2518797 ,\n",
       "       2.50125313, 1.75062657, 1.        , 2.50125313, 1.75062657,\n",
       "       1.        , 3.2518797 , 1.75062657, 3.2518797 , 1.75062657,\n",
       "       3.2518797 , 1.75062657, 1.75062657, 1.        , 1.        ,\n",
       "       1.        , 1.        , 2.50125313, 1.        , 1.        ,\n",
       "       1.75062657, 1.75062657, 1.75062657, 1.75062657, 1.75062657,\n",
       "       1.75062657, 2.50125313, 1.        , 2.50125313, 1.        ,\n",
       "       1.75062657, 1.        , 2.50125313, 1.75062657, 1.75062657,\n",
       "       1.        , 1.75062657, 2.50125313, 1.75062657, 1.        ,\n",
       "       1.        , 3.2518797 , 1.75062657, 1.        , 1.        ,\n",
       "       2.50125313, 3.2518797 , 1.75062657, 1.        , 1.        ,\n",
       "       1.        , 1.75062657, 1.        , 1.75062657, 1.75062657,\n",
       "       2.50125313, 1.        , 2.50125313, 1.75062657, 1.75062657,\n",
       "       1.75062657, 1.        , 1.75062657, 3.2518797 , 1.        ,\n",
       "       3.2518797 , 1.75062657, 2.50125313, 1.75062657, 2.50125313,\n",
       "       1.75062657, 1.75062657, 1.        , 1.75062657, 2.50125313,\n",
       "       2.50125313, 3.2518797 , 1.75062657, 2.50125313, 1.75062657,\n",
       "       1.        , 1.        , 1.        , 3.2518797 , 1.75062657,\n",
       "       1.        , 2.50125313, 2.50125313, 1.75062657, 1.75062657,\n",
       "       1.75062657, 1.75062657, 1.        , 3.2518797 , 1.75062657,\n",
       "       2.50125313, 1.75062657, 1.75062657, 1.        , 1.75062657,\n",
       "       2.50125313, 1.        , 1.75062657, 3.2518797 , 2.50125313,\n",
       "       1.75062657, 2.50125313, 1.        , 1.75062657, 2.50125313,\n",
       "       1.75062657, 1.75062657, 1.75062657, 1.75062657, 1.75062657,\n",
       "       1.75062657, 2.50125313, 1.        , 1.75062657, 1.75062657,\n",
       "       3.2518797 , 1.        , 1.75062657, 1.        , 1.75062657,\n",
       "       1.        , 2.50125313, 3.2518797 , 3.2518797 , 1.75062657,\n",
       "       2.50125313, 2.50125313, 2.50125313, 1.        , 1.        ,\n",
       "       1.75062657, 1.75062657, 1.75062657, 1.75062657, 1.75062657,\n",
       "       1.75062657, 1.75062657, 2.50125313, 1.75062657, 1.75062657,\n",
       "       2.50125313, 1.        , 4.00250627, 1.75062657, 1.75062657,\n",
       "       1.75062657, 1.75062657, 1.        , 2.50125313, 1.75062657,\n",
       "       1.75062657, 3.2518797 , 1.75062657, 1.75062657, 4.75313283,\n",
       "       1.        , 2.50125313, 1.75062657, 1.        , 1.        ,\n",
       "       2.50125313, 1.75062657, 2.50125313, 1.        , 1.75062657,\n",
       "       1.75062657, 4.00250627, 1.75062657, 1.        , 3.2518797 ,\n",
       "       1.        , 1.75062657, 1.75062657, 1.75062657, 3.2518797 ,\n",
       "       1.75062657, 1.        , 1.75062657, 1.75062657, 1.        ,\n",
       "       1.75062657, 1.75062657, 1.        , 2.50125313, 1.75062657,\n",
       "       2.50125313, 1.75062657, 2.50125313, 1.        , 1.75062657,\n",
       "       2.50125313, 1.        , 1.        , 2.50125313, 1.        ,\n",
       "       1.75062657, 1.75062657, 1.75062657, 1.75062657, 1.        ,\n",
       "       1.        , 1.75062657, 1.75062657, 2.50125313, 1.75062657,\n",
       "       3.2518797 , 1.        , 2.50125313, 1.75062657, 1.75062657,\n",
       "       1.        , 3.2518797 , 1.75062657, 3.2518797 , 2.50125313,\n",
       "       1.75062657, 2.50125313, 1.75062657, 1.        , 1.75062657,\n",
       "       2.50125313, 2.50125313, 1.        , 1.75062657, 2.50125313,\n",
       "       2.50125313, 1.75062657, 3.2518797 , 1.75062657, 1.75062657,\n",
       "       1.75062657, 1.75062657, 1.        , 1.        , 2.50125313,\n",
       "       1.        , 4.75313283, 2.50125313, 2.50125313, 1.75062657,\n",
       "       2.50125313, 2.50125313, 2.50125313, 2.50125313, 1.        ,\n",
       "       3.2518797 , 1.        , 1.        , 1.75062657, 1.75062657,\n",
       "       3.2518797 , 1.        , 1.75062657, 2.50125313, 1.75062657,\n",
       "       1.        , 1.75062657, 1.75062657, 1.        , 1.75062657,\n",
       "       1.        , 1.75062657, 1.75062657, 3.2518797 , 1.75062657,\n",
       "       3.2518797 , 1.        , 1.75062657, 1.75062657, 2.50125313,\n",
       "       3.2518797 , 1.75062657, 1.        , 1.75062657, 3.2518797 ,\n",
       "       1.75062657, 1.        , 1.75062657, 1.75062657, 1.75062657,\n",
       "       3.2518797 , 1.75062657, 2.50125313, 1.        , 2.50125313,\n",
       "       1.        , 1.75062657, 1.        , 3.2518797 , 1.75062657,\n",
       "       3.2518797 , 3.2518797 , 3.2518797 , 3.2518797 , 1.        ,\n",
       "       1.        , 3.2518797 , 2.50125313, 1.75062657, 3.2518797 ,\n",
       "       1.75062657, 1.75062657, 1.75062657, 2.50125313, 2.50125313,\n",
       "       1.75062657, 1.75062657, 1.75062657, 1.        , 1.75062657,\n",
       "       1.        , 1.75062657, 1.        , 1.75062657, 1.75062657,\n",
       "       1.75062657, 1.75062657, 2.50125313, 2.50125313, 1.75062657,\n",
       "       1.75062657, 1.        , 1.75062657, 1.75062657, 2.50125313,\n",
       "       2.50125313, 1.75062657, 1.75062657, 1.        , 1.75062657,\n",
       "       2.50125313, 2.50125313, 3.2518797 , 2.50125313, 1.        ,\n",
       "       1.        , 2.50125313, 1.75062657, 1.        , 2.50125313,\n",
       "       3.2518797 , 2.50125313, 1.75062657, 2.50125313, 1.        ,\n",
       "       1.75062657, 2.50125313, 2.50125313, 2.50125313, 1.        ,\n",
       "       1.        , 1.75062657, 3.2518797 , 1.75062657, 1.75062657,\n",
       "       3.2518797 , 2.50125313, 1.        , 1.75062657, 1.75062657,\n",
       "       2.50125313, 2.50125313, 1.75062657, 1.        , 1.75062657,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.75062657,\n",
       "       1.75062657, 1.        , 1.75062657, 1.        , 1.        ,\n",
       "       1.75062657, 2.50125313, 1.75062657, 3.2518797 , 1.75062657,\n",
       "       2.50125313, 1.        , 1.75062657, 1.75062657, 1.75062657,\n",
       "       1.75062657, 1.75062657, 1.        , 2.50125313, 1.75062657,\n",
       "       1.75062657, 3.2518797 , 1.75062657, 1.75062657, 1.75062657,\n",
       "       4.00250627, 1.75062657, 1.75062657, 2.50125313, 2.50125313,\n",
       "       1.75062657, 1.75062657, 2.50125313, 2.50125313, 3.2518797 ,\n",
       "       1.75062657, 1.75062657, 1.75062657, 2.50125313, 2.50125313,\n",
       "       1.75062657, 4.00250627, 1.75062657, 1.75062657, 1.75062657,\n",
       "       1.        , 1.75062657, 1.75062657, 3.2518797 , 1.        ,\n",
       "       1.        , 2.50125313, 2.50125313, 1.75062657, 1.75062657,\n",
       "       1.        , 1.75062657, 2.50125313, 2.50125313, 1.75062657,\n",
       "       2.50125313, 1.75062657, 1.75062657, 1.75062657, 3.2518797 ,\n",
       "       1.75062657, 2.50125313, 2.50125313, 1.        , 1.75062657,\n",
       "       1.        , 1.        , 1.75062657, 3.2518797 , 2.50125313,\n",
       "       1.        , 1.75062657, 1.        , 2.50125313, 1.75062657])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract loss and validation loss values\n",
    "loss = model.history.history['loss']\n",
    "val_loss = model.history.history['val_loss']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(loss, label='Training Loss', color='blue')\n",
    "plt.plot(val_loss, label='Validation Loss', color='red')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_publications_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
