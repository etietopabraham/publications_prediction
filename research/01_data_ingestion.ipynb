{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/predict_publications/publications_prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First thing we do is setup our config.yaml which holds information about where our data will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Root directory for all artifacts\n",
    "artifacts_root: artifacts\n",
    "\n",
    "# Configuration related to data ingestion\n",
    "data_ingestion:\n",
    "\n",
    "  # Directory where data ingestion artifacts are stored\n",
    "  root_dir: artifacts/data_ingestion\n",
    "\n",
    "  # Path to the local file where the data is already saved\n",
    "  local_data_file: /Users/macbookpro/Documents/predict_publications/publications_prediction/data/train_data.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Schema (Not required at this stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Params (Not Required at this stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Entity (Return types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    \"\"\"\n",
    "    Configuration for data ingestion process.\n",
    "    \n",
    "    Attributes:\n",
    "    - root_dir: Directory where data ingestion artifacts are stored.\n",
    "    - local_data_file: Path to the local file where the data is already saved.\n",
    "    \"\"\"\n",
    "    root_dir: Path  # Directory where data ingestion artifacts are stored\n",
    "    local_data_file: Path  # Path to the local file where the data is already saved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path to the main configuration file\n",
    "CONFIG_FILE_PATH = Path(\"config/config.yaml\")\n",
    "\n",
    "# Path to the parameters file\n",
    "PARAMS_FILE_PATH = Path(\"params.yaml\")\n",
    "\n",
    "# Path to the schema definition file\n",
    "SCHEMA_FILE_PATH = Path(\"schema.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Manager Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicting_publications.constants import *\n",
    "from predicting_publications.utils.common import read_yaml, create_directories\n",
    "from predicting_publications import logger\n",
    "# from predicting_publications.entity.config_entity import DataIngestionConfig\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager manages configurations needed for the data pipeline.\n",
    "\n",
    "    The class reads configuration, parameter, and schema settings from specified files\n",
    "    and provides a set of methods to access these settings. It also takes care of\n",
    "    creating necessary directories defined in the configurations.\n",
    "\n",
    "    Attributes:\n",
    "    - config (dict): Configuration settings.\n",
    "    - params (dict): Parameters for the pipeline.\n",
    "    - schema (dict): Schema information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH, \n",
    "                 params_filepath = PARAMS_FILE_PATH, \n",
    "                 schema_filepath = SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"schema\")\n",
    "\n",
    "        # Create the directory for storing artifacts if it doesn't exist\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data ingestion configurations as a DataIngestionConfig object.\n",
    "\n",
    "        This method fetches settings related to data ingestion, like directories and file paths,\n",
    "        and returns them as a DataIngestionConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataIngestionConfig: Object containing data ingestion configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_ingestion' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_ingestion\n",
    "            # Create the root directory for data ingestion if it doesn't already exist\n",
    "            create_directories([config.root_dir])\n",
    "            \n",
    "            return DataIngestionConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                local_data_file=Path(config.local_data_file),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            logger.error(\"The 'data_ingestion' attribute does not exist in the config file.\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from predicting_publications import logger\n",
    "from predicting_publications.utils.common import get_size\n",
    "# from predicting_publications.entity.config_entity import DataIngestionConfig\n",
    "\n",
    "class DataIngestion:\n",
    "    \"\"\"\n",
    "    DataIngestion handles the process of transferring data from a local directory \n",
    "    to the project's official artifact directories.\n",
    "\n",
    "    The class currently assumes that the data is already present locally, \n",
    "    and focuses on transferring this data to the specified directory.\n",
    "\n",
    "    Attributes:\n",
    "    - config (DataIngestionConfig): Configuration settings for data ingestion.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        \"\"\"\n",
    "        Initialize the DataIngestion component.\n",
    "\n",
    "        Args:\n",
    "        - config (DataIngestionConfig): Configuration settings for data ingestion.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def download_data(self):\n",
    "        \"\"\" \n",
    "        Placeholder for downloading data functionality. \n",
    "        Currently, data is assumed to be locally available.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def extract_zip_file(self):\n",
    "        \"\"\"\n",
    "        Placeholder for extracting zip files. \n",
    "        If the data comes as a zip file, this method can be used to extract it.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def transfer_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Transfer the data from the local directory to the project's artifact directory.\n",
    "\n",
    "        This method ensures that the artifact directory exists, and then transfers \n",
    "        the data file to this directory.\n",
    "\n",
    "        Raises:\n",
    "        - FileNotFoundError: If the local data file does not exist.\n",
    "        \"\"\"\n",
    "        root_dir = Path(self.config.root_dir)\n",
    "        local_data_path = Path(self.config.local_data_file)\n",
    "        \n",
    "        # Check if the local data file exists\n",
    "        if not local_data_path.exists():\n",
    "            logger.error(f\"Local data file not found at {local_data_path}.\")\n",
    "            raise FileNotFoundError(f\"No file found at {local_data_path}\")\n",
    "\n",
    "        # Get the file size using the utility function\n",
    "        file_size = get_size(local_data_path)\n",
    "\n",
    "        # Ensure the transfer directory exists\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "        # Transfer the file\n",
    "        shutil.copy2(local_data_path, root_dir)\n",
    "        logger.info(f\"Data transferred from {local_data_path} to {root_dir}. File size: {file_size}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-16 02:55:10,406: 42: predict_publications_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2023-10-16 02:55:10,408: 42: predict_publications_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2023-10-16 02:55:10,409: 42: predict_publications_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2023-10-16 02:55:10,409: 65: predict_publications_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2023-10-16 02:55:10,410: 34: predict_publications_logger: INFO: 2877418938:  >>>>>> Stage: Data Ingestion Stage started <<<<<<]\n",
      "[2023-10-16 02:55:10,410: 17: predict_publications_logger: INFO: 2877418938:  Fetching data ingestion configuration...]\n",
      "[2023-10-16 02:55:10,410: 65: predict_publications_logger: INFO: common:  Created directory at: artifacts/data_ingestion]\n",
      "[2023-10-16 02:55:10,411: 20: predict_publications_logger: INFO: 2877418938:  Initializing data ingestion process...]\n",
      "[2023-10-16 02:55:10,411: 23: predict_publications_logger: INFO: 2877418938:  Copying training data from /Users/macbookpro/Documents/predict_publications/publications_prediction/data/train_data.csv to artifacts/data_ingestion...]\n",
      "[2023-10-16 02:55:13,931: 68: predict_publications_logger: INFO: 4118853748:  Data transferred from /Users/macbookpro/Documents/predict_publications/publications_prediction/data/train_data.csv to artifacts/data_ingestion. File size: ~ 981573 KB.]\n",
      "[2023-10-16 02:55:13,934: 36: predict_publications_logger: INFO: 2877418938:  >>>>>> Stage Data Ingestion Stage completed <<<<<< \n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "# from predicting_publications.config.configuration import ConfigurationManager\n",
    "# from predicting_publications.components.data_ingestion import DataIngestion\n",
    "from predicting_publications import logger\n",
    "\n",
    "class DataIngestionPipeline:\n",
    "\n",
    "    STAGE_NAME = \"Data Ingestion Stage\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config_manager = ConfigurationManager()\n",
    "\n",
    "    def run_data_ingestion(self):\n",
    "        \"\"\"\n",
    "        Main method to run the data ingestion process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Fetching data ingestion configuration...\")\n",
    "            data_ingestion_config = self.config_manager.get_data_ingestion_config()\n",
    "            \n",
    "            logger.info(\"Initializing data ingestion process...\")\n",
    "            data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "            \n",
    "            logger.info(f\"Copying training data from {data_ingestion_config.local_data_file} to {data_ingestion_config.root_dir}...\")\n",
    "            data_ingestion.transfer_data()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(\"An error occurred during the data ingestion process.\")\n",
    "            raise e\n",
    "        \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the data ingestion training pipeline.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\">>>>>> Stage: {DataIngestionPipeline.STAGE_NAME} started <<<<<<\")\n",
    "            self.run_data_ingestion()\n",
    "            logger.info(f\">>>>>> Stage {DataIngestionPipeline.STAGE_NAME} completed <<<<<< \\n\\nx==========x\")\n",
    "        except Exception as e:\n",
    "            # No need to log the exception here since it's already logged in the run_data_ingestion method.\n",
    "            raise e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = DataIngestionPipeline()\n",
    "    pipeline.run_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_publications_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
