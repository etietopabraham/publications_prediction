{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/predict_publications/publications_prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration related to data validation\n",
    "data_validation:\n",
    "  # Directory where data validation results and artifacts are stored\n",
    "  root_dir: artifacts/data_validation\n",
    "  \n",
    "  # Path to the ingested data file that will be used for validation\n",
    "  data_source_file: artifacts/data_ingestion/train_data.csv\n",
    "  \n",
    "  # Path to the file that captures the validation status (e.g., success, errors encountered)\n",
    "  status_file: artifacts/initial_data_validation/status.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Update schema.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "schema_type: \"initial\"\n",
    "description: \"Schema of the initial data before feature engineering.\"\n",
    "\n",
    "columns:\n",
    "  timestamp: \n",
    "    type: int64\n",
    "    description: \"Timestamp of the data entry.\"\n",
    "  lon: \n",
    "    type: float64\n",
    "    description: \"Longitude value.\"\n",
    "  lat: \n",
    "    type: float64\n",
    "    description: \"Latitude value.\"\n",
    "  likescount: \n",
    "    type: int64\n",
    "    description: \"Count of likes.\"\n",
    "  commentscount: \n",
    "    type: int64\n",
    "    description: \"Count of comments.\"\n",
    "  symbols_cnt: \n",
    "    type: int64\n",
    "    description: \"Count of symbols.\"\n",
    "  words_cnt: \n",
    "    type: int64\n",
    "    description: \"Count of words.\"\n",
    "  hashtags_cnt: \n",
    "    type: int64\n",
    "    description: \"Count of hashtags.\"\n",
    "  mentions_cnt: \n",
    "    type: int64\n",
    "    description: \"Count of mentions.\"\n",
    "  links_cnt: \n",
    "    type: int64\n",
    "    description: \"Count of links.\"\n",
    "  emoji_cnt: \n",
    "    type: int64\n",
    "    description: \"Count of emojis.\"\n",
    "  point: \n",
    "    type: object\n",
    "    description: \"Geographical point object.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup Params.yaml (Not Required at this Stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for data validation process.\n",
    "    \n",
    "    This configuration class captures the necessary paths and directories \n",
    "    required for the validation of data both pre and post feature engineering.\n",
    "    \n",
    "    Attributes:\n",
    "    - root_dir: Directory where data validation results and artifacts are stored.\n",
    "    - data_source_file: Path to the file where the ingested or feature-engineered data is stored.\n",
    "    - status_file: Path to the file that captures the validation status (e.g., success, errors encountered).\n",
    "    - initial_schema: Dictionary holding all schema configurations. This can include initial data schema,\n",
    "                  feature-engineered data schema, and any other relevant schema definitions.\n",
    "    \"\"\"\n",
    "    \n",
    "    root_dir: Path  # Directory for storing validation results and related artifacts\n",
    "    data_source_file: Path  # Path to the ingested or feature-engineered data file\n",
    "    status_file: Path  # File for logging the validation status\n",
    "    initial_schema: Dict[str, Dict[str, str]]  # Dictionary containing initial schema configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Setup Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicting_publications.constants import *\n",
    "from predicting_publications.utils.common import read_yaml, create_directories\n",
    "from predicting_publications import logger\n",
    "from predicting_publications.entity.config_entity import DataIngestionConfig\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager manages configurations needed for the data pipeline.\n",
    "\n",
    "    The class reads configuration, parameter, and schema settings from specified files\n",
    "    and provides a set of methods to access these settings. It also takes care of\n",
    "    creating necessary directories defined in the configurations.\n",
    "\n",
    "    Attributes:\n",
    "    - config (dict): Configuration settings.\n",
    "    - params (dict): Parameters for the pipeline.\n",
    "    - schema (dict): Schema information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH, \n",
    "                 params_filepath = PARAMS_FILE_PATH, \n",
    "                 schema_filepath = SCHEMA_FILE_PATH,\n",
    "                 feature_schema_filepath = FEATURE_SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"initial_schema\")\n",
    "        self.feature_schema_filepath = self._read_config_file(feature_schema_filepath, \"feature_engineered_schema\")\n",
    "\n",
    "        # Create the directory for storing artifacts if it doesn't exist\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data ingestion configurations as a DataIngestionConfig object.\n",
    "\n",
    "        This method fetches settings related to data ingestion, like directories and file paths,\n",
    "        and returns them as a DataIngestionConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataIngestionConfig: Object containing data ingestion configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_ingestion' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_ingestion\n",
    "            # Create the root directory for data ingestion if it doesn't already exist\n",
    "            create_directories([config.root_dir])\n",
    "            \n",
    "            return DataIngestionConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                local_data_file=Path(config.local_data_file),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            logger.error(\"The 'data_ingestion' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data validation configurations as a DataValidationConfig object.\n",
    "\n",
    "        This method fetches settings related to data validation, like directories, file paths,\n",
    "        and schema, and returns them as a DataValidationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataValidationConfig: Object containing data validation configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_validation' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract data validation configurations\n",
    "            config = self.config.data_validation\n",
    "            \n",
    "            # Extract schema for data validation\n",
    "            schema = self.schema.columns\n",
    "            \n",
    "            # # Ensure the status directory for data validation exists\n",
    "            # create_directories([config.status_file])\n",
    "            # Ensure the parent directory for the status file exists\n",
    "            create_directories([os.path.dirname(config.status_file)])\n",
    "\n",
    "            \n",
    "            # Construct and return the DataValidationConfig object\n",
    "            return DataValidationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                status_file=Path(config.status_file),\n",
    "                initial_schema=schema\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_validation' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from predicting_publications import logger\n",
    "\n",
    "\n",
    "class DataValidation:\n",
    "    \"\"\"\n",
    "    Validates the data against a predefined schema to ensure that all expected columns \n",
    "    are present and of the correct type.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataValidation component by reading the data source file \n",
    "        specified in the config.\n",
    "\n",
    "        Args:\n",
    "        - config (DataValidationConfig): Configuration settings for data validation.\n",
    "\n",
    "        Attributes:\n",
    "        - df (pd.DataFrame): The data to be validated.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.config.data_source_file)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {self.config.data_source_file}\")\n",
    "            raise\n",
    "\n",
    "    def validate_all_features(self) -> bool:\n",
    "        \"\"\"\n",
    "        Validates that all expected columns are present in the dataframe.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if validation is successful, False otherwise.\n",
    "        \"\"\"\n",
    "        validation_status = True\n",
    "        status_message = \"Validation status: \"\n",
    "\n",
    "        # Determine missing or extra columns\n",
    "        all_columns = set(self.df.columns)\n",
    "        expected_columns = set(self.config.initial_schema.keys())\n",
    "\n",
    "        missing_columns = expected_columns - all_columns\n",
    "        extra_columns = all_columns - expected_columns\n",
    "\n",
    "        # Log and update the status message for any discrepancies\n",
    "        if missing_columns:\n",
    "            validation_status = False\n",
    "            logger.warning(f\"Missing columns: {', '.join(missing_columns)}\")\n",
    "            status_message += f\"Missing columns: {', '.join(missing_columns)}. \"\n",
    "        if extra_columns:\n",
    "            validation_status = False\n",
    "            logger.warning(f\"Extra columns found: {', '.join(extra_columns)}\")\n",
    "            status_message += f\"Extra columns found: {', '.join(extra_columns)}. \"\n",
    "        if validation_status:\n",
    "            logger.info(\"All expected columns are present in the dataframe.\")\n",
    "            status_message += \"All expected columns are present.\"\n",
    "\n",
    "        # Append the validation status to the file\n",
    "        self._write_status_to_file(status_message, overwrite=True)\n",
    "        return validation_status\n",
    "\n",
    "    def validate_data_types(self) -> bool:\n",
    "        \"\"\"\n",
    "        Validates the data types of each column in the dataframe against \n",
    "        the expected data types specified in the schema.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if all data types match, False otherwise.\n",
    "        \"\"\"\n",
    "        validation_status = True\n",
    "        status_message = \"Data type validation status: \"\n",
    "\n",
    "        expected_data_types = {col: self.config.initial_schema[col]['type'] for col in self.config.initial_schema}\n",
    "\n",
    "        for column, dtype in expected_data_types.items():\n",
    "            # Check if the column exists in the dataframe\n",
    "            if column in self.df.columns:\n",
    "                if not pd.api.types.is_dtype_equal(self.df[column].dtype, dtype):\n",
    "                    validation_status = False\n",
    "                    logger.warning(f\"Data type mismatch for column '{column}': Expected {dtype} but got {self.df[column].dtype}\")\n",
    "                    status_message += f\"Data type mismatch for column '{column}': Expected {dtype} but got {self.df[column].dtype}. \"\n",
    "            else:\n",
    "                validation_status = False\n",
    "                logger.warning(f\"Column '{column}' not found in dataframe.\")\n",
    "                status_message += f\"Column '{column}' not found in dataframe. \"\n",
    "\n",
    "        if validation_status:\n",
    "            logger.info(\"All data types are as expected.\")\n",
    "            status_message += \"All data types are as expected.\"\n",
    "\n",
    "        # Append the validation status to the file\n",
    "        self._write_status_to_file(status_message)\n",
    "        return validation_status\n",
    "\n",
    "    def _write_status_to_file(self, message: str, overwrite: bool = False):\n",
    "        \"\"\"\n",
    "        Writes a given message to the status file specified in the config.\n",
    "\n",
    "        Args:\n",
    "        - message (str): The message to write.\n",
    "        - overwrite (bool): If True, overwrites the file. If False, appends to the file.\n",
    "        \"\"\"\n",
    "        mode = 'w' if overwrite else 'a'\n",
    "        try:\n",
    "            with open(self.config.status_file, mode) as f:\n",
    "                f.write(message + \"\\n\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing to status file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_all_validations(self):\n",
    "        \"\"\"\n",
    "        Executes all validations and writes the overall validation status.\n",
    "        \"\"\"\n",
    "        feature_validation_status = self.validate_all_features()\n",
    "        data_type_validation_status = self.validate_data_types()\n",
    "\n",
    "        overall_status = \"Overall Validation Status: \"\n",
    "        if feature_validation_status and data_type_validation_status:\n",
    "            overall_status += \"All validations passed.\"\n",
    "        else:\n",
    "            overall_status += \"Some validations failed. Check the log for details.\"\n",
    "        \n",
    "        self._write_status_to_file(overall_status)\n",
    "\n",
    "    def _save_dataframe(self):\n",
    "        \"\"\"\n",
    "        Save the dataframe to the output path specified in the configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.df.to_csv(self.config.validated_data_file, index=False)\n",
    "            logger.info(f\"Data saved successfully to {self.config.root_dir}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while saving the dataframe: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Setup Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-16 15:04:33,154: 42: predict_publications_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2023-10-16 15:04:33,157: 42: predict_publications_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2023-10-16 15:04:33,161: 42: predict_publications_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2023-10-16 15:04:33,162: 42: predict_publications_logger: INFO: common:  yaml file: feature_engineered_schema.yaml loaded successfully]\n",
      "[2023-10-16 15:04:33,163: 65: predict_publications_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2023-10-16 15:04:33,163: 53: predict_publications_logger: INFO: 3638325286:  >>>>>> Stage: Initial Data Validation Pipeline started <<<<<<]\n",
      "[2023-10-16 15:04:33,164: 31: predict_publications_logger: INFO: 3638325286:  Fetching initial data validation configuration...]\n",
      "[2023-10-16 15:04:33,164: 65: predict_publications_logger: INFO: common:  Created directory at: artifacts/initial_data_validation]\n",
      "[2023-10-16 15:04:33,165: 34: predict_publications_logger: INFO: 3638325286:  Initializing data validation process...]\n",
      "[2023-10-16 15:04:38,715: 37: predict_publications_logger: INFO: 3638325286:  Executing Data Validations...]\n",
      "[2023-10-16 15:04:38,717: 56: predict_publications_logger: INFO: 2202699987:  All expected columns are present in the dataframe.]\n",
      "[2023-10-16 15:04:38,720: 89: predict_publications_logger: INFO: 2202699987:  All data types are as expected.]\n",
      "[2023-10-16 15:04:38,721: 40: predict_publications_logger: INFO: 3638325286:  Initial Data Validation Pipeline completed successfully.]\n",
      "[2023-10-16 15:04:38,752: 55: predict_publications_logger: INFO: 3638325286:  >>>>>> Stage Initial Data Validation Pipeline completed <<<<<< \n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "from predicting_publications import logger\n",
    "\n",
    "class InitialDataValidationPipeline:\n",
    "    \"\"\"\n",
    "    This pipeline handles the initial data validation steps.\n",
    "\n",
    "    After the data ingestion stage, it's imperative to ensure the data's integrity\n",
    "    before moving on to feature engineering or model training. This class\n",
    "    orchestrates that validation by checking for correct features and data types.\n",
    "\n",
    "    Attributes:\n",
    "        STAGE_NAME (str): The name of this pipeline stage.\n",
    "    \"\"\"\n",
    "\n",
    "    STAGE_NAME = \"Initial Data Validation Pipeline\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the pipeline with a configuration manager.\n",
    "        \"\"\"\n",
    "        self.config_manager = ConfigurationManager()\n",
    "\n",
    "    def run_data_validation(self):\n",
    "        \"\"\"\n",
    "        Run the set of data validations.\n",
    "        \n",
    "        This method orchestrates the different validation functions to ensure the\n",
    "        dataset's integrity.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Fetching initial data validation configuration...\")\n",
    "            data_validation_config = self.config_manager.get_data_validation_config()\n",
    "\n",
    "            logger.info(\"Initializing data validation process...\")\n",
    "            data_validation = DataValidation(config=data_validation_config)\n",
    "\n",
    "            logger.info(\"Executing Data Validations...\")\n",
    "            data_validation.run_all_validations()\n",
    "\n",
    "            logger.info(\"Initial Data Validation Pipeline completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the data validation: {e}\")\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the entire Initial Data Validation Pipeline.\n",
    "        \n",
    "        This method encapsulates the process of the initial data validation and\n",
    "        provides logs for each stage of the pipeline.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\">>>>>> Stage: {InitialDataValidationPipeline.STAGE_NAME} started <<<<<<\")\n",
    "            self.run_data_validation()\n",
    "            logger.info(f\">>>>>> Stage {InitialDataValidationPipeline.STAGE_NAME} completed <<<<<< \\n\\nx==========x\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the {InitialDataValidationPipeline.STAGE_NAME}: {e}\")\n",
    "            raise e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = InitialDataValidationPipeline()\n",
    "    pipeline.run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_publications_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
